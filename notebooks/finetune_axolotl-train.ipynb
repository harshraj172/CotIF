{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning using Axolotl and Python\n",
    "\n",
    "This notebook is an minimal example of how to finetune a LLM using Axolotl and Python. Axolotl is a CLI tool that uses config files for different methods of LLM finetuning. We created a Python wrapper around the CLI for the end-to-end workflow for this process.\n",
    "\n",
    "In the example below, we show how you can define or load finetuning configurations, start an finetuning job, and push it to Hugging Face.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure to run this notebook in a system with enough compute resources to run the finetuning, and follow the setup instructions in the README to install axolotl and related libraries.\n",
    "\n",
    "Let's start with loading the code components we need. The `FinetuneConfig` class holds configurations, and the `Finetune` class is used to create and run a finetuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "from finetune import Finetune, FinetuneConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make sure to login to Hugging Face Hub to save the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570ab58eec6c417d8e3031b7bfc09129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "We then load up a config to perform QLoRA finetuning of Llama 2 from in a config file stored locally. Optionally, we can assign a new field `hub_model_id`, indicating the Hugging Face model the finetuned LLM will be pushed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Specify the path to your YAML file\n",
    "file_path = os.path.join(os.getcwd(), '..', 'finetune/examples/llama-2/qlora.yml')\n",
    "\n",
    "# Open the file and load the data\n",
    "with open(file_path, encoding='utf-8') as file:\n",
    "    config_dict = yaml.safe_load(file)  # Load the existing data\n",
    "\n",
    "config_dict['hub_model_id'] = 'vijil/my_lora_tune'  # Add or update the model_id to push the trained model\n",
    "config_dict['eval_sample_packing'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_model': 'NousResearch/Llama-2-7b-hf',\n",
       " 'model_type': 'LlamaForCausalLM',\n",
       " 'tokenizer_type': 'LlamaTokenizer',\n",
       " 'load_in_8bit': False,\n",
       " 'load_in_4bit': True,\n",
       " 'strict': False,\n",
       " 'datasets': [{'path': 'mhenrichsen/alpaca_2k_test', 'type': 'alpaca'}],\n",
       " 'dataset_prepared_path': None,\n",
       " 'val_set_size': 0.05,\n",
       " 'output_dir': './qlora-out',\n",
       " 'adapter': 'qlora',\n",
       " 'lora_model_dir': None,\n",
       " 'sequence_len': 4096,\n",
       " 'sample_packing': True,\n",
       " 'pad_to_sequence_len': True,\n",
       " 'lora_r': 32,\n",
       " 'lora_alpha': 16,\n",
       " 'lora_dropout': 0.05,\n",
       " 'lora_target_modules': None,\n",
       " 'lora_target_linear': True,\n",
       " 'lora_fan_in_fan_out': None,\n",
       " 'wandb_project': None,\n",
       " 'wandb_entity': None,\n",
       " 'wandb_watch': None,\n",
       " 'wandb_name': None,\n",
       " 'wandb_log_model': None,\n",
       " 'gradient_accumulation_steps': 4,\n",
       " 'micro_batch_size': 2,\n",
       " 'num_epochs': 4,\n",
       " 'optimizer': 'paged_adamw_32bit',\n",
       " 'lr_scheduler': 'cosine',\n",
       " 'learning_rate': 0.0002,\n",
       " 'train_on_inputs': False,\n",
       " 'group_by_length': False,\n",
       " 'bf16': 'auto',\n",
       " 'fp16': None,\n",
       " 'tf32': False,\n",
       " 'gradient_checkpointing': True,\n",
       " 'early_stopping_patience': None,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'local_rank': None,\n",
       " 'logging_steps': 1,\n",
       " 'xformers_attention': None,\n",
       " 'flash_attention': True,\n",
       " 'warmup_steps': 10,\n",
       " 'evals_per_epoch': 4,\n",
       " 'eval_table_size': None,\n",
       " 'saves_per_epoch': 1,\n",
       " 'debug': None,\n",
       " 'deepspeed': None,\n",
       " 'weight_decay': 0.0,\n",
       " 'fsdp': None,\n",
       " 'fsdp_config': None,\n",
       " 'special_tokens': None,\n",
       " 'hub_model_id': 'vijil/my_lora_tune',\n",
       " 'eval_sample_packing': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the config dict in the `FinetuneConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:16:02,711] [DEBUG] [axolotl.normalize_config:79] [PID:18933] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:16:02,835] [INFO] [axolotl.normalize_config:182] [PID:18933] [RANK:0] GPU memory usage baseline: 0.000GB (+0.423GB misc)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# see all config options in './finetune/examples/config.qmd'\n",
    "config = FinetuneConfig(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the finetuning job\n",
    "Now simply load up the config into a `FineTune` object and kick off the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a finetune object with the config and run\n",
    "finetune = Finetune(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Job ID: 385293fa-0979-11ef-b980-024398e8947b\n",
      "[2024-05-03 18:16:11,511] [DEBUG] [axolotl.load_tokenizer:279] [PID:18933] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2024-05-03 18:16:11,512] [DEBUG] [axolotl.load_tokenizer:280] [PID:18933] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2024-05-03 18:16:11,512] [DEBUG] [axolotl.load_tokenizer:281] [PID:18933] [RANK:0] PAD: 0 / <unk>\u001b[39m\n",
      "[2024-05-03 18:16:11,513] [DEBUG] [axolotl.load_tokenizer:282] [PID:18933] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2024-05-03 18:16:11,513] [INFO] [axolotl.load_tokenizer:293] [PID:18933] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-05-03 18:16:11,514] [INFO] [axolotl.load_tokenized_prepared_datasets:183] [PID:18933] [RANK:0] Unable to find prepared dataset in last_run_prepared/5c09ab14f30fc60ead5860bcbbb2e263\u001b[39m\n",
      "[2024-05-03 18:16:11,514] [INFO] [axolotl.load_tokenized_prepared_datasets:184] [PID:18933] [RANK:0] Loading raw datasets...\u001b[39m\n",
      "\u001b[33m[2024-05-03 18:16:11,515] [WARNING] [axolotl.load_tokenized_prepared_datasets:186] [PID:18933] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n",
      "[2024-05-03 18:16:11,515] [INFO] [axolotl.load_tokenized_prepared_datasets:193] [PID:18933] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n",
      "[2024-05-03 18:16:18,549] [INFO] [axolotl.load_tokenized_prepared_datasets:410] [PID:18933] [RANK:0] merging datasets\u001b[39m\n",
      "[2024-05-03 18:16:18,568] [INFO] [axolotl.load_tokenized_prepared_datasets:423] [PID:18933] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/5c09ab14f30fc60ead5860bcbbb2e263\u001b[39m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9b7ee93a564173a6bb69bb9242b84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:16:18,626] [DEBUG] [axolotl.log:61] [PID:18933] [RANK:0] total_num_tokens: 414_041\u001b[39m\n",
      "[2024-05-03 18:16:18,666] [DEBUG] [axolotl.log:61] [PID:18933] [RANK:0] `total_supervised_tokens: 294_246`\u001b[39m\n",
      "[2024-05-03 18:16:22,644] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:18933] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 414041\u001b[39m\n",
      "[2024-05-03 18:16:22,644] [DEBUG] [axolotl.log:61] [PID:18933] [RANK:0] data_loader_len: 6\u001b[39m\n",
      "[2024-05-03 18:16:22,645] [INFO] [axolotl.log:61] [PID:18933] [RANK:0] sample_packing_eff_est across ranks: [0.9719637357271634]\u001b[39m\n",
      "[2024-05-03 18:16:22,645] [DEBUG] [axolotl.log:61] [PID:18933] [RANK:0] sample_packing_eff_est: 0.98\u001b[39m\n",
      "[2024-05-03 18:16:22,646] [DEBUG] [axolotl.log:61] [PID:18933] [RANK:0] total_num_steps: 24\u001b[39m\n",
      "[2024-05-03 18:16:22,656] [DEBUG] [axolotl.train.log:61] [PID:18933] [RANK:0] loading tokenizer... NousResearch/Llama-2-7b-hf\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:16:22,908] [DEBUG] [axolotl.load_tokenizer:279] [PID:18933] [RANK:0] EOS: 2 / </s>\u001b[39m\n",
      "[2024-05-03 18:16:22,909] [DEBUG] [axolotl.load_tokenizer:280] [PID:18933] [RANK:0] BOS: 1 / <s>\u001b[39m\n",
      "[2024-05-03 18:16:22,909] [DEBUG] [axolotl.load_tokenizer:281] [PID:18933] [RANK:0] PAD: 0 / <unk>\u001b[39m\n",
      "[2024-05-03 18:16:22,910] [DEBUG] [axolotl.load_tokenizer:282] [PID:18933] [RANK:0] UNK: 0 / <unk>\u001b[39m\n",
      "[2024-05-03 18:16:22,910] [INFO] [axolotl.load_tokenizer:293] [PID:18933] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n",
      "[2024-05-03 18:16:22,911] [DEBUG] [axolotl.train.log:61] [PID:18933] [RANK:0] loading model and peft_config...\u001b[39m\n",
      "[2024-05-03 18:16:23,278] [INFO] [axolotl.load_model:359] [PID:18933] [RANK:0] patching with flash attention for sample packing\u001b[39m\n",
      "[2024-05-03 18:16:23,279] [INFO] [axolotl.load_model:408] [PID:18933] [RANK:0] patching _expand_mask\u001b[39m\n",
      "[2024-05-03 18:16:23,604] [INFO] [accelerate.utils.modeling.get_balanced_memory:965] [PID:18933] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4b23e677e8442ab5688f75dfa9a0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:18:03,505] [INFO] [axolotl.load_model:720] [PID:18933] [RANK:0] GPU memory usage after model load: 3.710GB (+0.255GB cache, +0.709GB misc)\u001b[39m\n",
      "[2024-05-03 18:18:03,524] [INFO] [axolotl.load_model:771] [PID:18933] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n",
      "[2024-05-03 18:18:03,528] [INFO] [axolotl.load_model:780] [PID:18933] [RANK:0] converting modules to torch.bfloat16 for flash attention\u001b[39m\n",
      "[2024-05-03 18:18:03,533] [INFO] [axolotl.load_lora:924] [PID:18933] [RANK:0] found linear modules: ['gate_proj', 'o_proj', 'q_proj', 'up_proj', 'k_proj', 'v_proj', 'down_proj']\u001b[39m\n",
      "trainable params: 79,953,920 || all params: 6,818,369,536 || trainable%: 1.172625208678628\n",
      "[2024-05-03 18:18:04,304] [INFO] [axolotl.load_model:825] [PID:18933] [RANK:0] GPU memory usage after adapters: 3.859GB (+1.239GB cache, +0.709GB misc)\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:18:04,483] [INFO] [axolotl.train.log:61] [PID:18933] [RANK:0] Pre-saving adapter config to ./qlora-out\u001b[39m\n",
      "[2024-05-03 18:18:04,487] [INFO] [axolotl.train.log:61] [PID:18933] [RANK:0] Starting trainer...\u001b[39m\n",
      "[2024-05-03 18:18:04,739] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:18933] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2024-05-03 18:18:04,741] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:18933] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2024-05-03 18:18:04,795] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:18933] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 1:00:58, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.152100</td>\n",
       "      <td>1.170463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.096200</td>\n",
       "      <td>1.166554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.064900</td>\n",
       "      <td>1.107039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.935900</td>\n",
       "      <td>0.985908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>0.948929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.951500</td>\n",
       "      <td>0.936418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.927200</td>\n",
       "      <td>0.921776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.847500</td>\n",
       "      <td>0.910943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.973000</td>\n",
       "      <td>0.903616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.871800</td>\n",
       "      <td>0.899280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.897104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.904700</td>\n",
       "      <td>0.893823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.826300</td>\n",
       "      <td>0.891913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.860400</td>\n",
       "      <td>0.890088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.904200</td>\n",
       "      <td>0.889849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.810900</td>\n",
       "      <td>0.889658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.889787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:20:43,476] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 18:21:16,142] [INFO] [axolotl.callbacks.on_step_end:125] [PID:18933] [RANK:0] GPU memory usage while training: 3.875GB (+7.346GB cache, +1.358GB misc)\u001b[39m\n",
      "[2024-05-03 18:23:53,372] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 18:27:35,881] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 18:31:18,414] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 18:35:00,929] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:35:36,859] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:18933] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2024-05-03 18:38:46,701] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 18:42:29,216] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 18:46:11,786] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 18:49:54,324] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:50:57,994] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:18933] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2024-05-03 18:53:35,211] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 18:57:17,625] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 19:01:00,064] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 19:04:42,526] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 19:08:27,541] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 19:08:27,645] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:18933] [RANK:0] packing_efficiency_estimate: 0.98 total_num_tokens per device: 414041\u001b[39m\n",
      "[2024-05-03 19:12:10,016] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 19:15:52,487] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "[2024-05-03 19:19:34,980] [INFO] [accelerate.accelerator.log:61] [PID:18933] The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 19:19:44,329] [INFO] [axolotl.train.log:61] [PID:18933] [RANK:0] Training Completed!!! Saving pre-trained model to ./qlora-out\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1d742ab8454da4b1c1158b9138f570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetune.run() # start train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status\n",
    "To check the job status, please use `finetune_axolotl-status.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "conda_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
