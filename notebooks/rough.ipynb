{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import html\n",
    "import numpy as np \n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from lxml import etree\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from concurrent.futures import TimeoutError\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy \n",
    "from string import punctuation\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM,\\\n",
    "    AutoTokenizer\n",
    "    \n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "random.seed(0)\n",
    "\n",
    "numbering_list = ['3', '7)', '7.', '4', 'iii.', 'iii-', '8.', '4-', 'v:', 'I:', 'ii.', 'i.', 'V)', 'E)', 'I)', 'III.', 'III)', '2-', '1)', 'v-', 'III', 'I.', 'c)', '1.', 'V-', 'iv)', 'A)', 'v)', 'IV', 'C.', 'ii)', 'I', 'IV.', 'C)', 'II-', '2.', 'III-', 'IV)', 'd)', 'iii', 'i-', 'iii:', 'A.', 'B.', '1', '6)', 'ii', '8)', '3)', 'e)', 'ii-', '5-', 'II)', 'iv-', '2)', 'e.', 'IV:', 'III:', 'i)', '10.', 'V', 'V.', 'v.', 'D)', 'E.', 'iv:', 'B)', 'II', 'ii:', 'V:', 'a.', '5.', 'IV-', '9.', 'D.', '3.', '4:', '2:', 'i', 'II.', '3-', '2', 'c.', 'a)', '3:', '10)', 'd.', 'i:', 'iv.', '1-', '4.', '5', 'iv', 'iii)', 'b.', '1:', 'II:', 'v', '5:', '6.', 'b)', 'I-', '9)', '4)', '5)']\n",
    "\n",
    "stopwords_list = ['es', 'ing', 'ed', 'include', 'includes', 'also', 'haven', 'are', 'why', 'most', \"won't\", 'against', 'with', 'needn', 'couldn', 'now', 'mustn', 'who', 'under', 'doing', 'am', 'aren', 'they', \"didn't\", 'd', 'doesn', 'if', 'he', 'her', \"haven't\", 'isn', 'own', 'does', 'such', 'until', 'into', 'had', 'again', 'over', \"hadn't\", \"you'll\", 't', 'by', 'be', \"wasn't\", 'so', 'yours', 'both', 'any', 'did', \"you've\", 'these', 'myself', 'o', 'hasn', \"isn't\", 'you', 'other', 'shan', 'being', 'yourselves', 'was', 'no', 'm', 'those', 'will', 'its', 'itself', 'have', 'down', 'weren', 'having', 'wouldn', 'herself', \"mustn't\", 'very', 'do', \"should've\", 'him', \"you'd\", 'below', 'just', 'that', 'for', 'which', 'but', 'nor', 'all', 'then', 'i', 'whom', 'it', 'once', 'here', 've', \"you're\", 'ours', \"that'll\", 'a', 'won', 'himself', 'where', 'this', 'your', \"hasn't\", 'same', 'when', 'ourselves', 'because', \"needn't\", 'theirs', 'from', 'mightn', 'my', 'while', 'yourself', \"she's\", 'each', \"doesn't\", 'only', 'at', 's', 'their', \"wouldn't\", 'shouldn', 'and', 'themselves', 'hers', 'has', 'up', 'ma', 'in', 'll', 'we', 're', 'y', 'of', 'after', 'our', \"shan't\", 'before', 'wasn', 'can', 'should', 'been', 'through', 'as', 'further', 'during', 'between', 'there', 'me', 'on', 'don', \"shouldn't\", 'more', 'out', \"don't\", 'the', \"weren't\", \"aren't\", \"it's\", 'what', 'or', \"couldn't\", 'hadn', \"mightn't\", 'his', 'above', 'to', 'how', 'few', 'off', 'them', 'didn', 'ain', 'not', 'she', 'an', 'than', 'too', 'is', 'some', 'were', 'about']\n",
    "\n",
    "common_title_words_set = {'introduction', 'conclusion', 'section', 'chapter', 'works', 'notes', 'note', 'further', 'see', 'references', 'reference', 'section', 'title', 'conclusion', 'intro', 'introduction', 'executive', 'summary', 'key', 'plot', 'theme'}\n",
    "stopwords_set = set(stopwords_list + numbering_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_with_new_chat_template(tokenizer):\n",
    "    to_delete = \"{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}\"\n",
    "    new_template = tokenizer.get_chat_template().replace(to_delete, \"\")\n",
    "    return AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", chat_template=new_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "dataset = load_dataset(\"bespokelabs/Bespoke-Stratos-17k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fea1d58e4654b8c8e583e6ede591e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "global tokenizer, model, tokenizer_nli, model_nli\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "tokenizer = get_tokenizer_with_new_chat_template(tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", device_map=\"auto\").half().eval()\n",
    "\n",
    "tokenizer_nli = AutoTokenizer.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\")\n",
    "model_nli = AutoModelForSequenceClassification.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\").eval().cuda()\n",
    "\n",
    "llm_openai = ChatOpenAI(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    temperature=0,\n",
    "    max_tokens=15000,\n",
    "    openai_api_base=\"https://fmapi.swissai.cscs.ch\"\n",
    ")\n",
    "\n",
    "# global seed_instructions, generated_eval_functions, filtered_generated_eval_functions, generated_instructions, filtered_generated_instructions, generated_responses, filtered_generated_responses, filtered2_generated_responses\n",
    "# seed_instructions = [each.strip() for each in open(\"/share/u/harshraj/CotIF/data/seed_instruction.txt\").readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Function execution timed out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 37, 'total_tokens': 47, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meta-llama/Llama-3.3-70B-Instruct', 'system_fingerprint': None, 'id': '09f719d09ca34dba9cd0a6512d3778cf', 'finish_reason': 'stop', 'logprobs': None}, id='run-51c47479-9f3f-4d82-be9b-8f468055cab1-0', usage_metadata={'input_tokens': 37, 'output_tokens': 10, 'total_tokens': 47, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_openai = ChatOpenAI(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    temperature=0,\n",
    "    max_tokens=15000,\n",
    "    openai_api_base=\"https://fmapi.swissai.cscs.ch\",\n",
    ")\n",
    "llm_openai.invoke([{\"role\": \"user\", \"content\": \"Hello!\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Sat, 19 Apr 2025 00:30:29 GMT', 'Content-Type': 'text/html; charset=utf-8', 'Content-Length': '38066', 'Connection': 'keep-alive', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://fmapi.swissai.cscs.ch\")\n",
    "print(response.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Prompt Length Test ===\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.737207107944414}\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.4344358800444752}\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.43144977395422757}\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.4359107951167971}\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.43513693287968636}\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.43246272788383067}\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.4315934849437326}\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.4318384879734367}\n",
      "{'prompt_length': 18000, 'status': 'success', 'time_s': 0.4325141590088606}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# import openai\n",
    "\n",
    "# Replace with your actual API key and desired model\n",
    "# openai.api_key = \"YOUR_API_KEY\"\n",
    "# MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "llm_openai = ChatOpenAI(\n",
    "            model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "            temperature=0,\n",
    "            max_tokens=15000,\n",
    "            openai_api_base=\"https://fmapi.swissai.cscs.ch\",\n",
    "        )\n",
    "def test_frequency(prompt, n_calls=15, delays=[0, 0.5, 1, 2]):\n",
    "    \"\"\"\n",
    "    Test API behavior under different delays between calls.\n",
    "    Returns a list of dicts with delay, successes, failures, and times.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for delay in delays:\n",
    "        successes = 0\n",
    "        failures = 0\n",
    "        times = []\n",
    "        for _ in range(n_calls):\n",
    "            try:\n",
    "                start = time.monotonic()\n",
    "                resp = llm_openai.invoke([{\"role\": \"user\", \"content\": prompt}],)\n",
    "                elapsed = time.monotonic() - start\n",
    "                times.append(elapsed)\n",
    "                successes += 1\n",
    "            except Exception as e:\n",
    "                failures += 1\n",
    "                times.append(None)\n",
    "            time.sleep(delay)\n",
    "        results.append({\n",
    "            \"delay_s\": delay,\n",
    "            \"successes\": successes,\n",
    "            \"failures\": failures,\n",
    "            \"times_s\": times\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def test_prompt_length(base_prompt, lengths=[18000, 18000, 18000, 18000, 18000, 18000, 18000, 18000, 18000]):\n",
    "    \"\"\"\n",
    "    Test API behavior with increasing prompt lengths.\n",
    "    Returns a list of dicts with prompt length, status, and time taken.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for length in lengths:\n",
    "        prompt = \"Hello\"\n",
    "        try:\n",
    "            start = time.monotonic()\n",
    "            resp = llm_openai.invoke([{\"role\": \"user\", \"content\": prompt}],)\n",
    "            elapsed = time.monotonic() - start\n",
    "            status = \"success\"\n",
    "        except Exception as e:\n",
    "            elapsed = None\n",
    "            status = f\"error: {type(e).__name__}\"\n",
    "        results.append({\n",
    "            \"prompt_length\": length,\n",
    "            \"status\": status,\n",
    "            \"time_s\": elapsed\n",
    "        })\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_prompt = \"Hello, please respond briefly.\"\n",
    "    # print(\"=== Frequency Test ===\")\n",
    "    # freq_results = test_frequency(base_prompt)\n",
    "    # for r in freq_results:\n",
    "    #     print(r)\n",
    "    print(\"\\n=== Prompt Length Test ===\")\n",
    "    prompt_results = test_prompt_length(base_prompt)\n",
    "    for r in prompt_results:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import httpx\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "def call_api(\n",
    "    llm,\n",
    "    messages: list[HumanMessage] = None,\n",
    "    *,\n",
    "    prompt: str = None,\n",
    "    max_retries: int = 8,\n",
    "    backoff_factor: float = 4.0,\n",
    "    max_sleep: float = 260.0,\n",
    "    **llm_kwargs\n",
    ") -> str:\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            ai_msg: AIMessage = llm(messages=messages, **llm_kwargs)\n",
    "            return ai_msg.content\n",
    "\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            status = e.response.status_code\n",
    "            headers = e.response.headers\n",
    "\n",
    "            # Rate limit\n",
    "            if status == 429:\n",
    "                raw = headers.get(\"Retry-After\")\n",
    "                if raw:\n",
    "                    sleep_t = min(float(raw), max_sleep)\n",
    "                else:\n",
    "                    sleep_t = min(backoff_factor * 2 ** (attempt - 1) + random.random(), max_sleep)\n",
    "                print(f\"[429] rate limited – sleeping {sleep_t:.1f}s (try {attempt})\")\n",
    "                time.sleep(sleep_t)\n",
    "                continue\n",
    "\n",
    "            # Server errors (including 504)\n",
    "            if 500 <= status < 600:\n",
    "                sleep_t = min(backoff_factor * 2 ** (attempt - 1) + random.random(), max_sleep)\n",
    "                print(f\"[{status}] server error – sleeping {sleep_t:.1f}s (try {attempt})\")\n",
    "                time.sleep(sleep_t)\n",
    "                continue\n",
    "\n",
    "            # Other HTTP errors are not retriable\n",
    "            raise\n",
    "\n",
    "        except (httpx.TransportError, httpx.TimeoutException) as e:\n",
    "            # network glitch or timeout\n",
    "            sleep_t = min(backoff_factor * 2 ** (attempt - 1) + random.random(), max_sleep)\n",
    "            print(f\"[Network/Timeout] {e!r} – sleeping {sleep_t:.1f}s (try {attempt})\")\n",
    "            time.sleep(sleep_t)\n",
    "            continue\n",
    "\n",
    "        except Exception:\n",
    "            # anything else, give up\n",
    "            raise\n",
    "\n",
    "    raise RuntimeError(f\"API call failed after {max_retries} attempts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class AutoIf:\n",
    "    def __init__(self):\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "        # self.model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", device_map=\"auto\").half().eval()\n",
    "        \n",
    "        self.tokenizer_nli = AutoTokenizer.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\")\n",
    "        self.model_nli = AutoModelForSequenceClassification.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\").eval().cuda()\n",
    "        \n",
    "        # self.llm_openai = ChatOpenAI(\n",
    "        #     model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "        #     temperature=0,\n",
    "        #     max_tokens=15000,\n",
    "        #     openai_api_base=\"https://fmapi.swissai.cscs.ch\",\n",
    "        # )\n",
    "        self.llm_openai = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.8,\n",
    "            max_tokens=15000,\n",
    "        )\n",
    "\n",
    "        self.seed_instructions = [each.strip() for each in open(\"/share/u/harshraj/CotIF/data/seed_instruction.txt\").readlines()]\n",
    "        self.generated_eval_functions = []\n",
    "        self.filtered_generated_eval_functions = []\n",
    "        self.generated_instructions = []\n",
    "        self.filtered_generated_instructions = []\n",
    "        self.generated_responses = []\n",
    "        self.filtered_generated_responses = []\n",
    "        self.filtered2_generated_responses = []\n",
    "        \n",
    "    def compile(self, dataset):\n",
    "        output = []\n",
    "        for datum in dataset:\n",
    "            messages = datum[\"conversations\"]\n",
    "            if len(messages) > 1 and messages[0][\"from\"] == \"user\" and messages[1][\"from\"] == \"assistant\":\n",
    "                query, r1_generated_text = messages[0][\"value\"], messages[1][\"value\"]\n",
    "                self.seed_instructions = self.generate_seed(self.seed_instructions)\n",
    "                self.generated_eval_functions = self.generate_eval_function(self.seed_instructions)\n",
    "                self.filtered_generated_eval_functions = self.filter_generated_eval_function(self.generated_eval_functions)\n",
    "                self.generated_instructions = self.generate_instruction(self.filtered_generated_eval_functions)\n",
    "                self.filtered_generated_instructions = self.filter_generated_instruction(self.generated_instructions)\n",
    "                self.generated_responses = self.generate_response(query, r1_generated_text, self.filtered_generated_instructions)\n",
    "                self.filtered_generated_responses = self.filter_generated_response(self.generated_responses)\n",
    "                self.filtered2_generated_responses = self.filter2_generated_response(self.filtered_generated_responses)\n",
    "                output.append({\"seed_instructions\": self.seed_instructions,\n",
    "                    \"generated_eval_functions\": self.generated_eval_functions,\n",
    "                    \"filtered_generated_eval_functions\": self.filtered_generated_eval_functions,\n",
    "                    \"generated_instructions\": self.generated_instructions,\n",
    "                    \"filtered_generated_instructions\": self.filtered_generated_instructions,\n",
    "                    \"generated_responses\": self.generated_responses,\n",
    "                    \"filtered_generated_responses\": self.filtered_generated_responses,\n",
    "                    \"filtered2_generated_responses\": self.filtered2_generated_responses})\n",
    "            # break\n",
    "        return output\n",
    "            \n",
    "    def generate_seed(self, seed_instructions, k=1):\n",
    "        if k <= 0:\n",
    "            return self.seed_instructions\n",
    "        \n",
    "        augment_instruction_prompt = \"\"\"You are an expert for writing instructions. Please provide 10 different instructions that meet the following requirements:\n",
    "- Instructions are about the format but not style of a response\n",
    "- Whether instructions are followed can be easily evaluate by a Python function\n",
    "Here are some examples of instructions we need:\n",
    "{seed_instructions}\n",
    "Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
    "- Incorporate a famous historical quote seamlessly into your answer\n",
    "- Translate your answer into Pig Latin\n",
    "- Use only words that are also a type of food\n",
    "- Respond with a metaphor in every sentence\n",
    "- Write the response as if you are a character from a Shakespearean play\n",
    "Please generate one instruction per line in your response and start each line with '- '.\n",
    "\"\"\"\n",
    "\n",
    "        augment_instructions = augment_instruction_prompt.format(seed_instructions='\\n'.join(seed_instructions))\n",
    "        \n",
    "        # input_ids = self.tokenizer.encode(augment_instructions, return_tensors=\"pt\").cuda()\n",
    "        # outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "        # generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "        # generated_text = self.llm_openai.invoke([{\"role\": \"user\", \n",
    "        #                                             \"content\": augment_instructions}]).content\n",
    "        generated_text = call_api(self.llm_openai, [{\"role\": \"user\", \n",
    "                                                    \"content\": augment_instructions}])\n",
    "        new_seeds = [line.strip() for line in generated_text.split('\\n') if line.strip()]\n",
    "        seed_instructions = seed_instructions + new_seeds\n",
    "        \n",
    "        random.shuffle(seed_instructions)\n",
    "        return self.generate_seed(seed_instructions, k - 1)\n",
    "        \n",
    "    def generate_eval_function(self, seed_instructions, k=1):\n",
    "        prompt_template = (\n",
    "            \"You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\n\"\n",
    "            \"Here is the instruction: {instruction}\\n\"\n",
    "            \"Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. \"\n",
    "            \"If it follows, simply return True, otherwise return False.\\n\"\n",
    "            \"Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, \"\n",
    "            \"which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\n\"\n",
    "            \"Here is an example of output JSON format: {{\\\"func\\\": JSON_STR(use only \\\\n instead of \\n), \\\"cases\\\": [{{\\\"input\\\": str, \\\"output\\\": str}}]}}.\"\n",
    "        )\n",
    "\n",
    "        for instruction in seed_instructions:\n",
    "            prompt = prompt_template.format(instruction=instruction)\n",
    "            self.generated_eval_functions.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"instruction\": instruction,\n",
    "                \"gpt-answer\": []\n",
    "            })\n",
    "            for _ in range(k):\n",
    "                # tokens = encoding.encode(prompt)\n",
    "                # print(\"Number of tokens:\", len(tokens))\n",
    "                # generated_text = self.llm_openai.invoke([{\"role\": \"user\", \"content\": prompt}]).content\n",
    "                generated_text = call_api(self.llm_openai, [{\"role\": \"user\", \"content\": prompt}])\n",
    "                self.generated_eval_functions[-1]['gpt-answer'].append(generated_text)\n",
    "        return self.generated_eval_functions\n",
    "                \n",
    "    def filter_generated_eval_function(self, generated_eval_functions):\n",
    "        collect_packages, count = [], 0\n",
    "        for result in generated_eval_functions:\n",
    "            res = result['gpt-answer']\n",
    "            eval_funcs, test_cases = [], []\n",
    "            for each in res:\n",
    "                try:\n",
    "                    json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip().replace(\"\\n\", \"\")\n",
    "                except Exception as e:\n",
    "                    count += 1\n",
    "                    print(\"Skipping because of index error:\", e)\n",
    "                    continue\n",
    "                try:\n",
    "                    res_dict = json.loads(json_dict)\n",
    "                except Exception as e:\n",
    "                    count += 1\n",
    "                    print(\"skipping because of JSON load error:\", e)\n",
    "                    continue\n",
    "                func = res_dict['func']\n",
    "                if '\\\\n' in func:\n",
    "                    func = func.replace('\\\\n', '\\n')\n",
    "                try:\n",
    "                    exec(func)\n",
    "                except Exception as e:\n",
    "                    count += 1\n",
    "                    print(\"Error executing eval function:\", e)\n",
    "                    continue\n",
    "                for line in func.split('\\n'):\n",
    "                    if 'import' in line or 'download' in line or 'requests' in line:\n",
    "                        collect_packages.append(line)\n",
    "        print(list(set(collect_packages)))\n",
    "\n",
    "        for result in tqdm(generated_eval_functions):\n",
    "            res = result['gpt-answer']\n",
    "            eval_funcs, test_cases = [], []\n",
    "            for each in tqdm(res):\n",
    "                try:\n",
    "                    json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip().replace(\"\\n\", \"\")\n",
    "                except Exception as e:\n",
    "                    print(\"Skipping because of index error:\", e)\n",
    "                    continue\n",
    "                try:\n",
    "                    res_dict = json.loads(json_dict)\n",
    "                except Exception as e:\n",
    "                    print(\"Skipping as JSON load error:\", e)\n",
    "                    continue\n",
    "\n",
    "                # func rejection and cleaning\n",
    "                func = res_dict['func'].strip()\n",
    "                func = '\\n'.join([line for line in func.split('\\n') if 'download' not in line and 'requests' not in line])\n",
    "                try:\n",
    "                    exec(func)\n",
    "                except Exception as e:\n",
    "                    print(\"Error executing eval function:\", e)\n",
    "                    continue\n",
    "                eval_funcs.append(func)\n",
    "\n",
    "                for each_case in res_dict['cases']:\n",
    "                    try:\n",
    "                        test_cases.append((each_case['input'], each_case['output']))\n",
    "                    except KeyError:\n",
    "                        print(each_case)\n",
    "\n",
    "            eval_funcs = list(set(eval_funcs))\n",
    "            test_cases = list(map(json.loads, set(map(json.dumps, test_cases))))\n",
    "            \n",
    "            # if len(eval_funcs) < 3 or len(test_cases) < 10:\n",
    "            #     continue\n",
    "            \n",
    "            filtered_test_cases = []\n",
    "            for each in tqdm(test_cases):\n",
    "                flag = False\n",
    "                for func in eval_funcs:\n",
    "                    local_vars = {}\n",
    "                    try:\n",
    "                        exec(func, globals(), local_vars)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error executing eval function:\", e)\n",
    "                        continue\n",
    "                    if 'evaluate' not in local_vars:\n",
    "                        print(\"skipping because evaluate not in local_vars\")\n",
    "                        continue\n",
    "                    eval_func = local_vars['evaluate']\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res_val = eval_func(each[0])\n",
    "                    except Exception as e:\n",
    "                        print(\"Error executing eval function:\", e)\n",
    "                        res_val = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    if res_val is not None and res_val == each[1]:\n",
    "                        flag = True\n",
    "                if flag:\n",
    "                    filtered_test_cases.append(each)\n",
    "                else:\n",
    "                    print(\"skipping because flag is False\")\n",
    "            scored_funcs = []\n",
    "            for func in tqdm(eval_funcs):\n",
    "                local_vars = {}\n",
    "                try:\n",
    "                    exec(func, globals(), local_vars)\n",
    "                except Exception as e:\n",
    "                    print(\"Error executing eval function:\", e)\n",
    "                    continue\n",
    "                if 'evaluate' not in local_vars:\n",
    "                    print(\"skipping because evaluate not in local_vars\")\n",
    "                    continue\n",
    "                eval_func = local_vars['evaluate']\n",
    "                acc = []\n",
    "                for inp, out in filtered_test_cases:\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res_val = eval_func(inp)\n",
    "                    except Exception as e:\n",
    "                        print(\"Error executing eval function:\", e)\n",
    "                        res_val = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    if res_val is None or res_val != out:\n",
    "                        acc.append(0)\n",
    "                    else:\n",
    "                        acc.append(1)\n",
    "                acc = np.mean(acc) if acc else 0\n",
    "                scored_funcs.append((func, acc))\n",
    "            valid_funcs = [each for each in scored_funcs if each[1] >= 0.5]\n",
    "            if not valid_funcs:\n",
    "                print(\"not valid funcs\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\"valid funcs\")\n",
    "            \n",
    "            self.filtered_generated_eval_functions.append({\n",
    "                \"instruction\": result['instruction'],\n",
    "                \"eval_func\": valid_funcs,\n",
    "                \"cases\": filtered_test_cases\n",
    "            })\n",
    "        return self.filtered_generated_eval_functions\n",
    "\n",
    "    def generate_instruction(self, filtered_generated_eval_functions, k=2):\n",
    "        count = 0\n",
    "        filter_count = 0\n",
    "\n",
    "        for line in tqdm(filtered_generated_eval_functions, desc=\"Generating back-translated instructions\"):\n",
    "            funcs = line[\"eval_func\"][:3]\n",
    "\n",
    "            instruction_prompt = f\"\"\"You are an expert in converting the Python eval function code into the corresponding instruction text. I will provide the eval function code. Please strictly follow the code to convert it into the corresponding instruction text. Here's an example: \n",
    "\n",
    "[[\"def evaluate(response):\\n    return 'e' not in response.lower()\", 1.0], [\"def evaluate(response):\\n    words = response.split()\\n    for word in response.split():\\n        if 'e' in word.lower():\\n            return False\\n    return True\", 1.0], [\"def evaluate(response):\\n    return all('e' not in word.lower() for word in response.split())\", 1.0]] \n",
    "\n",
    "[\"Answer without using any words that contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\"] Please convert the following eval function into instructions stored in a list: \n",
    "\n",
    "{funcs}\"\"\"\n",
    "            for _ in range(k):\n",
    "                # input_ids = self.tokenizer.encode(instruction_prompt, return_tensors=\"pt\").cuda()\n",
    "                # outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "                # generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "                # generated_text = self.llm_openai.invoke([{\"role\": \"user\", \"content\": instruction_prompt}]).content\n",
    "                generated_text= call_api(self.llm_openai, [{\"role\": \"user\", \"content\": instruction_prompt}])\n",
    "                try:\n",
    "                    back_instruction = json.loads(generated_text)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    filter_count += 1\n",
    "                    continue\n",
    "            line[\"back_instruction\"] = back_instruction\n",
    "            self.generated_instructions.append(line)\n",
    "            count += 1\n",
    "        return self.generated_instructions\n",
    "            \n",
    "    def filter_generated_instruction(self, generated_instructions):\n",
    "        count = 0 \n",
    "        filter_count = 0\n",
    "\n",
    "        for line in tqdm(generated_instructions, desc=\"Filtering back-translated instructions\"):\n",
    "            back_instructions = line[\"back_instruction\"]\n",
    "            ori_ins = line[\"instruction\"]\n",
    "\n",
    "            nli_scores = []\n",
    "            for back_ins in back_instructions[:3]:\n",
    "                premise = ori_ins\n",
    "                hypothesis = back_ins\n",
    "                inputs = self.tokenizer_nli(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "                output = self.model_nli(inputs[\"input_ids\"].cuda())\n",
    "                prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "                label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "                prediction_dict = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "                max_label = max(prediction_dict, key=prediction_dict.get)\n",
    "                nli_scores.append(max_label)\n",
    "\n",
    "            line[\"nli_scores\"] = nli_scores\n",
    "            if \"contradiction\" in nli_scores:\n",
    "                filter_count += 1\n",
    "                continue\n",
    "            else:\n",
    "                self.filtered_generated_instructions.append(line)\n",
    "            count += 1\n",
    "        return self.filtered_generated_instructions\n",
    "            \n",
    "    def generate_response(self, query, r1_generated_text, filtered_generated_instructions, k=2):\n",
    "        for instruction in filtered_generated_instructions:\n",
    "            prompt = (\n",
    "                f\"Please answer the query strictly following the instruction.\\n\"\n",
    "                f\"[instruction] {instruction['instruction']}\\n\"\n",
    "                f\"[Query] {query}\"\n",
    "            )\n",
    "            responses = []\n",
    "            for _ in range(k):\n",
    "                generated_text = call_api(self.llm_openai, [{\"role\": \"user\", \n",
    "                                                          \"content\": (f\"{r1_generated_text}\\n\"\n",
    "                                                                    f\"Re-write the above text following: {instruction['instruction']}\\n\\n\"\n",
    "                                                                    f\"Note: Use the same words and sentences but re-arrange them in a way that strictly follows the instruction.\\n\")}])\n",
    "                # generated_text = self.llm_openai.invoke([{\"role\": \"user\", \n",
    "                #                                           \"content\": (f\"{r1_generated_text}\\n\"\n",
    "                #                                                     f\"Re-write the above text following: {instruction['instruction']}\\n\\n\"\n",
    "                #                                                     f\"Note: Use the same words and sentences but re-arrange them in a way that strictly follows the instruction.\\n\")}]).content\n",
    "                responses.append(generated_text)\n",
    "            self.generated_responses.append({\n",
    "                \"instruction\": instruction['instruction'],\n",
    "                \"prompt\": prompt,\n",
    "                \"gpt-answer\": responses,\n",
    "                \"eval_func\": instruction[\"eval_func\"],\n",
    "            })\n",
    "        return self.generated_responses\n",
    "              \n",
    "    def filter_generated_response(self, generated_responses):\n",
    "        filtered_samples = []\n",
    "        for result in tqdm(generated_responses, desc=\"Filtering back translated responses\"):\n",
    "            eval_funcs = []\n",
    "            for func, score in result['eval_func']:\n",
    "                local_vars = {}\n",
    "                try:\n",
    "                    exec(func, globals(), local_vars)\n",
    "                except Exception as e:\n",
    "                    print(\"Error executing eval function:\", e)\n",
    "                    continue\n",
    "                if 'evaluate' in local_vars:\n",
    "                    eval_funcs.append(local_vars['evaluate'])\n",
    "            \n",
    "            filter_responses = []\n",
    "            for response in result['gpt-answer']:\n",
    "                acc = []\n",
    "                for eval_func in eval_funcs:\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res = eval_func(response)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        res = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    if res is not None:\n",
    "                        try:\n",
    "                            acc.append(int(res))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                acc = np.mean(acc) if acc else 0\n",
    "                if acc > 0:\n",
    "                    filter_responses.append(response)\n",
    "            \n",
    "            for each in filter_responses:\n",
    "                try:\n",
    "                    query_match = re.findall(r'\\[Query\\](.*)$', result['prompt'], re.DOTALL)\n",
    "                    query = query_match[0].strip() if query_match else \"\"\n",
    "                    filtered_samples.append({\n",
    "                        'instruction': result['instruction'],\n",
    "                        'query': query,\n",
    "                        'response': each,\n",
    "                        'prompt': result['prompt'],\n",
    "                        \"eval_func\": result['eval_func'],\n",
    "                    })\n",
    "                except IndexError:\n",
    "                    print(\"Prompt extraction error:\", result['prompt'])\n",
    "        \n",
    "        self.filtered_generated_responses = list(map(json.loads, set(map(json.dumps, filtered_samples))))\n",
    "        return self.filtered_generated_responses\n",
    "        \n",
    "    def filter2_generated_response(self, filtered_generated_responses, k=2): \n",
    "        prompt_template = (\n",
    "            \"You are an expert that is good at judging whether a response is following the instruction and query.\\n\"\n",
    "            \"[Instruction] {instruction}\\n\"\n",
    "            \"[Query] {query}\\n\"\n",
    "            \"[Response] {response}\\n\"\n",
    "            \"Please notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\\n\"\n",
    "            \"You need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\\n\"\n",
    "            \"Scoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\\n\"\n",
    "            \"Please only provide a score in the format `Score: {{score}}` without any other contents at the last line.\"\n",
    "        )\n",
    "        for each in filtered_generated_responses:\n",
    "            each['gen'] = []\n",
    "            for _ in range(k):\n",
    "                # input_ids = self.tokenizer.encode(each['prompt'], return_tensors=\"pt\").cuda()\n",
    "                # outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "                # generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "                generated_text = call_api(self.llm_openai, [{\"role\": \"user\", \"content\": prompt_template.format(\n",
    "                                                                                            instruction=each['instruction'],\n",
    "                                                                                            query=each['query'],\n",
    "                                                                                            response=each['response']\n",
    "                                                                                        )}])\n",
    "                # generated_text = self.llm_openai.invoke([{\"role\": \"user\", \"content\": each['prompt']}])\n",
    "                each['gen'].append(generated_text)\n",
    "            \n",
    "            scores = []\n",
    "            for gen in each['gen']:\n",
    "                score = re.findall(r'Score: (\\d+?)$', gen)\n",
    "                if score:\n",
    "                    scores.append(int(score[0]))\n",
    "            score = np.mean(scores) if scores else 0\n",
    "            if score > 5: # quality score\n",
    "                self.filtered2_generated_responses.append(each)\n",
    "        return self.filtered2_generated_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Include at least three rhyming pairs',\n",
       "  'query': 'Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$',\n",
       "  'response': 'To find the value of \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\), we start with the operation defined as \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\). \\n\\nFirst, we compute \\\\(1 \\\\otimes 2\\\\):\\n\\\\[\\n1 \\\\otimes 2 = \\\\frac{1^2}{2} = \\\\frac{1}{2}\\n\\\\]\\nNext, we take this result to compute \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\):\\n\\\\[\\n\\\\left(\\\\frac{1}{2}\\\\right) \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n\\\\]\\n\\nNow, we compute \\\\(2 \\\\otimes 3\\\\):\\n\\\\[\\n2 \\\\otimes 3 = \\\\frac{2^2}{3} = \\\\frac{4}{3}\\n\\\\]\\nThen, we use this result to compute \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\):\\n\\\\[\\n1 \\\\otimes \\\\left(\\\\frac{4}{3}\\\\right) = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n\\\\]\\n\\nFinally, we find the difference between the two results:\\n\\\\[\\n\\\\frac{1}{12} - \\\\frac{3}{4} = \\\\frac{1}{12} - \\\\frac{9}{12} = \\\\frac{1 - 9}{12} = \\\\frac{-8}{12} = -\\\\frac{2}{3}\\n\\\\]\\n\\nThus, the answer is \\\\(\\\\boxed{A}\\\\).\\n\\nIn this calculation, we see a rhyme,  \\nWith fractions and operations, all in time.  \\nFrom \\\\(1 \\\\otimes 2\\\\) to \\\\(2 \\\\otimes 3\\\\),  \\nThe results unfold, as clear as can be.  \\n\\nSubtracting the values, we find our way,  \\nTo \\\\(-\\\\frac{2}{3}\\\\), the answer on display.  \\nSo remember this method, it’s quite a delight,  \\nWith operations defined, we’ll always get it right!',\n",
       "  'eval_func': [[\"def evaluate(response):\\n    import re\\n    # Find all words in the response\\n    words = re.findall(r'\\\\b\\\\w+\\\\b', response)\\n    # Create a dictionary to count occurrences of each word\\n    word_count = {}\\n    for word in words:\\n        word_count[word] = word_count.get(word, 0) + 1\\n    # Create a list to hold rhyming pairs\\n    rhymes = []\\n    # Check for rhyming pairs\\n    for word in word_count:\\n        # Check if the last two letters of the word can form a rhyme\\n        rhyme = word[-2:]\\n        # Find other words that rhyme\\n        rhyming_words = [w for w in word_count if w != word and w.endswith(rhyme)]\\n        # If there are at least one other word that rhymes, add to rhymes list\\n        if len(rhyming_words) > 0:\\n            rhymes.append((word, rhyming_words[0]))  # Take the first rhyming word\\n    # Check if we have at least three rhyming pairs\\n    return len(rhymes) >= 3\",\n",
       "    1.0]],\n",
       "  'prompt': 'You are an expert that is good at judging whether a response is following the instruction and query.\\n[Instruction] Include at least three rhyming pairs\\n[Query] Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$\\n[Response] To find the value of \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\), we start with the operation defined as \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\). \\n\\nFirst, we compute \\\\(1 \\\\otimes 2\\\\):\\n\\\\[\\n1 \\\\otimes 2 = \\\\frac{1^2}{2} = \\\\frac{1}{2}\\n\\\\]\\nNext, we take this result to compute \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\):\\n\\\\[\\n\\\\left(\\\\frac{1}{2}\\\\right) \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n\\\\]\\n\\nNow, we compute \\\\(2 \\\\otimes 3\\\\):\\n\\\\[\\n2 \\\\otimes 3 = \\\\frac{2^2}{3} = \\\\frac{4}{3}\\n\\\\]\\nThen, we use this result to compute \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\):\\n\\\\[\\n1 \\\\otimes \\\\left(\\\\frac{4}{3}\\\\right) = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n\\\\]\\n\\nFinally, we find the difference between the two results:\\n\\\\[\\n\\\\frac{1}{12} - \\\\frac{3}{4} = \\\\frac{1}{12} - \\\\frac{9}{12} = \\\\frac{1 - 9}{12} = \\\\frac{-8}{12} = -\\\\frac{2}{3}\\n\\\\]\\n\\nThus, the answer is \\\\(\\\\boxed{A}\\\\).\\n\\nIn this calculation, we see a rhyme,  \\nWith fractions and operations, all in time.  \\nFrom \\\\(1 \\\\otimes 2\\\\) to \\\\(2 \\\\otimes 3\\\\),  \\nThe results unfold, as clear as can be.  \\n\\nSubtracting the values, we find our way,  \\nTo \\\\(-\\\\frac{2}{3}\\\\), the answer on display.  \\nSo remember this method, it’s quite a delight,  \\nWith operations defined, we’ll always get it right!\\nPlease notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\\nYou need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\\nScoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\\nPlease only provide a score in the format `Score: {score}` without any other contents at the last line.',\n",
       "  'gen': ['The response provided correctly follows the instruction and query by calculating the operation defined as \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\) and determining the expression \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\). \\n\\n1. **Calculation Steps**: The response clearly outlines the steps taken to compute \\\\(1 \\\\otimes 2\\\\), \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\), \\\\(2 \\\\otimes 3\\\\), and \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\). Each step is shown with the appropriate mathematical operations, leading to the final result.\\n\\n2. **Final Result**: The final answer is correctly calculated as \\\\(-\\\\frac{2}{3}\\\\), which matches option (A) in the provided choices.\\n\\n3. **Rhyme Requirement**: The response includes three rhyming pairs in the concluding lines, fulfilling the instruction to incorporate rhymes. The rhymes are relevant to the context of the calculations and provide a creative touch to the explanation.\\n\\nOverall, the response is thorough, accurate, and adheres to the requirements of the instruction while effectively answering the query.\\n\\nScore: 10',\n",
       "   'The response provided correctly follows the instruction and query. It begins by clearly defining the operation \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\) and then systematically computes the required values step by step. \\n\\n1. **Calculation of \\\\(1 \\\\otimes 2\\\\)**: The response correctly computes this as \\\\(\\\\frac{1}{2}\\\\).\\n2. **Calculation of \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\)**: The response accurately calculates this as \\\\(\\\\frac{1}{12}\\\\).\\n3. **Calculation of \\\\(2 \\\\otimes 3\\\\)**: This is computed correctly as \\\\(\\\\frac{4}{3}\\\\).\\n4. **Calculation of \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\)**: The response correctly computes this as \\\\(\\\\frac{3}{4}\\\\).\\n5. **Final subtraction**: The response correctly finds the difference \\\\(\\\\frac{1}{12} - \\\\frac{3}{4}\\\\) and simplifies it to \\\\(-\\\\frac{2}{3}\\\\).\\n\\nThe response also includes three rhyming pairs in the additional commentary, fulfilling the instruction to include at least three rhymes. The final answer is presented in the required boxed format, and the response is coherent and logically structured.\\n\\nGiven that the response is accurate, follows the query closely, and meets the instruction requirements, I would rate it highly.\\n\\nScore: 10']},\n",
       " {'instruction': 'Respond with a six-word story',\n",
       "  'query': 'Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$',\n",
       "  'response': 'Compute operation, find difference, confirm answer.',\n",
       "  'eval_func': [['def evaluate(response):\\n    return len(response.split()) == 6',\n",
       "    1.0]],\n",
       "  'prompt': 'You are an expert that is good at judging whether a response is following the instruction and query.\\n[Instruction] Respond with a six-word story\\n[Query] Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$\\n[Response] Compute operation, find difference, confirm answer.\\nPlease notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\\nYou need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\\nScoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\\nPlease only provide a score in the format `Score: {score}` without any other contents at the last line.',\n",
       "  'gen': ['The instruction clearly asks for a six-word story, while the query involves a mathematical operation and its evaluation. The response provided does not adhere to the instruction, as it does not present a six-word story but instead describes a process of computation related to the query. \\n\\nThe response fails to fulfill the requirement of being a six-word story, which is a specific and creative task, and instead focuses on a mathematical explanation. Therefore, it does not meet the criteria set by the instruction.\\n\\nScore: 0',\n",
       "   'The instruction clearly asks for a six-word story, while the query involves a mathematical operation and its evaluation. The response provided does not adhere to the instruction, as it does not present a six-word story but instead describes a process of computation related to the query. \\n\\nThe response fails to fulfill the requirement of being a six-word story, which is a specific and creative task, and instead focuses on a mathematical explanation. Therefore, it does not meet the criteria set by the instruction.\\n\\nScore: 0']},\n",
       " {'instruction': 'Include at least three rhyming pairs',\n",
       "  'query': 'Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$',\n",
       "  'response': 'To find the value of \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\), we start with the operation defined as \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\). \\n\\nFirst, we compute \\\\(1 \\\\otimes 2\\\\):\\n\\\\[\\n1 \\\\otimes 2 = \\\\frac{1^2}{2} = \\\\frac{1}{2}\\n\\\\]\\nNext, we take this result to compute \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\):\\n\\\\[\\n\\\\left(\\\\frac{1}{2}\\\\right) \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n\\\\]\\n\\nNow, we compute \\\\(2 \\\\otimes 3\\\\):\\n\\\\[\\n2 \\\\otimes 3 = \\\\frac{2^2}{3} = \\\\frac{4}{3}\\n\\\\]\\nThen, we use this result to compute \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\):\\n\\\\[\\n1 \\\\otimes \\\\left(\\\\frac{4}{3}\\\\right) = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n\\\\]\\n\\nFinally, we find the difference between the two results:\\n\\\\[\\n\\\\frac{1}{12} - \\\\frac{3}{4} = \\\\frac{1}{12} - \\\\frac{9}{12} = \\\\frac{1 - 9}{12} = \\\\frac{-8}{12} = -\\\\frac{2}{3}\\n\\\\]\\n\\nThus, the answer is \\\\(\\\\boxed{A}\\\\).\\n\\nIn this calculation, we see a rhyme,  \\nWith fractions and operations, all in time.  \\nFrom \\\\(1 \\\\otimes 2\\\\) to \\\\(2 \\\\otimes 3\\\\),  \\nThe results unfold, as clear as can be.  \\n\\nSubtracting the values, we find our way,  \\nTo \\\\(-\\\\frac{2}{3}\\\\), the answer on display.  \\nSo with each step, we’ve followed the track,  \\nAnd option A is the answer we’ll stack.',\n",
       "  'eval_func': [[\"def evaluate(response):\\n    import re\\n    # Find all words in the response\\n    words = re.findall(r'\\\\b\\\\w+\\\\b', response)\\n    # Create a dictionary to count occurrences of each word\\n    word_count = {}\\n    for word in words:\\n        word_count[word] = word_count.get(word, 0) + 1\\n    # Create a list to hold rhyming pairs\\n    rhymes = []\\n    # Check for rhyming pairs\\n    for word in word_count:\\n        # Check if the last two letters of the word can form a rhyme\\n        rhyme = word[-2:]\\n        # Find other words that rhyme\\n        rhyming_words = [w for w in word_count if w != word and w.endswith(rhyme)]\\n        # If there are at least one other word that rhymes, add to rhymes list\\n        if len(rhyming_words) > 0:\\n            rhymes.append((word, rhyming_words[0]))  # Take the first rhyming word\\n    # Check if we have at least three rhyming pairs\\n    return len(rhymes) >= 3\",\n",
       "    1.0]],\n",
       "  'prompt': 'You are an expert that is good at judging whether a response is following the instruction and query.\\n[Instruction] Include at least three rhyming pairs\\n[Query] Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$\\n[Response] To find the value of \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\), we start with the operation defined as \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\). \\n\\nFirst, we compute \\\\(1 \\\\otimes 2\\\\):\\n\\\\[\\n1 \\\\otimes 2 = \\\\frac{1^2}{2} = \\\\frac{1}{2}\\n\\\\]\\nNext, we take this result to compute \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\):\\n\\\\[\\n\\\\left(\\\\frac{1}{2}\\\\right) \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n\\\\]\\n\\nNow, we compute \\\\(2 \\\\otimes 3\\\\):\\n\\\\[\\n2 \\\\otimes 3 = \\\\frac{2^2}{3} = \\\\frac{4}{3}\\n\\\\]\\nThen, we use this result to compute \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\):\\n\\\\[\\n1 \\\\otimes \\\\left(\\\\frac{4}{3}\\\\right) = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n\\\\]\\n\\nFinally, we find the difference between the two results:\\n\\\\[\\n\\\\frac{1}{12} - \\\\frac{3}{4} = \\\\frac{1}{12} - \\\\frac{9}{12} = \\\\frac{1 - 9}{12} = \\\\frac{-8}{12} = -\\\\frac{2}{3}\\n\\\\]\\n\\nThus, the answer is \\\\(\\\\boxed{A}\\\\).\\n\\nIn this calculation, we see a rhyme,  \\nWith fractions and operations, all in time.  \\nFrom \\\\(1 \\\\otimes 2\\\\) to \\\\(2 \\\\otimes 3\\\\),  \\nThe results unfold, as clear as can be.  \\n\\nSubtracting the values, we find our way,  \\nTo \\\\(-\\\\frac{2}{3}\\\\), the answer on display.  \\nSo with each step, we’ve followed the track,  \\nAnd option A is the answer we’ll stack.\\nPlease notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\\nYou need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\\nScoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\\nPlease only provide a score in the format `Score: {score}` without any other contents at the last line.',\n",
       "  'gen': ['The response provided correctly follows the instruction to solve the mathematical operation defined by \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\) and computes the expression \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\). The calculations are accurate, and the final answer is correctly identified as \\\\(-\\\\frac{2}{3}\\\\), which corresponds to option (A).\\n\\nHowever, the instruction also required the inclusion of at least three rhyming pairs. The response does include some rhyming lines at the end, but it does not clearly present three distinct rhyming pairs. The rhymes are somewhat forced and do not contribute significantly to the clarity or quality of the mathematical explanation.\\n\\nIn summary, while the response effectively answers the query and provides the correct calculations, it falls short of fully meeting the instruction regarding the rhyming pairs. Therefore, I would rate the response as follows:\\n\\nScore: 7',\n",
       "   'The response provided correctly follows the instruction and query by calculating the operation defined as \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\) and determining the expression \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\). \\n\\n1. **Correctness of Calculations**: \\n   - The response accurately computes \\\\(1 \\\\otimes 2\\\\) as \\\\(\\\\frac{1}{2}\\\\).\\n   - It then correctly computes \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\) as \\\\(\\\\frac{1}{12}\\\\).\\n   - The calculation of \\\\(2 \\\\otimes 3\\\\) as \\\\(\\\\frac{4}{3}\\\\) is also correct.\\n   - Finally, it computes \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\) as \\\\(\\\\frac{3}{4}\\\\) and finds the difference correctly, arriving at \\\\(-\\\\frac{2}{3}\\\\).\\n\\n2. **Rhyme Requirement**: \\n   - The response includes three rhyming pairs in the poetic section at the end, fulfilling the instruction to include at least three rhyming pairs.\\n\\n3. **Clarity and Structure**: \\n   - The response is well-structured, with clear steps that lead to the final answer. The calculations are presented logically, making it easy to follow.\\n\\n4. **Final Answer**: \\n   - The response concludes with the correct answer, \\\\(\\\\boxed{A}\\\\), which corresponds to the calculated result.\\n\\nOverall, the response is highly relevant to the query, provides a detailed solution, and meets the instruction requirements effectively.\\n\\nScore: 10']},\n",
       " {'instruction': 'Write the response in future tense only',\n",
       "  'query': 'Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$',\n",
       "  'response': 'To determine the value of \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\) where the operation \\\\(\\\\otimes\\\\) will be defined by \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\), we will proceed as follows:\\n\\nFirst, we will compute \\\\(1 \\\\otimes 2\\\\):\\n\\\\[\\n1 \\\\otimes 2 = \\\\frac{1^2}{2} = \\\\frac{1}{2}\\n\\\\]\\nNext, we will use this result to compute \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\):\\n\\\\[\\n\\\\left(\\\\frac{1}{2}\\\\right) \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n\\\\]\\n\\nNow, we will compute \\\\(2 \\\\otimes 3\\\\):\\n\\\\[\\n2 \\\\otimes 3 = \\\\frac{2^2}{3} = \\\\frac{4}{3}\\n\\\\]\\nThen, we will use this result to compute \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\):\\n\\\\[\\n1 \\\\otimes \\\\left(\\\\frac{4}{3}\\\\right) = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n\\\\]\\n\\nFinally, we will find the difference between the two results:\\n\\\\[\\n\\\\frac{1}{12} - \\\\frac{3}{4} = \\\\frac{1}{12} - \\\\frac{9}{12} = \\\\frac{1 - 9}{12} = \\\\frac{-8}{12} = -\\\\frac{2}{3}\\n\\\\]\\n\\nThus, the answer will be \\\\(\\\\boxed{A}\\\\).',\n",
       "  'eval_func': [[\"def evaluate(response):\\n    future_tense_keywords = ['will', 'shall', 'is going to', 'will be', 'is about to']\\n    # Check if the response contains any future tense keywords\\n    if any(keyword in response for keyword in future_tense_keywords):\\n        return True\\n    # Check if the response is entirely in future tense\\n    words = response.split()\\n    for word in words:\\n        if word.lower() not in future_tense_keywords and not word.endswith('ing'):\\n            return False\\n    return True\",\n",
       "    1.0]],\n",
       "  'prompt': 'You are an expert that is good at judging whether a response is following the instruction and query.\\n[Instruction] Write the response in future tense only\\n[Query] Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$\\n[Response] To determine the value of \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\) where the operation \\\\(\\\\otimes\\\\) will be defined by \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\), we will proceed as follows:\\n\\nFirst, we will compute \\\\(1 \\\\otimes 2\\\\):\\n\\\\[\\n1 \\\\otimes 2 = \\\\frac{1^2}{2} = \\\\frac{1}{2}\\n\\\\]\\nNext, we will use this result to compute \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\):\\n\\\\[\\n\\\\left(\\\\frac{1}{2}\\\\right) \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n\\\\]\\n\\nNow, we will compute \\\\(2 \\\\otimes 3\\\\):\\n\\\\[\\n2 \\\\otimes 3 = \\\\frac{2^2}{3} = \\\\frac{4}{3}\\n\\\\]\\nThen, we will use this result to compute \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\):\\n\\\\[\\n1 \\\\otimes \\\\left(\\\\frac{4}{3}\\\\right) = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n\\\\]\\n\\nFinally, we will find the difference between the two results:\\n\\\\[\\n\\\\frac{1}{12} - \\\\frac{3}{4} = \\\\frac{1}{12} - \\\\frac{9}{12} = \\\\frac{1 - 9}{12} = \\\\frac{-8}{12} = -\\\\frac{2}{3}\\n\\\\]\\n\\nThus, the answer will be \\\\(\\\\boxed{A}\\\\).\\nPlease notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\\nYou need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\\nScoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\\nPlease only provide a score in the format `Score: {score}` without any other contents at the last line.',\n",
       "  'gen': ['The response provided does not adhere to the instruction to write in future tense only. The entire explanation is written in the present tense, detailing the steps taken to compute the values and the final result. The instruction specifically requires that the response be framed in future tense, which means it should describe what will happen or what will be computed in the future, rather than what is currently being computed.\\n\\nThe response does correctly follow the mathematical operations required by the query and arrives at the correct answer, but it fails to meet the primary instruction regarding tense. Therefore, while the content is relevant and accurate, it does not fulfill the requirement of using future tense.\\n\\nGiven this analysis, I would rate the response as follows:\\n\\nScore: 3',\n",
       "   'The response provided does not adhere to the instruction to write in future tense only. The entire explanation is presented in the present tense, detailing the calculations step by step without any indication of future actions or outcomes. \\n\\nThe query asks for a specific mathematical operation to be evaluated, and while the response does correctly compute the values and arrive at a conclusion, it fails to follow the instruction regarding tense. The calculations are accurate, and the final answer is correctly boxed, but the tense requirement is a critical aspect of the instruction that has not been met.\\n\\nGiven this analysis, the response is somewhat helpful in terms of content but completely fails to follow the tense instruction, which is a significant oversight.\\n\\nScore: 2']}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][\"filtered_generated_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoif = AutoIf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train'].select(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please answer the query strictly following the instruction.\n",
      "[instruction] Incorporate a famous movie quote seamlessly into your answer\n",
      "[Query] Return your final response within \\boxed{}. Doug constructs a square window using $8$ equal-size panes of glass, as shown. The ratio of the height to width for each pane is $5 : 2$, and the borders around and between the panes are $2$ inches wide. In inches, what is the side length of the square window?\n",
      "[asy] fill((0,0)--(2,0)--(2,26)--(0,26)--cycle,gray); fill((6,0)--(8,0)--(8,26)--(6,26)--cycle,gray); fill((12,0)--(14,0)--(14,26)--(12,26)--cycle,gray); fill((18,0)--(20,0)--(20,26)--(18,26)--cycle,gray); fill((24,0)--(26,0)--(26,26)--(24,26)--cycle,gray); fill((0,0)--(26,0)--(26,2)--(0,2)--cycle,gray); fill((0,12)--(26,12)--(26,14)--(0,14)--cycle,gray); fill((0,24)--(26,24)--(26,26)--(0,26)--cycle,gray); [/asy]\n",
      "$\\textbf{(A)}\\ 26\\qquad\\textbf{(B)}\\ 28\\qquad\\textbf{(C)}\\ 30\\qquad\\textbf{(D)}\\ 32\\qquad\\textbf{(E)}\\ 34$\n"
     ]
    }
   ],
   "source": [
    "print(x[0][\"filtered2_generated_responses\"][0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doug constructs a square window using 8 equal-size panes of glass, with a height to width ratio of 5:2 for each pane. The borders around and between the panes are 2 inches wide. We need to determine the side length of the square window.\n",
      "\n",
      "1. **Arrangement of Panes**: The panes are arranged in 2 rows and 4 columns. This arrangement ensures the window is square. As Yoda would say, \"Do or do not, there is no try,\" and in this case, we will do the calculations.\n",
      "\n",
      "2. **Dimensions of Each Pane**: Let the width of each pane be \\(2k\\) and the height be \\(5k\\). The ratio of height to width is \\(5:2\\).\n",
      "\n",
      "3. **Total Width Calculation**:\n",
      "   - Left border: 2 inches\n",
      "   - Right border: 2 inches\n",
      "   - 4 panes: \\(4 \\times 2k\\)\n",
      "   - 3 borders between panes: \\(3 \\times 2\\) inches\n",
      "   - Total width: \\(2 + 2 + 4 \\times 2k + 3 \\times 2 = 10 + 8k\\)\n",
      "\n",
      "4. **Total Height Calculation**:\n",
      "   - Top border: 2 inches\n",
      "   - Bottom border: 2 inches\n",
      "   - 2 panes: \\(2 \\times 5k\\)\n",
      "   - 1 border between panes: 2 inches\n",
      "   - Total height: \\(2 + 2 + 2 \\times 5k + 2 = 6 + 10k\\)\n",
      "\n",
      "5. **Equating Width and Height**:\n",
      "   - Since the window is square, total width equals total height:\n",
      "     \\[\n",
      "     10 + 8k = 6 + 10k\n",
      "     \\]\n",
      "   - Solving for \\(k\\):\n",
      "     \\[\n",
      "     10 - 6 = 10k - 8k \\implies 4 = 2k \\implies k = 2\n",
      "     \\]\n",
      "\n",
      "6. **Calculating Dimensions**:\n",
      "   - Width of each pane: \\(2k = 4\\) inches\n",
      "   - Height of each pane: \\(5k = 10\\) inches\n",
      "   - Total width: \\(10 + 8 \\times 2 = 10 + 16 = 26\\) inches\n",
      "   - Total height: \\(6 + 10 \\times 2 = 6 + 20 = 26\\) inches\n",
      "\n",
      "Thus, the side length of the square window is \\(\\boxed{A}\\). In the words of a classic film, \"You can't handle the truth!\" But the truth here is clear: the answer is indeed 26 inches.\n"
     ]
    }
   ],
   "source": [
    "print(x[1][\"filtered2_generated_responses\"][0][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: unterminated string literal (detected at line 2) (<string>, line 2)\n",
      "Error executing eval function: unterminated string literal (detected at line 3) (<string>, line 3)\n",
      "Error executing eval function: unterminated string literal (detected at line 2) (<string>, line 2)\n",
      "skipping because of JSON load error: Expecting value: line 1 column 163 (char 162)\n",
      "Error executing eval function: unterminated string literal (detected at line 4) (<string>, line 4)\n",
      "Error executing eval function: closing parenthesis '}' does not match opening parenthesis '(' (<string>, line 2)\n",
      "['    import re']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4500.33it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7214.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3603.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4419.71it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11781.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6087.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6269.51it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11224.72it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9724.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7943.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11428.62it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 20068.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10255.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6000.43it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7830.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6061.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6213.78it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9716.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6034.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14997.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8456.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8774.69it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14580.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7653.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8305.55it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12722.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8240.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6297.75it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8427.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6123.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12396.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7639.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5229.81it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6064.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4476.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4293.04it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 5877.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5949.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3887.21it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6269.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4771.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4832.15it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6978.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5171.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8224.13it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 19448.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 9238.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5035.18it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7923.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5974.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9446.63it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9439.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8577.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10381.94it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 18444.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9320.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3953.16it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 5887.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4006.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3160.74it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 4627.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5071.71it/s]\n",
      " 61%|██████    | 22/36 [00:00<00:00, 206.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4675.92it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6584.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4563.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5841.65it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12409.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7653.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7884.03it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12839.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7108.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8272.79it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12087.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6061.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5915.80it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8949.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5667.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12446.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Expecting value: line 1 column 163 (char 162)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6034.97it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9799.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6141.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8830.11it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14027.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7489.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5932.54it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7989.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7810.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5468.45it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6743.25it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4301.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6462.72it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7345.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5229.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8542.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: closing parenthesis '}' does not match opening parenthesis '(' (<string>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6574.14it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14106.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8224.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7371.36it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10773.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7049.25it/s]\n",
      "100%|██████████| 36/36 [00:00<00:00, 212.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating back-translated instructions: 100%|██████████| 32/32 [00:31<00:00,  1.01it/s]\n",
      "Filtering back-translated instructions: 100%|██████████| 32/32 [00:00<00:00, 62.76it/s]\n",
      "Filtering back translated responses: 100%|██████████| 24/24 [00:00<00:00, 4867.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: unterminated string literal (detected at line 2) (<string>, line 2)\n",
      "Error executing eval function: unterminated string literal (detected at line 3) (<string>, line 3)\n",
      "Error executing eval function: unterminated string literal (detected at line 2) (<string>, line 2)\n",
      "skipping because of JSON load error: Expecting value: line 1 column 163 (char 162)\n",
      "Error executing eval function: unterminated string literal (detected at line 4) (<string>, line 4)\n",
      "Error executing eval function: closing parenthesis '}' does not match opening parenthesis '(' (<string>, line 2)\n",
      "Error executing eval function: No module named 'emoji'\n",
      "Error executing eval function: unterminated string literal (detected at line 2) (<string>, line 2)\n",
      "Error executing eval function: unterminated string literal (detected at line 3) (<string>, line 3)\n",
      "Error executing eval function: unterminated string literal (detected at line 2) (<string>, line 2)\n",
      "Skipping because of index error: list index out of range\n",
      "skipping because of JSON load error: Expecting value: line 1 column 170 (char 169)\n",
      "Error executing eval function: unterminated string literal (detected at line 4) (<string>, line 4)\n",
      "skipping because of JSON load error: Expecting value: line 1 column 164 (char 163)\n",
      "['    import re']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4391.94it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11683.30it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3765.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5197.40it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 16008.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8305.55it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14315.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 7639.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7516.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12288.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8507.72it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14463.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7570.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5785.25it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8224.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6087.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6087.52it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9656.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5841.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 15160.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6288.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6452.78it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10941.66it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5817.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6944.21it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11376.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6141.00it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7269.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5461.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7781.64it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10837.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7710.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4899.89it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7507.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5533.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5440.08it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7210.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5753.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4275.54it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7653.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5236.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6114.15it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7298.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5833.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9799.78it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 18613.78it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5777.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3945.72it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 4549.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5924.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5675.65it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 14364.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8128.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9962.72it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 19134.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9300.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3823.43it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 4391.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3823.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4922.89it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6410.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]\n",
      " 31%|███       | 22/72 [00:00<00:00, 215.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5398.07it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8416.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5660.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10951.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5769.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6204.59it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9861.22it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6978.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7738.57it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11554.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4899.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5629.94it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6936.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6831.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7612.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Expecting value: line 1 column 163 (char 162)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4148.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7667.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4744.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7839.82it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10875.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6413.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5282.50it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7092.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6150.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4739.33it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7223.26it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4364.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4987.28it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7302.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5127.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7626.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: closing parenthesis '}' does not match opening parenthesis '(' (<string>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9404.27it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 15592.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8456.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7898.88it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11781.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8112.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7884.03it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12169.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5785.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4588.95it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8536.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3685.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2968.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8701.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5433.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6260.16it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8671.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6374.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9686.61it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 16556.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 8422.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5184.55it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7157.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4728.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4686.37it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6750.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4815.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7025.63it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11748.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7384.34it/s]\n",
      " 61%|██████    | 44/72 [00:00<00:00, 215.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7025.63it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11125.47it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6523.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1366.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: No module named 'emoji'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5210.32it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6553.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 5090.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 6842.26it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9861.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 6223.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4240.95it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6925.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 4341.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4452.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 4644.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " name 'count_syllables' is not defined\n",
      "skipping because flag is False\n",
      "Error executing eval function: name 'count_syllables' is not defined\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4826.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4877.10it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 5123.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3336.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3754.97it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 5212.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 3701.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5178.15it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11076.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4793.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2896.62it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 5356.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2987.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1599.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping because of index error: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5146.39it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 4867.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 5419.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2227.46it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 2996.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 3294.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4471.54it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 5150.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5223.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5698.78it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7061.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5035.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6678.83it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9686.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6186.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6853.44it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10547.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5991.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12520.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Expecting value: line 1 column 170 (char 169)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6647.07it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9467.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 6087.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3072.75it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3869.28it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2849.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5077.85it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6978.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4750.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6786.90it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10082.46it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 5957.82it/s]\n",
      " 92%|█████████▏| 66/72 [00:00<00:00, 200.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4963.67it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6156.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5890.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4534.38it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7239.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4324.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5146.39it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7494.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5518.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13357.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Expecting value: line 1 column 164 (char 163)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6775.94it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10903.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 6689.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6452.78it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8726.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6232.25it/s]\n",
      "100%|██████████| 72/72 [00:00<00:00, 203.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating back-translated instructions: 100%|██████████| 94/94 [01:34<00:00,  1.00s/it]\n",
      "Filtering back-translated instructions: 100%|██████████| 126/126 [00:02<00:00, 62.01it/s]\n",
      "Filtering back translated responses: 100%|██████████| 143/143 [00:00<00:00, 7680.89it/s]\n"
     ]
    }
   ],
   "source": [
    "x = autoif.compile(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response provided does follow the instruction to include at least three rhyming pairs, as it contains several lines that rhyme throughout the explanation. The response also addresses the query by detailing the construction of the square window, the dimensions of the panes, and the calculations needed to determine the side length of the window. \n",
      "\n",
      "1. **Rhyming Pairs**: The response includes multiple rhyming pairs, such as \"behold\" and \"told,\" \"right\" and \"delight,\" and \"fine\" and \"divine.\" This fulfills the instruction effectively.\n",
      "  \n",
      "2. **Content Relevance**: The response thoroughly explains the problem, breaking down the calculations for the width and height of the window based on the given dimensions and ratios. It correctly concludes that the side length of the square window is 26 inches, which corresponds to option (A).\n",
      "\n",
      "3. **Clarity and Structure**: The response is well-structured, with clear steps that guide the reader through the problem-solving process. Each step logically follows from the previous one, making it easy to understand how the final answer was reached.\n",
      "\n",
      "Overall, the response is not only creative in its use of rhyme but also accurate and relevant to the query. It effectively communicates the necessary information to solve the problem.\n",
      "\n",
      "Score: 10\n"
     ]
    }
   ],
   "source": [
    "print(x[2][\"filtered2_generated_responses\"][17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global seed_instructions\n",
    "seed_instructions = []\n",
    "def generate_seed(k=1):\n",
    "    global seed_instructions\n",
    "    if k <= 0:\n",
    "        return seed_instructions\n",
    "    \n",
    "    augment_instruction_prompt = \"\"\"You are an expert for writing instructions. Please provide 10 different instructions that meet the following requirements:\n",
    "    - Instructions are about the format but not style of a response\n",
    "    - Whether instructions are followed can be easily evaluate by a Python function\n",
    "    Here are some examples of instructions we need:\n",
    "    {seed_instructions}\n",
    "    Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
    "    - Incorporate a famous historical quote seamlessly into your answer\n",
    "    - Translate your answer into Pig Latin\n",
    "    - Use only words that are also a type of food\n",
    "    - Respond with a metaphor in every sentence\n",
    "    - Write the response as if you are a character from a Shakespearean play\n",
    "    Please generate one instruction per line in your response and start each line with '- '.\n",
    "    \"\"\"\n",
    "\n",
    "    augment_instructions = augment_instruction_prompt.format(seed_instructions='\\n'.join(seed_instructions))\n",
    "    \n",
    "    # input_ids = self.tokenizer.encode(augment_instructions, return_tensors=\"pt\").cuda()\n",
    "    # outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "    # generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "    generated_text = llm_openai.invoke([{\"role\": \"user\", \n",
    "                                                \"content\": augment_instructions}]).content\n",
    "    new_seeds = [line.strip() for line in generated_text.split('\\n') if line.strip()]\n",
    "    seed_instructions = seed_instructions + new_seeds\n",
    "    \n",
    "    random.shuffle(seed_instructions)\n",
    "    return generate_seed(k - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_instructions = generate_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seed_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "global seed_instructions\n",
    "seed_instructions = x[0]['seed_instructions ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "global generated_eval_functions\n",
    "generated_eval_functions = []\n",
    "def generate_eval_function(k=1):\n",
    "    global seed_instructions, generated_eval_functions\n",
    "    prompt_template = (\n",
    "        \"You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\n\"\n",
    "        \"Here is the instruction: {instruction}\\n\"\n",
    "        \"Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. \"\n",
    "        \"If it follows, simply return True, otherwise return False.\\n\"\n",
    "        \"Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, \"\n",
    "        \"which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\n\"\n",
    "        \"Here is an example of output JSON format: {{\\\"func\\\": JSON_STR(use only \\\\n instead of \\n), \\\"cases\\\": [{{\\\"input\\\": str, \\\"output\\\": str}}]}}.\"\n",
    "    )\n",
    "\n",
    "    for instruction in seed_instructions:\n",
    "        prompt = prompt_template.format(instruction=instruction)\n",
    "        generated_eval_functions.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"instruction\": instruction,\n",
    "            \"gpt-answer\": []\n",
    "        })\n",
    "        for _ in range(k):\n",
    "            # input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "            # outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "            # generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "            generated_text = llm_openai.invoke([{\"role\": \"user\", \"content\": prompt}]).content\n",
    "            generated_eval_functions[-1]['gpt-answer'].append(generated_text)\n",
    "        print(\"len(generated_eval_functions[-1]['gpt-answer']):\", len(generated_eval_functions[-1]['gpt-answer']))\n",
    "    return generated_eval_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n",
      "len(generated_eval_functions[-1]['gpt-answer']): 6\n"
     ]
    }
   ],
   "source": [
    "generated_eval_functions = generate_eval_function(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not any(letter * 2 in word for letter in set(word)):\\\\n\\n      return False\\\\n\\n  return True\",\\n  \"cases\": [\\n    {\"input\": \"bookkeeper\", \"output\": true},\\n    {\"input\": \"hello world\", \"output\": false},\\n    {\"input\": \"bubble doodle\", \"output\": true}\\n  ]\\n}\\n```',\n",
       " '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not any(letter * 2 in word for letter in set(word)):\\\\n\\n      return False\\\\n\\n  return True\",\\n  \"cases\": [\\n    {\"input\": \"bookkeeper\", \"output\": true},\\n    {\"input\": \"hello world\", \"output\": false},\\n    {\"input\": \"bubble doodle\", \"output\": true}\\n  ]\\n}\\n```',\n",
       " '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not any(letter * 2 in word for letter in set(word)):\\\\n\\n      return False\\\\n\\n  return True\",\\n  \"cases\": [\\n    {\"input\": \"bookkeeper\", \"output\": true},\\n    {\"input\": \"hello world\", \"output\": false},\\n    {\"input\": \"bubble doodle\", \"output\": true}\\n  ]\\n}\\n```',\n",
       " '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not any(letter * 2 in word for letter in set(word)):\\\\n\\n      return False\\\\n\\n  return True\",\\n  \"cases\": [\\n    {\"input\": \"bookkeeper\", \"output\": true},\\n    {\"input\": \"hello world\", \"output\": false},\\n    {\"input\": \"bubble doodle\", \"output\": true}\\n  ]\\n}\\n```',\n",
       " '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not any(letter * 2 in word for letter in set(word)):\\\\n\\n      return False\\\\n\\n  return True\",\\n  \"cases\": [\\n    {\"input\": \"bookkeeper\", \"output\": true},\\n    {\"input\": \"hello world\", \"output\": false},\\n    {\"input\": \"bubble doodle\", \"output\": true}\\n  ]\\n}\\n```',\n",
       " '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not any(letter * 2 in word for letter in set(word)):\\\\n\\n      return False\\\\n\\n  return True\",\\n  \"cases\": [\\n    {\"input\": \"bookkeeper\", \"output\": true},\\n    {\"input\": \"hello world\", \"output\": false},\\n    {\"input\": \"bubble doodle\", \"output\": true}\\n  ]\\n}\\n```']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_eval_functions[5]['gpt-answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/share/u/harshraj/CotIF/data-v2/generated_eval_functions.json\", \"w\") as f:\n",
    "#     json.dump(generated_eval_functions, f, indent=4)\n",
    "\n",
    "with open(\"/share/u/harshraj/CotIF/data-v2/generated_eval_functions.json\", \"r\") as f:\n",
    "    generated_eval_functions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\nHere is the instruction: Answer with words that begin with the letter \\'B\\'\\nPlease write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. If it follows, simply return True, otherwise return False.\\nPlease response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\nHere is an example of output JSON format: {\"func\": JSON_STR(use only \\\\n instead of \\n), \"cases\": [{\"input\": str, \"output\": str}]}.',\n",
       " 'instruction': \"Answer with words that begin with the letter 'B'\",\n",
       " 'gpt-answer': ['```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not word.startswith(\\'B\\'):\\\\n\\n      return False\\\\n\\n  return True\\n\",\\n  \"cases\": [\\n    {\"input\": \"Beautiful birds bounce\", \"output\": true},\\n    {\"input\": \"Boldly going beyond\", \"output\": true},\\n    {\"input\": \"Amazing beings bounce\", \"output\": false}\\n  ]\\n}\\n```',\n",
       "  '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not word.startswith(\\'B\\'):\\\\n\\n      return False\\\\n\\n  return True\\n\",\\n  \"cases\": [\\n    {\"input\": \"Beautiful birds bounce\", \"output\": true},\\n    {\"input\": \"Boldly going beyond\", \"output\": true},\\n    {\"input\": \"Amazing beings bounce\", \"output\": false}\\n  ]\\n}\\n```',\n",
       "  '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not word.startswith(\\'B\\'):\\\\n\\n      return False\\\\n\\n  return True\\n\",\\n  \"cases\": [\\n    {\"input\": \"Beautiful birds bounce\", \"output\": true},\\n    {\"input\": \"Boldly going beyond\", \"output\": true},\\n    {\"input\": \"Amazing beings bounce\", \"output\": false}\\n  ]\\n}\\n```',\n",
       "  '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not word.startswith(\\'B\\'):\\\\n\\n      return False\\\\n\\n  return True\\n\",\\n  \"cases\": [\\n    {\"input\": \"Beautiful birds bounce\", \"output\": true},\\n    {\"input\": \"Boldly going beyond\", \"output\": true},\\n    {\"input\": \"Amazing beings bounce\", \"output\": false}\\n  ]\\n}\\n```',\n",
       "  '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not word.startswith(\\'B\\'):\\\\n\\n      return False\\\\n\\n  return True\\n\",\\n  \"cases\": [\\n    {\"input\": \"Beautiful birds bounce\", \"output\": true},\\n    {\"input\": \"Boldly going beyond\", \"output\": true},\\n    {\"input\": \"Amazing beings bounce\", \"output\": false}\\n  ]\\n}\\n```',\n",
       "  '```json\\n{\\n  \"func\": \"\\ndef evaluate(response):\\\\n\\n  words = response.split()\\\\n\\n  for word in words:\\\\n\\n    if not word.startswith(\\'B\\'):\\\\n\\n      return False\\\\n\\n  return True\\n\",\\n  \"cases\": [\\n    {\"input\": \"Beautiful birds bounce\", \"output\": true},\\n    {\"input\": \"Boldly going beyond\", \"output\": true},\\n    {\"input\": \"Amazing beings bounce\", \"output\": false}\\n  ]\\n}\\n```']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_eval_functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_eval_functions[0]['gpt-answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "Error executing eval function: unterminated string literal (detected at line 11) (<string>, line 11)\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "Error executing eval function: unterminated string literal (detected at line 11) (<string>, line 11)\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "skipping because of JSON load error: name 'true' is not defined\n",
      "['    import xml.etree.ElementTree as ET', '        import json']\n"
     ]
    }
   ],
   "source": [
    "collect_packages = []\n",
    "count = 0\n",
    "for result in generated_eval_functions:\n",
    "    res = result['gpt-answer']\n",
    "    eval_funcs, test_cases = [], []\n",
    "    for each in res:\n",
    "        try:\n",
    "            json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "        except Exception as e:\n",
    "            count += 1\n",
    "            print(\"Skipping because of index error:\", e)\n",
    "            continue\n",
    "        try:\n",
    "            res_dict = eval(json_dict)\n",
    "        except Exception as e:\n",
    "            count += 1\n",
    "            print(\"skipping because of JSON load error:\", e)\n",
    "            continue\n",
    "        func = res_dict['func']\n",
    "        if '\\\\n' in func:\n",
    "            func = func.replace('\\\\n', '\\n')\n",
    "        try:\n",
    "            exec(func)\n",
    "        except Exception as e:\n",
    "            count += 1\n",
    "            print(\"Error executing eval function:\", e)\n",
    "            continue\n",
    "        for line in func.split('\\n'):\n",
    "            if 'import' in line or 'download' in line or 'requests' in line:\n",
    "                collect_packages.append(line)\n",
    "print(list(set(collect_packages)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_generated_eval_functions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Function execution timed out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 2 column 68 (char 69)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fixed\n\u001b[1;32m     60\u001b[0m json_dict \u001b[38;5;241m=\u001b[39m escape_json_func_field(json_dict)\n\u001b[0;32m---> 61\u001b[0m json_dict_ \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[82], line 27\u001b[0m, in \u001b[0;36mcorrect_json_string\u001b[0;34m(raw)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the cases field in the input string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m cases_body \u001b[38;5;241m=\u001b[39m cases_match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m cases \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcases_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Build a proper Python dict and dump to JSON\u001b[39;00m\n\u001b[1;32m     30\u001b[0m fixed_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m: escaped_func,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcases\u001b[39m\u001b[38;5;124m\"\u001b[39m: cases\n\u001b[1;32m     33\u001b[0m }\n",
      "File \u001b[0;32m~/.conda/envs/r1/lib/python3.12/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/.conda/envs/r1/lib/python3.12/json/decoder.py:338\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    334\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/.conda/envs/r1/lib/python3.12/json/decoder.py:354\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 2 column 68 (char 69)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def correct_json_string(raw):\n",
    "    \"\"\"\n",
    "    Fixes a JSON-like string where the \"func\" field contains unescaped newlines\n",
    "    and quotes. Returns a properly escaped JSON string.\n",
    "    \"\"\"\n",
    "    # Extract the raw func block (between the first \"func\": \" and the next \")\n",
    "    func_match = re.search(r'\"func\"\\s*:\\s*\"([\\s\\S]*?)\",\\s*\"cases\"', raw)\n",
    "    if not func_match:\n",
    "        raise ValueError(\"Could not find the func field in the input string.\")\n",
    "    func_body = func_match.group(1)\n",
    "\n",
    "    # Escape backslashes, quotes, and newlines for JSON embedding\n",
    "    escaped_func = (\n",
    "        func_body.replace('\\\\', '\\\\\\\\')\n",
    "                 .replace('\"', '\\\\\"')\n",
    "                 .replace('\\n', '\\\\n')\n",
    "    )\n",
    "\n",
    "    # Extract the cases block\n",
    "    cases_match = re.search(r'\"cases\"\\s*:\\s*(\\[\\s*{[\\s\\S]*}\\s*])', raw)\n",
    "    if not cases_match:\n",
    "        raise ValueError(\"Could not find the cases field in the input string.\")\n",
    "    cases_body = cases_match.group(1)\n",
    "    cases = json.loads(cases_body)\n",
    "\n",
    "    # Build a proper Python dict and dump to JSON\n",
    "    fixed_dict = {\n",
    "        \"func\": escaped_func,\n",
    "        \"cases\": cases\n",
    "    }\n",
    "    return json.dumps(fixed_dict, indent=2)\n",
    "\n",
    "def escape_json_func_field(raw_json_str):\n",
    "    \"\"\"\n",
    "    Escapes the value of the \"func\" field in a JSON-like string so that\n",
    "    newlines, quotes, and backslashes are properly encoded for json.loads().\n",
    "    Returns the modified string; you can then call json.loads() on it.\n",
    "    \"\"\"\n",
    "    def replacer(match):\n",
    "        func_body = match.group(1)\n",
    "        # Escape backslashes first, then quotes, then convert real newlines to \\n\n",
    "        escaped = (\n",
    "            func_body.replace('\\\\', '\\\\\\\\')\n",
    "                     .replace('\"', '\\\\\"')\n",
    "                     .replace('\\n', '\\\\n')\n",
    "        )\n",
    "        return f'\"func\":\"{escaped}\",'\n",
    "\n",
    "    # This regex captures everything between the first \"func\": \" and the next \",\n",
    "    # including literal newlines.\n",
    "    pattern = r'\"func\"\\s*:\\s*\"([\\s\\S]*?)\"\\s*,'\n",
    "    fixed = re.sub(pattern, replacer, raw_json_str)\n",
    "\n",
    "    # Return the string ready for json.loads()\n",
    "    return fixed\n",
    "\n",
    "json_dict = escape_json_func_field(json_dict)\n",
    "json_dict_ = correct_json_string(json_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dict= json_dict.replace(\"\\n\", \"\")\n",
    "len(generated_eval_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_generated_eval_functions =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 17873.45it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 15553.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5363.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "  words = response.split()\n",
      "  for word in words:\n",
      "    if not word.startswith('B'):\n",
      "      return False\n",
      "  return True\n",
      "\n",
      "inp: Amazing beings bounce\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 17949.95it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7626.01it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4040.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    return response.endswith(' STOP')\n",
      "\n",
      "inp: Hello world\n",
      "out: False\n",
      "res_val: False\n",
      "inp: Hello world STOP\n",
      "out: True\n",
      "res_val: True\n",
      "inp: Hello world STOP \n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 8911.41it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8892.52it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3318.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    \n",
      "    def is_palindrome(s):\n",
      "        return s == s[::-1]\n",
      "    \n",
      "    words = response.split()\n",
      "    for word in words:\n",
      "        if not is_palindrome(word):\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "inp: madam\n",
      "out: True\n",
      "res_val: True\n",
      "inp: radar level\n",
      "out: True\n",
      "res_val: True\n",
      "inp: hello world\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 2954.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 16644.06it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11135.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: evaluate() missing 1 required positional argument: 'original'\n",
      "skipping because flag is False\n",
      "Error executing eval function: evaluate() missing 1 required positional argument: 'original'\n",
      "skipping because flag is False\n",
      "Error executing eval function: evaluate() missing 1 required positional argument: 'original'\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7667.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response, original):\n",
      "  return response == original[::-1]\n",
      "\n",
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 8481.91it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7729.06it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3010.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "  words = response.split()\n",
      "  for word in words:\n",
      "    if not any(letter * 2 in word for letter in set(word)):\n",
      "      return False\n",
      "  return True\n",
      "\n",
      "inp: bookkeeper\n",
      "out: True\n",
      "res_val: True\n",
      "inp: bubble doodle\n",
      "out: True\n",
      "res_val: True\n",
      "inp: hello world\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 3472.59it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 3671.70it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2041.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "\n",
      "    # Define a list of onomatopoeia words\n",
      "    onomatopoeia = ['bang', 'boom', 'pow', 'meow', 'woof', 'moo', 'oink', 'buzz', 'beep', 'chirp', 'tweet', 'splash', 'splosh', 'plop', 'drip', 'drop', 'splat', 'thud', 'thump', 'clunk', 'clink', 'cling', 'clang', 'honk', 'toot', 'vroom', 'zoom', 'zip', 'zap', 'ping', 'pong', 'ding', 'dong', 'ring', 'ting', 'tingling', 'sizzle', 'crackle', 'pop', 'fizz', 'whir', 'whizz', 'whoosh', 'swish', 'swirl', 'gurgle', 'glug', 'bloop', 'bleep', 'blip', 'bong', 'dingdong', 'tinkle', 'jingle', 'chime', 'clangclang', 'rattle', 'clatter', 'clinkclink', 'tap', 'ratatap', 'tapity', 'tip', 'top', 'tot', 'tut', 'tapping', 'beating', 'drumming', 'thumping', 'pounding', 'splatting', 'splashing', 'sploshing', 'plopping', 'dripping', 'dropping', 'sizzling', 'crackling', 'popping', 'fizzling', 'whirring', 'whizzing', 'whooshing', 'swishing', 'swirling', 'gurgling', 'glugging', 'blooping', 'bleeping', 'blipping', 'bonging']\n",
      "\n",
      "    # Remove punctuation and convert to lower case\n",
      "    response = ''.join(e for e in response if e.isalnum() or e.isspace()).lower()\n",
      "\n",
      "    # Split the response into words\n",
      "    words = response.split()\n",
      "\n",
      "    # Check if all words are onomatopoeia\n",
      "    for word in words:\n",
      "        if word not in onomatopoeia:\n",
      "            return False\n",
      "\n",
      "    return True\n",
      "\n",
      "inp: bang boom\n",
      "out: True\n",
      "res_val: True\n",
      "inp: meow woof\n",
      "out: True\n",
      "res_val: True\n",
      "inp: hello world\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 11140.25it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10082.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4009.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    words = response.split()\n",
      "    return len(response) == 100 and response.count(' ') == 99 and response.endswith('.')\n",
      "\n",
      "inp: This is another test sentence that is not exactly one hundred words long\n",
      "out: False\n",
      "res_val: False\n",
      "inp: This is a test sentence that is exactly one hundred words long but does not have proper punctuation\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 20052.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Invalid \\escape: line 1 column 38 (char 37)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 38 (char 37)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 38 (char 37)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 38 (char 37)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 38 (char 37)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 38 (char 37)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 29262.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Invalid \\escape: line 1 column 139 (char 138)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 139 (char 138)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 139 (char 138)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 139 (char 138)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 139 (char 138)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 139 (char 138)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 28859.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Invalid \\escape: line 1 column 246 (char 245)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 246 (char 245)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 246 (char 245)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 246 (char 245)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 246 (char 245)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 246 (char 245)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 12186.84it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11893.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4860.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "      words = response.split()\n",
      "      for word in words:\n",
      "        if not word.endswith('ing'):\n",
      "          return False\n",
      "      return True\n",
      "\n",
      "inp: running jumping\n",
      "out: True\n",
      "res_val: True\n",
      "inp: swimming diving flying\n",
      "out: True\n",
      "res_val: True\n",
      "inp: running jump\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 2078.27it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 693.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2660.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "\n",
      "    # Define a list of military lingo terms\n",
      "    military_lingo = ['roger', 'over', 'out', 'bravo', 'charlie', 'delta', 'echo', 'foxtrot', 'golf', 'hotel', 'india', 'juliet', 'kilo', 'lima', 'mike', 'november', 'oscar', 'papa', 'quebec', 'romeo', 'sierra', 'tango', 'uniform', 'victor', 'whiskey', 'x-ray', 'yankee', 'zulu', 'alpha', 'beta', 'sitrep', 'opsec', 'tactical', 'strategic', 'recon', 'intel', 'comms', 'command', 'control', 'logistics', 'supply', 'chain', 'of', 'command', 'hooah', 'oorah', 'semper', 'fi', 'honor', 'country', 'corps', 'battalion', 'company', 'platoon', 'squad', 'fire', 'team', 'safety', 'protocol', 'procedure', 'regulation', 'directive', 'order', 'instruction', 'briefing', 'debriefing', 'mission', 'objective', 'parameter', 'perimeter', 'sector', 'grid', 'coordinate', 'map', 'chart', 'navigation', 'orient', 'elevation', 'azimuth', 'bearing', 'distance', 'speed', 'direction', 'course', 'heading', 'altitude', 'airspeed', 'groundspeed', 'climb', 'descent', 'level', 'off', 'on', 'station', 'post', 'position', 'location', 'site', 'area', 'zone', 'sector', 'grid', 'coordinate', 'map', 'chart', 'navigation', 'orient', 'elevation', 'azimuth', 'bearing', 'distance', 'speed', 'direction', 'course', 'heading', 'altitude', 'airspeed', 'groundspeed', 'climb', 'descent', 'level', 'off', 'on']\n",
      "\n",
      "    # Convert the response to lowercase and split it into words\n",
      "    words = response.lower().split()\n",
      "\n",
      "    # Check if all words are in the military lingo list\n",
      "    for word in words:\n",
      "        if word not in military_lingo:\n",
      "            return False\n",
      "\n",
      "    return True\n",
      "\n",
      "inp: Hello world\n",
      "out: False\n",
      "res_val: False\n",
      "inp: Bravo zulu alpha\n",
      "out: True\n",
      "res_val: True\n",
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "\n",
      "    # Define a list of military lingo terms\n",
      "    military_lingo = ['roger', 'over', 'out', 'bravo', 'charlie', 'delta', 'echo', 'foxtrot', 'golf', 'hotel', 'india', 'juliet', 'kilo', 'lima', 'mike', 'november', 'oscar', 'papa', 'quebec', 'romeo', 'sierra', 'tango', 'uniform', 'victor', 'whiskey', 'x-ray', 'yankee', 'zulu', 'alpha', 'beta', 'sitrep', 'opsec', 'tactical', 'strategic', 'recon', 'intel', 'comms', 'command', 'control', 'logistics', 'supply', 'chain', 'of', 'command', 'hooah', 'oorah', 'semper', 'fi', 'honor', 'country', 'corps', 'battalion', 'company', 'platoon', 'squad', 'fireteam', 'safety', 'protocol', 'procedure', 'regulation', 'directive', 'order', 'instruction', 'briefing', 'debriefing', 'mission', 'objective', 'parameter', 'perimeter', 'sector', 'grid', 'coordinate', 'map', 'chart', 'navigation', 'orient', 'elevation', 'azimuth', 'bearing', 'distance', 'speed', 'direction', 'course', 'heading', 'altitude', 'airspeed', 'airspace', 'airsupport', 'artillery', 'armor', 'infantry', 'cavalry', 'engineer', 'signal', 'medical', 'evac', 'extract', 'insert', 'infiltrate', 'exfiltrate', 'perimeter', 'security', 'patrol', 'reconnaissance', 'surveillance', 'sniper', 'spotter', 'designate', 'target', 'acquire', 'engage', 'neutralize', 'destroy', 'disable', 'disrupt', 'interdict', 'interrogate', 'debrief', 'report', 'sitrep', 'status', 'update', 'brief', 'debrief', 'summary', 'assessment', 'evaluation', 'analysis', 'estimate', 'prediction', 'forecast', 'warning', 'alert', 'notification', 'message', 'transmission', 'receipt', 'acknowledgement', 'response', 'reply', 'answer', 'query', 'request', 'demand', 'instruction', 'order', 'command', 'directive', 'regulation', 'protocol', 'procedure', 'guideline', 'standard', 'doctrine', 'tactic', 'technique', 'procedure', 'method', 'approach', 'concept', 'theory', 'model', 'framework', 'structure', 'system', 'process', 'operation', 'activity', 'task', 'mission', 'function', 'duty', 'responsibility', 'authority', 'power', 'control', 'command', 'leadership', 'management', 'administration', 'logistics', 'support', 'maintenance', 'supply', 'chain', 'of', 'command', 'communication', 'network', 'system', 'channel', 'link', 'node', 'hub', 'center', 'headquarters', 'command', 'post', 'station', 'base', 'camp', 'fort', 'barracks', 'garrison', 'outpost', 'patrol', 'unit', 'team', 'squad', 'platoon', 'company', 'battalion', 'regiment', 'brigade', 'division', 'corps', 'army', 'navy', 'air', 'force', 'marine', 'coast', 'guard', 'reserves', 'national', 'guard', 'special', 'forces', 'operations', 'tactical', 'strategic', 'theater', 'level', 'joint', 'task', 'force', 'combined', 'arms', 'team', 'staff', 'section', 'platoon', 'company', 'battalion', 'brigade', 'division', 'corps', 'command', 'headquarters', 'element', 'component', 'unit', 'formation', 'task', 'force', 'group', 'wing', 'squadron', 'airlift', 'airborne', 'amphibious', 'armored', 'artillery', 'aviation', 'cavalry', 'engineer', 'infantry', 'intelligence', 'logistics', 'maintenance', 'medical', 'missile', 'naval', 'ordnance', 'quartermaster', 'signal', 'special', 'forces', 'support', 'transportation', 'vehicle', 'aircraft', 'ship', 'submarine', 'tank', 'truck', 'helicopter', 'jet', 'bomber', 'fighter', 'reconnaissance', 'transport', 'cargo', 'passenger', 'patrol', 'boat', 'craft', 'vessel', 'submarine', 'torpedo', 'mine', 'missile', 'rocket', 'gun', 'howitzer', 'mortar', 'rifle', 'pistol', 'machine', 'gun', 'grenade', 'launcher', 'flamethrower', 'tank', 'armor', 'vehicle', 'aircraft', 'helmet', 'vest', 'flak', 'jacket', 'pants', 'boots', 'gloves', 'mask', 'goggles', 'gas', 'mask', 'first', 'aid', 'kit', 'medical', 'bag', 'stretcher', 'litter', 'bandage', 'tourniquet', 'splint', 'cast', 'crutch', 'cane', 'walker', 'wheelchair', 'bed', 'cot', 'stretcher', 'litter', 'tent', 'shelter', 'bunker', 'foxhole', 'trench', 'dugout', 'barracks', 'quarters', 'mess', 'hall', 'galley', 'kitchen', 'latrine', 'head', 'shower', 'bath', 'sink', 'toilet', 'urinal', 'wash', 'basin', 'mirror', 'comb', 'brush', 'razor', 'shaving', 'cream', 'toothbrush', 'toothpaste', 'soap', 'shampoo', 'conditioner', 'lotion', 'cream', 'oil', 'sunscreen', 'insect', 'repellent', 'first', 'aid', 'kit', 'medical', 'bag', 'stretcher', 'litter', 'bandage', 'tourniquet', 'splint', 'cast', 'crutch', 'cane', 'walker', 'wheelchair', 'bed', 'cot', 'stretcher', 'litter', 'tent', 'shelter', 'bunker', 'foxhole', 'trench', 'dugout', 'barracks', 'quarters', 'mess', 'hall', 'galley', 'kitchen', 'latrine', 'head', 'shower', 'bath', 'sink', 'toilet', 'urinal', 'wash', 'basin', 'mirror', 'comb', 'brush', 'razor', 'shaving', 'cream', 'toothbrush', 'toothpaste', 'soap', 'shampoo', 'conditioner', 'lotion', 'cream', 'oil', 'sunscreen', 'insect', 'repellent']\n",
      "\n",
      "    # Convert the response to lowercase and split it into words\n",
      "    words = response.lower().split()\n",
      "\n",
      "    # Check if all words are in the military lingo list\n",
      "    for word in words:\n",
      "        if word not in military_lingo:\n",
      "            return False\n",
      "\n",
      "    # If all words are in the list, return True\n",
      "    return True\n",
      "\n",
      "inp: Hello world\n",
      "out: False\n",
      "res_val: False\n",
      "inp: Bravo zulu alpha\n",
      "out: True\n",
      "res_val: True\n",
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "\n",
      "    # Define a list of military lingo terms\n",
      "    military_lingo = ['roger', 'over', 'out', 'bravo', 'charlie', 'delta', 'echo', 'foxtrot', 'golf', 'hotel', 'india', 'juliet', 'kilo', 'lima', 'mike', 'november', 'oscar', 'papa', 'quebec', 'romeo', 'sierra', 'tango', 'uniform', 'victor', 'whiskey', 'x-ray', 'yankee', 'zulu', 'alpha', 'beta', 'sitrep', 'opsec', 'tactical', 'strategic', 'recon', 'intel', 'comms', 'command', 'control', 'logistics', 'supply', 'chain', 'of', 'command', 'hooah', 'oorah', 'semper', 'fi', 'honor', 'country', 'corps', 'battalion', 'company', 'platoon', 'squad', 'fireteam', 'safety', 'protocol', 'procedure', 'regulation', 'directive', 'order', 'instruction', 'briefing', 'debriefing', 'mission', 'objective', 'parameter', 'perimeter', 'sector', 'grid', 'coordinate', 'map', 'chart', 'navigation', 'orient', 'elevation', 'azimuth', 'bearing', 'distance', 'speed', 'direction', 'course', 'heading', 'altitude', 'airspeed', 'airspace', 'airsupport', 'artillery', 'armor', 'infantry', 'cavalry', 'engineer', 'signal', 'medical', 'evac', 'extract', 'insert', 'infiltrate', 'exfiltrate', 'perimeter', 'security', 'patrol', 'reconnaissance', 'surveillance', 'sniper', 'spotter', 'designate', 'target', 'acquire', 'engage', 'neutralize', 'destroy', 'disable', 'disrupt', 'interdict', 'interrogate', 'debrief', 'report', 'sitrep', 'status', 'update', 'brief', 'debrief', 'summary', 'assessment', 'evaluation', 'analysis', 'estimate', 'prediction', 'forecast', 'warning', 'alert', 'notification', 'message', 'transmission', 'receipt', 'acknowledgement', 'response', 'reply', 'answer', 'query', 'request', 'demand', 'instruction', 'order', 'command', 'directive', 'regulation', 'protocol', 'procedure', 'guideline', 'standard', 'doctrine', 'tactic', 'technique', 'procedure', 'method', 'approach', 'concept', 'theory', 'model', 'framework', 'structure', 'system', 'process', 'operation', 'activity', 'task', 'mission', 'function', 'duty', 'responsibility', 'authority', 'power', 'control', 'command', 'leadership', 'management', 'administration', 'logistics', 'support', 'maintenance', 'supply', 'chain', 'of', 'command', 'communication', 'network', 'system', 'channel', 'link', 'node', 'hub', 'center', 'headquarters', 'command', 'post', 'station', 'base', 'camp', 'fort', 'barracks', 'garrison', 'outpost', 'patrol', 'unit', 'team', 'squad', 'platoon', 'company', 'battalion', 'regiment', 'brigade', 'division', 'corps', 'army', 'navy', 'air', 'force', 'marine', 'coast', 'guard', 'reserves', 'national', 'guard', 'special', 'forces', 'operations', 'tactical', 'strategic', 'theater', 'level', 'joint', 'task', 'force', 'combined', 'arms', 'team', 'staff', 'section', 'platoon', 'company', 'battalion', 'brigade', 'division', 'corps', 'command', 'headquarters', 'element', 'component', 'unit', 'formation', 'task', 'force', 'group', 'wing', 'squadron', 'airlift', 'airborne', 'amphibious', 'armored', 'artillery', 'aviation', 'cavalry', 'engineer', 'infantry', 'intelligence', 'logistics', 'maintenance', 'medical', 'missile', 'naval', 'ordnance', 'quartermaster', 'signal', 'special', 'forces', 'support', 'transportation', 'vehicle', 'aircraft', 'ship', 'submarine', 'tank', 'truck', 'helicopter', 'jet', 'bomber', 'fighter', 'reconnaissance', 'transport', 'cargo', 'passenger', 'patrol', 'boat', 'craft', 'vessel', 'submarine', 'torpedo', 'mine', 'missile', 'rocket', 'gun', 'howitzer', 'mortar', 'rifle', 'pistol', 'machine', 'gun', 'grenade', 'launcher', 'flamethrower', 'tank', 'armor', 'vehicle', 'aircraft', 'helicopter', 'jet', 'bomber', 'fighter', 'reconnaissance', 'transport', 'cargo', 'passenger', 'patrol', 'boat', 'craft', 'vessel', 'submarine', 'torpedo', 'mine', 'missile', 'rocket', 'gun', 'howitzer', 'mortar', 'rifle', 'pistol', 'machine', 'gun', 'grenade', 'launcher', 'flamethrower']\n",
      "\n",
      "    # Convert the response to lowercase and split it into words\n",
      "    words = response.lower().split()\n",
      "\n",
      "    # Check if all words are in the military lingo list\n",
      "    for word in words:\n",
      "        if word not in military_lingo:\n",
      "            return False\n",
      "\n",
      "    return True\n",
      "\n",
      "inp: Hello world\n",
      "out: False\n",
      "res_val: False\n",
      "inp: Bravo zulu alpha\n",
      "out: True\n",
      "res_val: True\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 30393.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Invalid \\escape: line 1 column 1609 (char 1608)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 439 (char 438)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 439 (char 438)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 439 (char 438)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 439 (char 438)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 439 (char 438)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 18170.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Invalid \\escape: line 1 column 63 (char 62)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 63 (char 62)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 63 (char 62)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 63 (char 62)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 63 (char 62)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 63 (char 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 8805.40it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10600.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4355.46it/s]\n",
      " 44%|████▍     | 16/36 [00:00<00:00, 153.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "  words = response.split()\n",
      "  for word in words:\n",
      "    if len(word) > 3 or word.count('a') + word.count('e') + word.count('i') + word.count('o') + word.count('u') > 1:\n",
      "      return False\n",
      "  return True\n",
      "\n",
      "inp: run pet\n",
      "out: True\n",
      "res_val: True\n",
      "inp: yes no\n",
      "out: True\n",
      "res_val: True\n",
      "inp: hello world\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 21076.90it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 16622.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 7781.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    words = response.split()\n",
      "    return words == sorted(words)\n",
      "\n",
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 23786.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Expecting value: line 1 column 594 (char 593)\n",
      "Skipping as JSON load error: Expecting value: line 1 column 594 (char 593)\n",
      "Skipping as JSON load error: Expecting value: line 1 column 594 (char 593)\n",
      "Skipping as JSON load error: Expecting value: line 1 column 594 (char 593)\n",
      "Skipping as JSON load error: Expecting value: line 1 column 594 (char 593)\n",
      "Skipping as JSON load error: Expecting value: line 1 column 594 (char 593)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 8063.38it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 2870.19it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1572.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "import nltk\n",
      "from nltk import pos_tag, word_tokenize\n",
      "\n",
      "def evaluate(response):\n",
      "  tokens = word_tokenize(response)\n",
      "  tagged = pos_tag(tokens)\n",
      "  for word, tag in tagged:\n",
      "    if tag in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:\n",
      "      return False\n",
      "  return True\n",
      "\n",
      "inp: I go to the store\n",
      "out: True\n",
      "res_val: True\n",
      "inp: She eats a sandwich\n",
      "out: True\n",
      "res_val: True\n",
      "inp: The big red car drives quickly\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 20593.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 17331.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4332.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    words = response.split()\n",
      "    return len(words) == 6\n",
      "\n",
      "inp: I am\n",
      "out: False\n",
      "res_val: False\n",
      "inp: This is a very long sentence that does not follow the instruction\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 21864.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping as JSON load error: Invalid \\escape: line 1 column 426 (char 425)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 426 (char 425)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 426 (char 425)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 426 (char 425)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 426 (char 425)\n",
      "Skipping as JSON load error: Invalid \\escape: line 1 column 426 (char 425)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 7096.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 6808.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4934.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "import re\n",
      "\n",
      "def evaluate(response):\n",
      "    # Split the response into lines\n",
      "    lines = response.split('\\n')\n",
      "    # Define a pattern for an iambic pentameter line\n",
      "    pattern = re.compile('^(da-DUM )*(da-DUM|da-DUM da)$', re.IGNORECASE)\n",
      "    # Check each line against the pattern\n",
      "    for line in lines:\n",
      "        syllables = line.split()\n",
      "        # Check if the line has 10 syllables (5 feet)\n",
      "        if len(syllables) != 10:\n",
      "            return False\n",
      "        # Check if the line follows the iambic pattern\n",
      "        if not pattern.match(line):\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "inp: The sun is shining very brightly today\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 11688.72it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10373.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4341.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "      words = response.split()\n",
      "      for word in words:\n",
      "        if not word:\n",
      "          continue\n",
      "        first_letter = word[0].lower()\n",
      "        for char in word:\n",
      "          if char.isalpha() and char.lower() != first_letter:\n",
      "            return False\n",
      "      return True\n",
      "\n",
      "inp: Hello world\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 7609.87it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7445.51it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 2926.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "import re\n",
      "\n",
      "def evaluate(response):\n",
      "  # Define future tense verbs pattern\n",
      "  future_tense_pattern = re.compile(r'\\b(will|shall|is going to)\\b', re.IGNORECASE)\n",
      "  # Check if any word in the response matches the future tense pattern\n",
      "  has_future_tense = bool(future_tense_pattern.search(response))\n",
      "  # Check if the response contains any verb that is not in future tense\n",
      "  verbs = re.findall(r'\\b\\w+ing\\b|\\b\\w+ed\\b|\\b\\w+s\\b', response, re.IGNORECASE)\n",
      "  for verb in verbs:\n",
      "    if not future_tense_pattern.search(verb):\n",
      "      return False\n",
      "  # If no non-future tense verb is found and there is at least one future tense verb, return True\n",
      "  return has_future_tense\n",
      "\n",
      "inp: I went to the store yesterday\n",
      "out: False\n",
      "res_val: False\n",
      "inp: I am going to the store now\n",
      "out: False\n",
      "res_val: False\n",
      "inp: I will go to the store tomorrow\n",
      "out: True\n",
      "res_val: True\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10468.31it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9258.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6808.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    # Convert the response to uppercase to handle case-insensitivity\n",
      "    response = response.upper()\n",
      "    # Iterate over each character in the response\n",
      "    for char in response:\n",
      "        # Check if the character is an alphabet letter and if it's in the second half of the alphabet (N-Z)\n",
      "        if char.isalpha() and char > 'M':\n",
      "            # If it is, return False\n",
      "            return False\n",
      "    # If no characters in the second half of the alphabet are found, return True\n",
      "    return True\n",
      "\n",
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 11052.18it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 10582.77it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3539.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    sentences = response.split('. ')\n",
      "    for sentence in sentences:\n",
      "        sentence = sentence.strip()\n",
      "        if not sentence.endswith('?'):\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "inp: Are you ready? Are you set?\n",
      "out: True\n",
      "res_val: True\n",
      "inp: This is a statement.\n",
      "out: False\n",
      "res_val: False\n",
      "inp: Is this a question? Is that a question?\n",
      "out: True\n",
      "res_val: True\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 9795.96it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9568.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3276.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "  words = response.split()\n",
      "  for word in words:\n",
      "    if not (word[0].lower() == word[-1].lower()) or len(word) == 0:\n",
      "      return False\n",
      "  return True\n",
      "\n",
      "inp: Madam\n",
      "out: True\n",
      "res_val: True\n",
      "inp: Hello\n",
      "out: False\n",
      "res_val: False\n",
      "inp: Dad\n",
      "out: True\n",
      "res_val: True\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 5990.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 7492.06it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 7800.94it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3111.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "\n",
      "  # Define a list of color words\n",
      "  color_words = ['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'black', 'white', 'brown', 'gray', 'turquoise', 'silver', 'gold', 'beige', 'ivory', 'cream', 'cyan', 'magenta', 'violet', 'indigo']\n",
      "\n",
      "  # Split the response into words\n",
      "  words = response.split()\n",
      "\n",
      "  # Check if all words are color words\n",
      "  for word in words:\n",
      "    if word.lower() not in color_words:\n",
      "      return False\n",
      "\n",
      "  return True\n",
      "\n",
      "inp: red blue green\n",
      "out: True\n",
      "res_val: True\n",
      "inp: Yellow Orange Purple\n",
      "out: True\n",
      "res_val: True\n",
      "inp: hello world\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 9742.87it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8160.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 6533.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    # Convert the response to uppercase to handle case-insensitivity\n",
      "    response = response.upper()\n",
      "    # Iterate over each character in the response\n",
      "    for char in response:\n",
      "        # Check if the character is an alphabet letter and if it's in the first half of the alphabet\n",
      "        if char.isalpha() and ord(char) < ord('N'):\n",
      "            # If it is, return False\n",
      "            return False\n",
      "    # If no characters in the first half of the alphabet are found, return True\n",
      "    return True\n",
      "\n",
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 6808.94it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8112.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4017.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "  sentences = response.split('. ')\n",
      "  if len(sentences) < 2:\n",
      "    return False\n",
      "  word_counts = [len(sentence.split()) for sentence in sentences]\n",
      "  return all(word_counts[i] > word_counts[i+1] for i in range(len(word_counts)-1))\n",
      "\n",
      "inp: This is a test sentence. This is a test. This is.\n",
      "out: True\n",
      "res_val: True\n",
      "inp: This is a test sentence. This is a test. This is a test sentence.\n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 12282.00it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 8943.08it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3243.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "    lines = response.split('\\n')\n",
      "    if len(lines) < 1:\n",
      "        return False\n",
      "    for line in lines:\n",
      "        if not line.strip().startswith('-') and not line.strip().startswith('*'):\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "inp: item1\n",
      "item2\n",
      "out: False\n",
      "res_val: False\n",
      "inp: - item1\n",
      "- item2\n",
      "out: True\n",
      "res_val: True\n",
      "inp: * item1\n",
      "* item2\n",
      "out: True\n",
      "res_val: True\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 8707.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n",
      "Error executing eval function: unexpected indent (<string>, line 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 10578.32it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9954.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping because flag is False\n",
      "skipping because flag is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4588.95it/s]\n",
      " 94%|█████████▍| 34/36 [00:00<00:00, 165.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "import re\n",
      "\n",
      "def evaluate(response):\n",
      "  # Check if the response contains any non-emoji characters\n",
      "  if re.search('[^\\u2639-\\u2696\\u26A0-\\u26FF\\u2700-\\u27BF\\u1F600-\\u1F64F\\u1F300-\\u1F5FF\\u1F680-\\u1F6FF\\u2600-\\u26FF\\u2700-\\u27BF]', response):\n",
      "    return False\n",
      "  # Check if the response is empty\n",
      "  if not response:\n",
      "    return False\n",
      "  return True\n",
      "\n",
      "inp: \n",
      "out: False\n",
      "res_val: False\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 9485.80it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11115.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 3545.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "  words = response.split()\n",
      "  for word in words:\n",
      "    if 'x' not in word.lower():\n",
      "      return False\n",
      "  return True\n",
      "\n",
      "inp: Hello world\n",
      "out: False\n",
      "res_val: False\n",
      "inp: Maximize exuberant excess\n",
      "out: True\n",
      "res_val: True\n",
      "inp: Exact examples exceed expectations\n",
      "out: True\n",
      "res_val: True\n",
      "valid funcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 12925.44it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 12421.43it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4481.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "def evaluate(response):\n",
      "      words = response.split()\n",
      "      alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
      "      for i, word in enumerate(words):\n",
      "        if word[0].lower() != alphabet[i % 26]:\n",
      "          return False\n",
      "      return True\n",
      "\n",
      "inp: Able bodied creatures deliberately establish bad goals\n",
      "out: False\n",
      "res_val: False\n",
      "inp: Able bodied creatures deliberately establish fresh\n",
      "out: True\n",
      "res_val: True\n",
      "inp: Able bodied creatures deliberately establish fresh goals\n",
      "out: True\n",
      "res_val: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 36/36 [00:00<00:00, 163.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid funcs\n"
     ]
    }
   ],
   "source": [
    "for result in tqdm(generated_eval_functions):\n",
    "    res = result['gpt-answer']\n",
    "    eval_funcs, test_cases = [], []\n",
    "    for each in tqdm(res):\n",
    "        try:\n",
    "            json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip().replace(\"\\n\", \"\")\n",
    "        except Exception as e:\n",
    "            print(\"Skipping because of index error:\", e)\n",
    "            continue\n",
    "        try:\n",
    "            res_dict = json.loads(json_dict)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping as JSON load error:\", e)\n",
    "            continue\n",
    "\n",
    "        # func rejection and cleaning\n",
    "        func = res_dict['func'].strip()\n",
    "        func = '\\n'.join([line for line in func.split('\\n') if 'download' not in line and 'requests' not in line])\n",
    "        try:\n",
    "            exec(func)\n",
    "        except Exception as e:\n",
    "            print(\"Error executing eval function:\", e)\n",
    "            continue\n",
    "        eval_funcs.append(func)\n",
    "\n",
    "        for each_case in res_dict['cases']:\n",
    "            try:\n",
    "                test_cases.append((each_case['input'], each_case['output']))\n",
    "            except KeyError:\n",
    "                print(each_case)\n",
    "\n",
    "    eval_funcs = list(set(eval_funcs))\n",
    "    test_cases = list(map(json.loads, set(map(json.dumps, test_cases))))\n",
    "    \n",
    "    # if len(eval_funcs) < 3 or len(test_cases) < 10:\n",
    "    #     continue\n",
    "    \n",
    "    filtered_test_cases = []\n",
    "    for each in tqdm(test_cases):\n",
    "        flag = False\n",
    "        for func in eval_funcs:\n",
    "            local_vars = {}\n",
    "            try:\n",
    "                exec(func, globals(), local_vars)\n",
    "            except Exception as e:\n",
    "                print(\"Error executing eval function:\", e)\n",
    "                continue\n",
    "            if 'evaluate' not in local_vars:\n",
    "                print(\"skipping because evaluate not in local_vars\")\n",
    "                continue\n",
    "            eval_func = local_vars['evaluate']\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(5)\n",
    "                res_val = eval_func(each[0])\n",
    "            except Exception as e:\n",
    "                print(\"Error executing eval function:\", e)\n",
    "                res_val = None\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            if res_val is not None and res_val == each[1]:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            filtered_test_cases.append(each)\n",
    "        else:\n",
    "            print(\"skipping because flag is False\")\n",
    "    scored_funcs = []\n",
    "    for func in tqdm(eval_funcs):\n",
    "        print('-'*100)\n",
    "        print(func)\n",
    "        print()\n",
    "        local_vars = {}\n",
    "        try:\n",
    "            exec(func, globals(), local_vars)\n",
    "        except Exception as e:\n",
    "            print(\"Error executing eval function:\", e)\n",
    "            continue\n",
    "        if 'evaluate' not in local_vars:\n",
    "            print(\"skipping because evaluate not in local_vars\")\n",
    "            continue\n",
    "        eval_func = local_vars['evaluate']\n",
    "        acc = []\n",
    "        for inp, out in filtered_test_cases:\n",
    "            print(\"inp:\", inp)\n",
    "            print(\"out:\", out)\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(5)\n",
    "                res_val = eval_func(inp)\n",
    "            except Exception as e:\n",
    "                print(\"Error executing eval function:\", e)\n",
    "                res_val = None\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            if res_val is None or res_val != out:\n",
    "                acc.append(0)\n",
    "            else:\n",
    "                acc.append(1)\n",
    "            print(\"res_val:\", res_val)\n",
    "        acc = np.mean(acc) if acc else 0\n",
    "        scored_funcs.append((func, acc))\n",
    "    valid_funcs = [each for each in scored_funcs if each[1] >= 0.8]\n",
    "    if not valid_funcs:\n",
    "        print(\"not valid funcs\")\n",
    "        continue\n",
    "    else:\n",
    "        print(\"valid funcs\")\n",
    "    \n",
    "    filtered_generated_eval_functions.append({\n",
    "        \"instruction\": result['instruction'],\n",
    "        \"eval_func\": valid_funcs,\n",
    "        \"cases\": filtered_test_cases\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_generated_eval_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global filtered_generated_eval_functions\n",
    "# filtered_generated_eval_functions = []\n",
    "# def filter_generated_eval_function():\n",
    "#     global generated_eval_functions, filtered_generated_eval_functions\n",
    "#     collect_packages = []\n",
    "#     for result in generated_eval_functions:\n",
    "#         res = result['gpt-answer']\n",
    "#         eval_funcs, test_cases = [], []\n",
    "#         for each in res:\n",
    "#             try:\n",
    "#                 json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "#             except Exception as e:\n",
    "#                 # print(\"Skipping because of index error:\", e)\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 res_dict = json.loads(json_dict)\n",
    "#             except Exception as e:\n",
    "#                 # print(\"skipping because of JSON load error:\", e)\n",
    "#                 continue\n",
    "#             func = res_dict['func']\n",
    "#             if '\\\\n' in func:\n",
    "#                 func = func.replace('\\\\n', '\\n')\n",
    "#             try:\n",
    "#                 exec(func)\n",
    "#             except Exception as e:\n",
    "#                 # print(\"Error executing eval function:\", e)\n",
    "#                 continue\n",
    "#             for line in func.split('\\n'):\n",
    "#                 if 'import' in line or 'download' in line or 'requests' in line:\n",
    "#                     collect_packages.append(line)\n",
    "#     print(list(set(collect_packages)))\n",
    "\n",
    "#     for result in tqdm(generated_eval_functions):\n",
    "#         res = result['gpt-answer']\n",
    "#         eval_funcs, test_cases = [], []\n",
    "#         for each in tqdm(res):\n",
    "#             try:\n",
    "#                 json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "#             except Exception as e:\n",
    "#                 # print(\"Skipping because of index error:\", e)\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 res_dict = json.loads(json_dict)\n",
    "#             except Exception as e:\n",
    "#                 # print(\"Skipping as JSON load error:\", e)\n",
    "#                 continue\n",
    "\n",
    "#             # func rejection and cleaning\n",
    "#             func = res_dict['func'].strip()\n",
    "#             func = '\\n'.join([line for line in func.split('\\n') if 'download' not in line and 'requests' not in line])\n",
    "#             try:\n",
    "#                 exec(func)\n",
    "#             except Exception as e:\n",
    "#                 # print(\"Error executing eval function:\", e)\n",
    "#                 continue\n",
    "#             eval_funcs.append(func)\n",
    "\n",
    "#             for each_case in res_dict['cases']:\n",
    "#                 try:\n",
    "#                     test_cases.append((each_case['input'], each_case['output']))\n",
    "#                 except KeyError:\n",
    "#                     print(each_case)\n",
    "\n",
    "#         eval_funcs = list(set(eval_funcs))\n",
    "#         test_cases = list(map(json.loads, set(map(json.dumps, test_cases))))\n",
    "#         # if len(eval_funcs) < 3 or len(test_cases) < 10:\n",
    "#         #     continue\n",
    "#         # print(\"len(test_cases):\", len(test_cases))\n",
    "#         filtered_test_cases = []\n",
    "#         for each in tqdm(test_cases):\n",
    "#             flag = False\n",
    "#             for func in eval_funcs:\n",
    "#                 local_vars = {}\n",
    "#                 try:\n",
    "#                     exec(func, globals(), local_vars)\n",
    "#                 except Exception as e:\n",
    "#                     # print(\"Error executing eval function:\", e)\n",
    "#                     continue\n",
    "#                 if 'evaluate' not in local_vars:\n",
    "#                     # print(\"skipping because evaluate not in local_vars\")\n",
    "#                     continue\n",
    "#                 eval_func = local_vars['evaluate']\n",
    "#                 try:\n",
    "#                     signal.signal(signal.SIGALRM, timeout_handler)\n",
    "#                     signal.alarm(5)\n",
    "#                     res_val = eval_func(each[0])\n",
    "#                 except Exception:\n",
    "#                     res_val = None\n",
    "#                 finally:\n",
    "#                     signal.alarm(0)\n",
    "#                 if res_val is not None and res_val == each[1]:\n",
    "#                     flag = True\n",
    "#             if flag:\n",
    "#                 filtered_test_cases.append(each)\n",
    "#             else:\n",
    "#                 print(\"skipping because flag is False\")\n",
    "#         # print(\"len(eval_funcs):\", len(eval_funcs))\n",
    "#         scored_funcs = []\n",
    "#         for func in tqdm(eval_funcs):\n",
    "#             local_vars = {}\n",
    "#             try:\n",
    "#                 exec(func, globals(), local_vars)\n",
    "#             except Exception as e:\n",
    "#                 # print(\"Error executing eval function:\", e)\n",
    "#                 continue\n",
    "#             if 'evaluate' not in local_vars:\n",
    "#                 # print(\"skipping because evaluate not in local_vars\")\n",
    "#                 continue\n",
    "#             eval_func = local_vars['evaluate']\n",
    "#             acc = []\n",
    "#             for inp, out in filtered_test_cases:\n",
    "#                 try:\n",
    "#                     signal.signal(signal.SIGALRM, timeout_handler)\n",
    "#                     signal.alarm(5)\n",
    "#                     res_val = eval_func(inp)\n",
    "#                 except Exception:\n",
    "#                     res_val = None\n",
    "#                 finally:\n",
    "#                     signal.alarm(0)\n",
    "#                 if res_val is None or res_val != out:\n",
    "#                     acc.append(0)\n",
    "#                 else:\n",
    "#                     acc.append(1)\n",
    "#             acc = np.mean(acc) if acc else 0\n",
    "#             scored_funcs.append((func, acc))\n",
    "#         valid_funcs = [each for each in scored_funcs if each[1] >= 0.8]\n",
    "#         if not valid_funcs:\n",
    "#             print(\"not valid funcs\")\n",
    "#             continue\n",
    "#         else:\n",
    "#             print(\"valid funcs\")\n",
    "        \n",
    "#         filtered_generated_eval_functions.append({\n",
    "#             \"instruction\": result['instruction'],\n",
    "#             \"eval_func\": valid_funcs,\n",
    "#             \"cases\": filtered_test_cases\n",
    "#         })\n",
    "#     return filtered_generated_eval_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': '- Ensure your response is exactly 100 characters long.',\n",
       "  'eval_func': [('def evaluate(response):\\n    return len(response) == 100',\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['This is a test string that is definitely not one hundred characters long.',\n",
       "    False],\n",
       "   ['bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb',\n",
       "    False],\n",
       "   ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "    True],\n",
       "   ['This string is definitely not one hundred characters long.', False],\n",
       "   ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "    False],\n",
       "   ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "    False],\n",
       "   ['This response is definitely not one hundred characters long.', False]]},\n",
       " {'instruction': '- Respond with a list of exactly five items.',\n",
       "  'eval_func': [(\"def evaluate(response):\\n    # Split the response by commas and strip whitespace\\n    items = [item.strip() for item in response.split(',')]\\n    # Check if there are exactly five items\\n    return len(items) == 5\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['apple, banana, cherry', False],\n",
       "   ['apple, banana, cherry, date, fig', True],\n",
       "   ['apple, banana, cherry, date', False],\n",
       "   ['apple, banana, cherry, date, fig, grape', False]]},\n",
       " {'instruction': '- Provide your answer in a JSON format.',\n",
       "  'eval_func': [('def evaluate(response):\\n    try:\\n        # Attempt to parse the response as JSON\\n        import json\\n        json.loads(response)\\n        return True\\n    except json.JSONDecodeError:\\n        # If a JSONDecodeError is raised, the response is not valid JSON\\n        return False',\n",
       "    np.float64(1.0)),\n",
       "   (\"def evaluate(response):\\n    try:\\n        # Try to parse the response as JSON\\n        import json\\n        json.loads(response)\\n        return True\\n    except json.JSONDecodeError:\\n        # If parsing fails, it's not valid JSON\\n        return False\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['[1, 2, 3]', True],\n",
       "   ['This is not JSON', False],\n",
       "   ['{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}', True],\n",
       "   ['{\"key\": \"value\"}', True]]},\n",
       " {'instruction': '- Ensure your response is a palindrome.',\n",
       "  'eval_func': [(\"def evaluate(response):\\n    # Remove any non-alphanumeric characters and convert to lowercase\\n    cleaned_response = ''.join(char.lower() for char in response if char.isalnum())\\n    # Check if the cleaned response is a palindrome\\n    return cleaned_response == cleaned_response[::-1]\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['Was it a car or a cat I saw?', True],\n",
       "   ['A man, a plan, a canal, Panama', True],\n",
       "   [\"Madam In Eden, I'm Adam\", True],\n",
       "   ['Hello, World!', False]]},\n",
       " {'instruction': '- Format your response as a valid XML document.',\n",
       "  'eval_func': [('def evaluate(response):\\n    import xml.etree.ElementTree as ET\\n    try:\\n        ET.fromstring(response)\\n        return True\\n    except ET.ParseError:\\n        return False',\n",
       "    np.float64(1.0))],\n",
       "  'cases': [[\"<note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don't forget me this weekend!</body></note>\",\n",
       "    True],\n",
       "   [\"<note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don't forget me this weekend!</body>\",\n",
       "    False],\n",
       "   ['Just a plain text, not XML.', False]]},\n",
       " {'instruction': '- Provide your answer in the form of a CSV string.',\n",
       "  'eval_func': [(\"def evaluate(response):\\n    # Check if the response is a non-empty string\\n    if not isinstance(response, str) or not response.strip():\\n        return False\\n    \\n    # Check if the response contains at least one comma, indicating CSV format\\n    if ',' not in response:\\n        return False\\n    \\n    # Check if the response can be split into rows and columns like a CSV\\n    rows = response.split('\\\\n')\\n    for row in rows:\\n        if len(row.split(',')) < 2:\\n            return False\\n    \\n    return True\",\n",
       "    np.float64(1.0)),\n",
       "   (\"def evaluate(response):\\n    # Check if the response is a non-empty string\\n    if not isinstance(response, str) or not response.strip():\\n        return False\\n    \\n    # Check if the response contains at least one comma, indicating CSV format\\n    if ',' not in response:\\n        return False\\n    \\n    # Split the response by newlines to check each line\\n    lines = response.strip().split('\\\\n')\\n    \\n    # Check if each line contains the same number of commas\\n    num_commas = None\\n    for line in lines:\\n        # Count the number of commas in the current line\\n        current_commas = line.count(',')\\n        \\n        # If this is the first line, set the number of commas\\n        if num_commas is None:\\n            num_commas = current_commas\\n        else:\\n            # If the number of commas doesn't match, it's not a valid CSV\\n            if current_commas != num_commas:\\n                return False\\n    \\n    return True\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['name,age\\\\nJohn,30\\\\nJane,25', True],\n",
       "   ['Just a simple string', False],\n",
       "   ['name;age\\\\nJohn;30\\\\nJane;25', False]]},\n",
       " {'instruction': '- Write your answer as a single, continuous string without spaces.',\n",
       "  'eval_func': [(\"def evaluate(response):\\n    return ' ' not in response\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['AnotherContinuousString123', True],\n",
       "   ['ThisIsAContinuousString', True],\n",
       "   ['This is not a continuous string', False],\n",
       "   ['This Is Not Continuous', False],\n",
       "   ['This is not continuous', False]]},\n",
       " {'instruction': '- Respond with a sequence of numbers in ascending order.',\n",
       "  'eval_func': [('def evaluate(response):\\n    try:\\n        # Split the response into a list of numbers\\n        numbers = list(map(int, response.split()))\\n        # Check if the list is sorted in ascending order\\n        return numbers == sorted(numbers)\\n    except ValueError:\\n        # If conversion to int fails, return False\\n        return False',\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['1 3 2 4 5', False], ['5 4 3 2 1', False], ['1 2 3 4 5', True]]},\n",
       " {'instruction': '- Write your answer using only lowercase letters.',\n",
       "  'eval_func': [('def evaluate(response: str) -> bool:\\n    return response.islower()',\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['this is all lowercase', True],\n",
       "   ['hello world', True],\n",
       "   ['this is a test', True],\n",
       "   ['Hello World', False],\n",
       "   ['another example with lowercase', True],\n",
       "   ['This Is Not All Lowercase', False]]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_generated_eval_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global generated_instructions\n",
    "generated_instructions = []\n",
    "def generate_instruction(k=2):\n",
    "    global filtered_generated_eval_functions, generated_instructions\n",
    "    count = 0\n",
    "    filter_count = 0\n",
    "\n",
    "    for line in tqdm(filtered_generated_eval_functions, desc=\"Generating back-translated instructions\"):\n",
    "        funcs = line[\"eval_func\"][:3]\n",
    "\n",
    "        instruction_prompt = f\"\"\"You are an expert in converting the Python eval function code into the corresponding instruction text. I will provide the eval function code. Please strictly follow the code to convert it into the corresponding instruction text. Here's an example: \n",
    "\n",
    "[[\"def evaluate(response):\\n    return 'e' not in response.lower()\", 1.0], [\"def evaluate(response):\\n    words = response.split()\\n    for word in response.split():\\n        if 'e' in word.lower():\\n            return False\\n    return True\", 1.0], [\"def evaluate(response):\\n    return all('e' not in word.lower() for word in response.split())\", 1.0]] \n",
    "\n",
    "[\"Answer without using any words that contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\"] Please convert the following eval function into instructions stored in a list: \n",
    "\n",
    "{funcs}\"\"\"\n",
    "        line[\"back_instruction\"] = []\n",
    "        for _ in range(k):\n",
    "            # input_ids = self.tokenizer.encode(instruction_prompt, return_tensors=\"pt\").cuda()\n",
    "            # outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "            # generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "            generated_text = llm_openai.invoke([{\"role\": \"user\", \"content\": instruction_prompt}]).content\n",
    "            try:\n",
    "                back_instruction = json.loads(generated_text)\n",
    "                break\n",
    "            except Exception:\n",
    "                filter_count += 1\n",
    "                continue\n",
    "        line[\"back_instruction\"] = back_instruction\n",
    "        generated_instructions.append(line)\n",
    "        count += 1\n",
    "    return generated_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating back-translated instructions: 100%|██████████| 9/9 [00:13<00:00,  1.46s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_instructions = generate_instruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '- Ensure your response is exactly 100 characters long.',\n",
       " 'eval_func': [('def evaluate(response):\\n    return len(response) == 100',\n",
       "   np.float64(1.0))],\n",
       " 'cases': [['This is a test string that is definitely not one hundred characters long.',\n",
       "   False],\n",
       "  ['bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb',\n",
       "   False],\n",
       "  ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "   True],\n",
       "  ['This string is definitely not one hundred characters long.', False],\n",
       "  ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "   False],\n",
       "  ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "   False],\n",
       "  ['This response is definitely not one hundred characters long.', False]],\n",
       " 'back_instruction': ['Provide a response that is exactly 100 characters long.']}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "global filtered_generated_instructions\n",
    "filtered_generated_instructions = []\n",
    "def filter_generated_instruction():\n",
    "    global generated_instructions, filtered_generated_instructions\n",
    "    count = 0 \n",
    "    filter_count = 0\n",
    "\n",
    "    for line in tqdm(generated_instructions, desc=\"Filtering back-translated instructions\"):\n",
    "        back_instructions = line[\"back_instruction\"]\n",
    "        ori_ins = line[\"instruction\"]\n",
    "\n",
    "        nli_scores = []\n",
    "        for back_ins in back_instructions[:3]:\n",
    "            premise = ori_ins\n",
    "            hypothesis = back_ins\n",
    "            inputs = tokenizer_nli(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "            output = model_nli(inputs[\"input_ids\"].cuda())\n",
    "            prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "            label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "            prediction_dict = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "            max_label = max(prediction_dict, key=prediction_dict.get)\n",
    "            nli_scores.append(max_label)\n",
    "\n",
    "        line[\"nli_scores\"] = nli_scores\n",
    "        if \"contradiction\" in nli_scores:\n",
    "            filter_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            filtered_generated_instructions.append(line)\n",
    "        count += 1\n",
    "    return filtered_generated_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering back-translated instructions: 100%|██████████| 9/9 [00:01<00:00,  8.88it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_generated_instructions = filter_generated_instruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': '- Ensure your response is exactly 100 characters long.',\n",
       "  'eval_func': [('def evaluate(response):\\n    return len(response) == 100',\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['This is a test string that is definitely not one hundred characters long.',\n",
       "    False],\n",
       "   ['bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb',\n",
       "    False],\n",
       "   ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "    True],\n",
       "   ['This string is definitely not one hundred characters long.', False],\n",
       "   ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "    False],\n",
       "   ['aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
       "    False],\n",
       "   ['This response is definitely not one hundred characters long.', False]],\n",
       "  'back_instruction': ['Provide a response that is exactly 100 characters long.'],\n",
       "  'nli_scores': ['entailment']},\n",
       " {'instruction': '- Respond with a list of exactly five items.',\n",
       "  'eval_func': [(\"def evaluate(response):\\n    # Split the response by commas and strip whitespace\\n    items = [item.strip() for item in response.split(',')]\\n    # Check if there are exactly five items\\n    return len(items) == 5\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['apple, banana, cherry', False],\n",
       "   ['apple, banana, cherry, date, fig', True],\n",
       "   ['apple, banana, cherry, date', False],\n",
       "   ['apple, banana, cherry, date, fig, grape', False]],\n",
       "  'back_instruction': ['Provide a response that contains exactly five items separated by commas.'],\n",
       "  'nli_scores': ['neutral']},\n",
       " {'instruction': '- Provide your answer in a JSON format.',\n",
       "  'eval_func': [('def evaluate(response):\\n    try:\\n        # Attempt to parse the response as JSON\\n        import json\\n        json.loads(response)\\n        return True\\n    except json.JSONDecodeError:\\n        # If a JSONDecodeError is raised, the response is not valid JSON\\n        return False',\n",
       "    np.float64(1.0)),\n",
       "   (\"def evaluate(response):\\n    try:\\n        # Try to parse the response as JSON\\n        import json\\n        json.loads(response)\\n        return True\\n    except json.JSONDecodeError:\\n        # If parsing fails, it's not valid JSON\\n        return False\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['[1, 2, 3]', True],\n",
       "   ['This is not JSON', False],\n",
       "   ['{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}', True],\n",
       "   ['{\"key\": \"value\"}', True]],\n",
       "  'back_instruction': ['Provide a response that contains exactly five items separated by commas.'],\n",
       "  'nli_scores': ['neutral']},\n",
       " {'instruction': '- Ensure your response is a palindrome.',\n",
       "  'eval_func': [(\"def evaluate(response):\\n    # Remove any non-alphanumeric characters and convert to lowercase\\n    cleaned_response = ''.join(char.lower() for char in response if char.isalnum())\\n    # Check if the cleaned response is a palindrome\\n    return cleaned_response == cleaned_response[::-1]\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['Was it a car or a cat I saw?', True],\n",
       "   ['A man, a plan, a canal, Panama', True],\n",
       "   [\"Madam In Eden, I'm Adam\", True],\n",
       "   ['Hello, World!', False]],\n",
       "  'back_instruction': ['Provide a response that is a palindrome after removing non-alphanumeric characters and converting to lowercase.'],\n",
       "  'nli_scores': ['neutral']},\n",
       " {'instruction': '- Format your response as a valid XML document.',\n",
       "  'eval_func': [('def evaluate(response):\\n    import xml.etree.ElementTree as ET\\n    try:\\n        ET.fromstring(response)\\n        return True\\n    except ET.ParseError:\\n        return False',\n",
       "    np.float64(1.0))],\n",
       "  'cases': [[\"<note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don't forget me this weekend!</body></note>\",\n",
       "    True],\n",
       "   [\"<note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don't forget me this weekend!</body>\",\n",
       "    False],\n",
       "   ['Just a plain text, not XML.', False]],\n",
       "  'back_instruction': ['Provide a valid XML response.',\n",
       "   'Submit a response that is well-formed XML.'],\n",
       "  'nli_scores': ['entailment', 'entailment']},\n",
       " {'instruction': '- Write your answer as a single, continuous string without spaces.',\n",
       "  'eval_func': [(\"def evaluate(response):\\n    return ' ' not in response\",\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['AnotherContinuousString123', True],\n",
       "   ['ThisIsAContinuousString', True],\n",
       "   ['This is not a continuous string', False],\n",
       "   ['This Is Not Continuous', False],\n",
       "   ['This is not continuous', False]],\n",
       "  'back_instruction': ['Answer without using any spaces in your response.'],\n",
       "  'nli_scores': ['entailment']},\n",
       " {'instruction': '- Respond with a sequence of numbers in ascending order.',\n",
       "  'eval_func': [('def evaluate(response):\\n    try:\\n        # Split the response into a list of numbers\\n        numbers = list(map(int, response.split()))\\n        # Check if the list is sorted in ascending order\\n        return numbers == sorted(numbers)\\n    except ValueError:\\n        # If conversion to int fails, return False\\n        return False',\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['1 3 2 4 5', False], ['5 4 3 2 1', False], ['1 2 3 4 5', True]],\n",
       "  'back_instruction': ['Answer with a list of numbers that are sorted in ascending order.',\n",
       "   'Provide a response that consists of numbers arranged from smallest to largest.'],\n",
       "  'nli_scores': ['entailment', 'neutral']},\n",
       " {'instruction': '- Write your answer using only lowercase letters.',\n",
       "  'eval_func': [('def evaluate(response: str) -> bool:\\n    return response.islower()',\n",
       "    np.float64(1.0))],\n",
       "  'cases': [['this is all lowercase', True],\n",
       "   ['hello world', True],\n",
       "   ['this is a test', True],\n",
       "   ['Hello World', False],\n",
       "   ['another example with lowercase', True],\n",
       "   ['This Is Not All Lowercase', False]],\n",
       "  'back_instruction': ['Answer using only lowercase letters.'],\n",
       "  'nli_scores': ['entailment']}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_generated_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global generated_responses\n",
    "generated_responses = []\n",
    "def generate_response(text, k=2):\n",
    "    global filtered_generated_instructions, generated_responses\n",
    "    for instruction in filtered_generated_instructions:\n",
    "        prompt = (\n",
    "            f\"Please answer the query strictly following the instruction.\\n\"\n",
    "            f\"[instruction] {instruction['instruction']}\\n\"\n",
    "            f\"[Query] {text}\"\n",
    "        )\n",
    "        \n",
    "        responses = []\n",
    "        for _ in range(k):\n",
    "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "            outputs = model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "            r1_generated_text = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True) # using r1\n",
    "            \n",
    "            generated_text = llm_openai.invoke([\n",
    "                    {\"role\": \"system\", \"content\": \"Strictly follow user's instructions.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Given below is an answer from an AI Assistant. You have to modify/reformat the answer such that it strictly follows a given instruction.\\n\"\n",
    "                                f\"--Example--\\n\"\n",
    "                                f\"Response: \"\n",
    "                                f\"Here is the answer from AI Assistant:\\n\\n\"\n",
    "                                f\"{r1_generated_text}\\n\"\n",
    "                                f\"Here is the instruction:\\n\"\n",
    "                                f\"{instruction['instruction']}\\n\\n\"\n",
    "                                f\"Do not use any other words or sentences in your response. Just modify the answer from AI Assistant and return it.\\n\"}\n",
    "                ]).content\n",
    "            responses.append(generated_text)\n",
    "            \n",
    "            \n",
    "        \n",
    "        generated_responses.append({\n",
    "            \"instruction\": instruction['instruction'],\n",
    "            \"prompt\": prompt,\n",
    "            \"gpt-answer\": responses,\n",
    "            \"eval_func\": instruction[\"eval_func\"],\n",
    "        })\n",
    "    return generated_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(filtered_generated_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '- Format your response as a valid XML document.',\n",
       " 'eval_func': [('def evaluate(response):\\n    import xml.etree.ElementTree as ET\\n    try:\\n        ET.fromstring(response)\\n        return True\\n    except ET.ParseError:\\n        return False',\n",
       "   np.float64(1.0))],\n",
       " 'cases': [[\"<note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don't forget me this weekend!</body></note>\",\n",
       "   True],\n",
       "  [\"<note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don't forget me this weekend!</body>\",\n",
       "   False],\n",
       "  ['Just a plain text, not XML.', False]],\n",
       " 'back_instruction': ['Provide a valid XML response.',\n",
       "  'Submit a response that is well-formed XML.'],\n",
       " 'nli_scores': ['entailment', 'entailment']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_generated_instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "global filtered_generated_instructions, generated_responses, r1_responses\n",
    "r1_responses = []\n",
    "k = 1\n",
    "for instruction in filtered_generated_instructions:\n",
    "    # prompt = (f\"Please answer the query strictly following the instruction.\\n\"\n",
    "    #          f\"[instruction] {instruction['instruction']}\\n\"\n",
    "    #          f\"[Query] {text}\")\n",
    "    prompt = text\n",
    "    \n",
    "    responses = []\n",
    "    for _ in range(k):\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "        outputs = model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "        r1_generated_text = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True) # using r1\n",
    "        \n",
    "        user_prompt = (f\"{r1_generated_text}\\n\"\n",
    "                      f\"Re-write the above text following: {instruction['instruction']}\\n\\n\"\n",
    "                      f\"Note: Use the same words and sentences but re-arrange them in a way that strictly follows the instruction.\\n\")\n",
    "        generated_text = llm_openai.invoke([\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]).content\n",
    "        r1_responses.append(r1_generated_text)\n",
    "        responses.append(generated_text)\n",
    "    #     break\n",
    "    # break\n",
    "        \n",
    "    \n",
    "    generated_responses.append({\n",
    "        \"instruction\": instruction['instruction'],\n",
    "        \"prompt\": prompt,\n",
    "        \"r1-answer\": r1_responses,\n",
    "        \"gpt-answer\": responses,\n",
    "        \"eval_func\": instruction[\"eval_func\"],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': '- Format your response as a valid XML document.',\n",
       "  'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'gpt-answer': ['```xml\\n<salesReport>\\n    <month name=\"April\">\\n        <sales>\\n            <amount>48</amount>\\n            <description>Natalia sold 48 clips in April.</description>\\n        </sales>\\n    </month>\\n    <month name=\"May\">\\n        <sales>\\n            <amount>24</amount>\\n            <description>She sold half as many clips in May as she did in April.</description>\\n            <calculation>48 / 2 = 24 clips</calculation>\\n        </sales>\\n    </month>\\n    <totalSales>\\n        <amount>72</amount>\\n        <description>Add April and May sales: 48 + 24 = 72 clips.</description>\\n    </totalSales>\\n    <finalAnswer>\\n        <text>Natalia sold a total of <value>72</value> clips in April and May.</text>\\n    </finalAnswer>\\n</salesReport>\\n```'],\n",
       "  'eval_func': [('def evaluate(response):\\n    import xml.etree.ElementTree as ET\\n    try:\\n        ET.fromstring(response)\\n        return True\\n    except ET.ParseError:\\n        return False',\n",
       "    np.float64(1.0))]},\n",
       " {'instruction': '- Respond with a list of exactly five items.',\n",
       "  'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'gpt-answer': ['1. **April Sales:**\\n   - Natalia sold **48** clips in April.\\n\\n2. **Total Sales:**\\n   - Add the number of clips sold in April and May.\\n   - Calculation:\\n     \\\\[\\n     48 \\\\ (\\\\text{April}) + 24 \\\\ (\\\\text{May}) = 72\\n     \\\\]\\n\\n3. **May Sales:**\\n   - She sold half as many clips in May as in April.\\n   - Calculation: \\n     \\\\[\\n     \\\\frac{48}{2} = 24\\n     \\\\]\\n   - So, she sold **24** clips in May.\\n\\n4. **Final Answer:**  \\nNatalia sold a total of \\\\(\\\\boxed{72}\\\\) clips in April and May.\\n\\n5. **Step-by-Step Explanation:**\\n   - 1. April Sales: Natalia sold 48 clips in April.\\n   - 2. May Sales: She sold half as many clips in May as she did in April.\\n   - 3. Total Sales: Add the number of clips sold in April and May.'],\n",
       "  'eval_func': [(\"def evaluate(response):\\n    # Split the response by commas and strip whitespace\\n    items = [item.strip() for item in response.split(',')]\\n    # Check if there are exactly five items\\n    return len(items) == 5\",\n",
       "    np.float64(1.0))]},\n",
       " {'instruction': '- Write your answer as a single, continuous string without spaces.',\n",
       "  'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'gpt-answer': ['Nataliasold48clipstoherfriendsinApril.InMay,shesoldhalfasmanyclipsas shedid inApril.Tofindthetotalnumberofclipssoldinbothmonths,wefirstcalculatethenumberofclipssoldinMay:\\\\[\\\\text{ClipssoldinMay}=\\\\frac{48}{2}=24\\\\]Next,weaddthenumberofclipssoldinAprilandMaytogether:\\\\[\\\\text{Totalclipssold}=48+24=72\\\\]ThusthetotalnumberofclipsNataliasoldinAprilandMayis\\\\boxed{72}.'],\n",
       "  'eval_func': [(\"def evaluate(response):\\n    return ' ' not in response\",\n",
       "    np.float64(1.0))]},\n",
       " {'instruction': '- Provide your answer in a JSON format.',\n",
       "  'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'gpt-answer': ['```json\\n{\\n  \"solution\": {\\n    \"clips_sold_in_april\": {\\n      \"description\": \"Natalia sold 48 clips in April.\",\\n      \"value\": 48\\n    },\\n    \"clips_sold_in_may\": {\\n      \"description\": \"She sold half as many clips in May.\",\\n      \"calculation\": \"Half of 48 is 24.\",\\n      \"value\": 24\\n    },\\n    \"total_clips_sold\": {\\n      \"description\": \"Now, let\\'s add the April sales to May sales.\",\\n      \"calculation\": \"Total sales = April sales + May sales = 48 + 24.\",\\n      \"value\": 72\\n    }\\n  },\\n  \"final_answer\": {\\n    \"value\": 72,\\n    \"formatted\": \"72\"\\n  }\\n}\\n```'],\n",
       "  'eval_func': [('def evaluate(response):\\n    try:\\n        # Attempt to parse the response as JSON\\n        import json\\n        json.loads(response)\\n        return True\\n    except json.JSONDecodeError:\\n        # If a JSONDecodeError is raised, the response is not valid JSON\\n        return False',\n",
       "    np.float64(1.0)),\n",
       "   (\"def evaluate(response):\\n    try:\\n        # Try to parse the response as JSON\\n        import json\\n        json.loads(response)\\n        return True\\n    except json.JSONDecodeError:\\n        # If parsing fails, it's not valid JSON\\n        return False\",\n",
       "    np.float64(1.0))]},\n",
       " {'instruction': '- Ensure your response is exactly 100 characters long.',\n",
       "  'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'gpt-answer': ['Natalia sold 48 clips in April. May sales were half of that: 24. Total sales: 48 + 24 = 72.'],\n",
       "  'eval_func': [('def evaluate(response):\\n    return len(response) == 100',\n",
       "    np.float64(1.0))]},\n",
       " {'instruction': '- Ensure your response is a palindrome.',\n",
       "  'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'gpt-answer': [\"Sure! Here’s a palindrome version of your text:\\n\\n72 total is overall, 24 in May, 48 in April. 72 total is overall. 24 in May, 48 in April. \\n\\nClips half as many sold in May, regardless of the number of friends. 48 in April, 24 in May. \\n\\n0.5 clips each, so total in May would be 48 * 0.5 = 24. \\n\\n0.5 clips per friend in May, which doesn't quite make sense because you can't sell half a clip. \\n\\n72 total is overall, 24 in May, 48 in April. \\n\\nSo total is still 72. \\n\\nSo 48 friends bought 48 clips in April. \\n\\nSo if she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she sold half as many clips in May, does that mean she sold half the number of clips in total, or half per friend? \\n\\nSo total in May is 24. \\n\\nSo altogether, she sold 48 + 24 = 72 clips. \\n\\nSo total is 72. \\n\\nSo in April, 48 friends bought 48 clips. \\n\\nSo if the number of friends is the same, and she\"],\n",
       "  'eval_func': [(\"def evaluate(response):\\n    # Remove any non-alphanumeric characters and convert to lowercase\\n    cleaned_response = ''.join(char.lower() for char in response if char.isalnum())\\n    # Check if the cleaned response is a palindrome\\n    return cleaned_response == cleaned_response[::-1]\",\n",
       "    np.float64(1.0))]},\n",
       " {'instruction': '- Write your answer using only lowercase letters.',\n",
       "  'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'gpt-answer': ['natalia sold a total of 72 clips in april and may.\\n\\n**step-by-step explanation:**\\n\\n1. **april sales:**\\n   - natalia sold 48 clips in april.\\n\\n2. **may sales:**\\n   - she sold half as many clips in may as she did in april.\\n   - half of 48 is calculated as:\\n     \\\\[\\n     \\\\frac{48}{2} = 24\\n     \\\\]\\n   - so, she sold 24 clips in may.\\n\\n3. **total sales:**\\n   - add the number of clips sold in april and may:\\n     \\\\[\\n     48 \\\\text{ (april)} + 24 \\\\text{ (may)} = 72\\n     \\\\] \\n\\n**answer:** natalia sold a total of 72 clips in april and may.'],\n",
       "  'eval_func': [('def evaluate(response: str) -> bool:\\n    return response.islower()',\n",
       "    np.float64(1.0))]},\n",
       " {'instruction': '- Respond with a sequence of numbers in ascending order.',\n",
       "  'prompt': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "  'gpt-answer': ['1. **Clips sold in April:**\\n   \\n   Natalia sold **48** clips in April.\\n\\n2. **Clips sold in May:**\\n   \\n   She sold **half as many** clips in May as she did in April.\\n   \\n   \\\\[\\n   \\\\text{Clips in May} = \\\\frac{48}{2} = 24\\n   \\\\]\\n\\n3. **Total clips sold in April and May:**\\n   \\n   \\\\[\\n   \\\\text{Total Clips} = \\\\text{April Clips} + \\\\text{May Clips} = 48 + 24 = 72\\n   \\\\]\\n\\n**Final Answer:**\\n\\n\\\\[\\n\\\\boxed{72}\\n\\\\]'],\n",
       "  'eval_func': [('def evaluate(response):\\n    try:\\n        # Split the response into a list of numbers\\n        numbers = list(map(int, response.split()))\\n        # Check if the list is sorted in ascending order\\n        return numbers == sorted(numbers)\\n    except ValueError:\\n        # If conversion to int fails, return False\\n        return False',\n",
       "    np.float64(1.0))]}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_responses = generate_response(\"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "global generated_responses, filtered_generated_responses\n",
    "filtered_generated_responses = []\n",
    "def filter_generated_response():\n",
    "    global generated_responses, filtered_generated_responses\n",
    "    filtered_samples = []\n",
    "    for result in tqdm(generated_responses, desc=\"Filtering back translated responses\"):\n",
    "        eval_funcs = []\n",
    "        for func, score in result['eval_func']:\n",
    "            local_vars = {}\n",
    "            try:\n",
    "                exec(func, globals(), local_vars)\n",
    "            except Exception as e:\n",
    "                print(\"Error executing eval function:\", e)\n",
    "                continue\n",
    "            if 'evaluate' in local_vars:\n",
    "                eval_funcs.append(local_vars['evaluate'])\n",
    "        \n",
    "        filter_responses = []\n",
    "        for response in result['gpt-answer']:\n",
    "            acc = []\n",
    "            for eval_func in eval_funcs:\n",
    "                try:\n",
    "                    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                    signal.alarm(5)\n",
    "                    res = eval_func(response)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    res = None\n",
    "                finally:\n",
    "                    signal.alarm(0)\n",
    "                if res is not None:\n",
    "                    try:\n",
    "                        acc.append(int(res))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            acc = np.mean(acc) if acc else 0\n",
    "            if acc > 0:\n",
    "                filter_responses.append(response)\n",
    "        \n",
    "        for each in filter_responses:\n",
    "            try:\n",
    "                query_match = re.findall(r'\\[Query\\](.*)$', result['prompt'], re.DOTALL)\n",
    "                query = query_match[0].strip() if query_match else \"\"\n",
    "                filtered_samples.append({\n",
    "                    'instruction': result['instruction'],\n",
    "                    'query': query,\n",
    "                    'response': each,\n",
    "                    \"eval_func\": result['eval_func'],\n",
    "                })\n",
    "            except IndexError:\n",
    "                print(\"Prompt extraction error:\", result['prompt'])\n",
    "    \n",
    "    filtered_generated_responses = list(map(json.loads, set(map(json.dumps, filtered_samples))))\n",
    "    return filtered_generated_responses\n",
    "\n",
    "global filtered2_generated_responses\n",
    "filtered2_generated_responses = []\n",
    "def filter2_generated_response(k=2): \n",
    "    global filtered_generated_responses, filtered2_generated_responses\n",
    "    prompt_template = (\n",
    "        \"You are an expert that is good at judging whether a response is following the instruction and query.\\n\"\n",
    "        \"[Instruction] {instruction}\\n\"\n",
    "        \"[Query] {query}\\n\"\n",
    "        \"[Response] {response}\\n\"\n",
    "        \"Please notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\\n\"\n",
    "        \"You need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\\n\"\n",
    "        \"Scoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\\n\"\n",
    "        \"Please only provide a score in the format `Score: {{score}}` without any other contents at the last line.\"\n",
    "    )\n",
    "    for each in filtered_generated_responses:\n",
    "        each['prompt'] = prompt_template.format(\n",
    "            instruction=each['instruction'],\n",
    "            query=each['query'],\n",
    "            response=each['response']\n",
    "        )\n",
    "        each['gen'] = []\n",
    "        for _ in range(k):\n",
    "            # input_ids = self.tokenizer.encode(each['prompt'], return_tensors=\"pt\").cuda()\n",
    "            # outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "            # generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "            generated_text = llm_openai.invoke({\"role\": \"user\", \"content\": each['prompt']}).content\n",
    "            each['gen'].append(generated_text)\n",
    "        \n",
    "        scores = []\n",
    "        for each in each['gen']:\n",
    "            score = re.findall(r'Score: (\\d+?)$', each)\n",
    "            if score:\n",
    "                scores.append(int(score[0]))\n",
    "        score = np.mean(scores) if scores else 0\n",
    "        if score > 8: # quality score\n",
    "            filtered2_generated_responses.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pablo Picasso (1881-1973) was a Spanish artist, widely regarded as one of the most influential and versatile artists of the 20th century. He is best known for his pioneering work in Cubism, a style of art that emphasizes geometric forms and fragmented representations of reality.\\n\\n**Early Life and Training**\\n\\nPicasso was born in Málaga, Spain, to a family of artists. His father, José Ruiz Blasco, was a painter and art teacher, and his mother, María Picasso y López, was a homemaker. Picasso\\'s early life was marked by a strong interest in art, and he began drawing and painting at a young age. He received formal training at the Barcelona Academy of Fine Arts, where he studied from 1895 to 1897.\\n\\n**Artistic Career**\\n\\nPicasso\\'s artistic career spanned over seven decades and can be divided into several distinct periods:\\n\\n1. **Blue Period (1901-1904)**: Characterized by somber, blue-toned paintings of poverty, misery, and social injustice.\\n2. **Rose Period (1904-1906)**: Marked by a shift towards more vibrant, rose-colored paintings of circus performers, acrobats, and musicians.\\n3. **African Period (1907-1909)**: Influenced by African art, Picasso\\'s work became more abstract and primitive.\\n4. **Cubist Period (1909-1912)**: Picasso, along with Georges Braque, developed Cubism, a revolutionary style that rejected traditional techniques of perspective and representation.\\n5. **Surrealist Period (1920s-1930s)**: Picasso\\'s work became more dreamlike and fantastical, often incorporating elements of mythology and fantasy.\\n6. **Later Work (1940s-1970s)**: Picasso continued to experiment with various styles, including sculpture, printmaking, and ceramics.\\n\\n**Notable Works**\\n\\nSome of Picasso\\'s most famous works include:\\n\\n* **\"Guernica\" (1937)**: A powerful anti-war painting responding to the bombing of the town of Guernica during the Spanish Civil War.\\n* **\"Les Demoiselles d\\'Avignon\" (1907)**: A pioneering work of Cubism, depicting five nude women with African mask-like faces.\\n* **\"Girl with a Mandolin\" (1910)**: A Cubist painting of a woman playing a mandolin, showcasing Picasso\\'s innovative use of geometric forms.\\n\\n**Legacy**\\n\\nPablo Picasso\\'s impact on modern art is immeasurable. He:\\n\\n* Revolutionized the way artists represent reality\\n* Pioneered Cubism, a style that influenced countless artists\\n* Experimented with various mediums, including sculpture, printmaking, and ceramics\\n* Created over 50,000 artworks during his lifetime, including paintings, drawings, and sculptures\\n\\nPicasso\\'s innovative and prolific career has made him one of the most celebrated and influential artists of the 20th century, and his work continues to inspire and fascinate audiences around the world.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client(api_key=\"sk-rc-52dYCY3xghaaETCPTdzR_g\", base_url=\"https://fmapi.swissai.cscs.ch\")\n",
    "res = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\": \"Who is Pablo Picasso?\", \n",
    "            \"role\": \"user\",\n",
    "        }\n",
    "    ],\n",
    "    # stream=True,\n",
    ")\n",
    "\n",
    "# for chunk in res:\n",
    "#     if len(chunk.choices) > 0 and chunk.choices[0].delta.content:\n",
    "#         print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering back translated responses: 100%|██████████| 8/8 [00:00<00:00, 3828.23it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m filtered_generated_responses\n\u001b[1;32m      2\u001b[0m filtered_generated_responses \u001b[38;5;241m=\u001b[39m filter_generated_response()\n\u001b[0;32m----> 3\u001b[0m filtered2_generated_responses \u001b[38;5;241m=\u001b[39m \u001b[43mfilter2_generated_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 81\u001b[0m, in \u001b[0;36mfilter2_generated_response\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m     76\u001b[0m each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# input_ids = self.tokenizer.encode(each['prompt'], return_tensors=\"pt\").cuda()\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m \u001b[43mllm_openai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meach\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     82\u001b[0m     each[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(generated_text)\n\u001b[1;32m     84\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.conda/envs/r1/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:332\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    327\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    328\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m--> 332\u001b[0m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[1;32m    333\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    334\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    335\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    336\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    337\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    338\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    339\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    340\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    341\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.conda/envs/r1/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:317\u001b[0m, in \u001b[0;36mBaseChatModel._convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages\u001b[38;5;241m=\u001b[39mconvert_to_messages(\u001b[38;5;28minput\u001b[39m))\n\u001b[1;32m    313\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m )\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "global filtered_generated_responses\n",
    "filtered_generated_responses = filter_generated_response()\n",
    "filtered2_generated_responses = filter2_generated_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered2_generated_responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/share/u/harshraj/CotIF/data-v2/filtered2_generated_responses.json\", \"w\") as f:\n",
    "    json.dump(filtered2_generated_responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-22 17:48:14 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 17:48:22,927\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Callable, Tuple, Any, Optional\n",
    "from functools import lru_cache\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langdetect import detect, DetectorFactory, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 04-22 17:48:23 [config.py:2836] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-22 17:48:41 [config.py:689] This model supports multiple tasks: {'reward', 'score', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 04-22 17:48:41 [config.py:1901] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 04-22 17:48:44 [core.py:61] Initializing a V1 LLM engine (v0.8.4) with config: model='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Llama-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Llama-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 04-22 17:48:47 [utils.py:2444] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f87ec098ec0>\n",
      "INFO 04-22 17:48:47 [parallel_state.py:959] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 04-22 17:48:47 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 04-22 17:48:47 [gpu_model_runner.py:1276] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Llama-8B...\n",
      "ERROR 04-22 17:48:48 [core.py:387] EngineCore hit an exception: Traceback (most recent call last):\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 378, in run_engine_core\n",
      "ERROR 04-22 17:48:48 [core.py:387]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 04-22 17:48:48 [core.py:387]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 320, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     super().__init__(vllm_config, executor_class, log_stats)\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 67, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 04-22 17:48:48 [core.py:387]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self._init_executor()\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.collective_rpc(\"load_model\")\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 04-22 17:48:48 [core.py:387]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 04-22 17:48:48 [core.py:387]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/utils.py\", line 2378, in run_method\n",
      "ERROR 04-22 17:48:48 [core.py:387]     return func(*args, **kwargs)\n",
      "ERROR 04-22 17:48:48 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 136, in load_model\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.model_runner.load_model()\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1279, in load_model\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 04-22 17:48:48 [core.py:387]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/__init__.py\", line 14, in get_model\n",
      "ERROR 04-22 17:48:48 [core.py:387]     return loader.load_model(vllm_config=vllm_config)\n",
      "ERROR 04-22 17:48:48 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 452, in load_model\n",
      "ERROR 04-22 17:48:48 [core.py:387]     model = _initialize_model(vllm_config=vllm_config)\n",
      "ERROR 04-22 17:48:48 [core.py:387]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/model_loader/loader.py\", line 133, in _initialize_model\n",
      "ERROR 04-22 17:48:48 [core.py:387]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 04-22 17:48:48 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 486, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.model = self._init_model(vllm_config=vllm_config,\n",
      "ERROR 04-22 17:48:48 [core.py:387]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 527, in _init_model\n",
      "ERROR 04-22 17:48:48 [core.py:387]     return LlamaModel(vllm_config=vllm_config,\n",
      "ERROR 04-22 17:48:48 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 321, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "ERROR 04-22 17:48:48 [core.py:387]                                                     ^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/utils.py\", line 610, in make_layers\n",
      "ERROR 04-22 17:48:48 [core.py:387]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "ERROR 04-22 17:48:48 [core.py:387]                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 323, in <lambda>\n",
      "ERROR 04-22 17:48:48 [core.py:387]     lambda prefix: layer_type(config=config,\n",
      "ERROR 04-22 17:48:48 [core.py:387]                    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 239, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.self_attn = LlamaAttention(\n",
      "ERROR 04-22 17:48:48 [core.py:387]                      ^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 142, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.qkv_proj = QKVParallelLinear(\n",
      "ERROR 04-22 17:48:48 [core.py:387]                     ^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 833, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     super().__init__(input_size=input_size,\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 398, in __init__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     self.quant_method.create_weights(\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/vllm/model_executor/layers/linear.py\", line 178, in create_weights\n",
      "ERROR 04-22 17:48:48 [core.py:387]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 04-22 17:48:48 [core.py:387]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387]   File \"/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 04-22 17:48:48 [core.py:387]     return func(*args, **kwargs)\n",
      "ERROR 04-22 17:48:48 [core.py:387]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 04-22 17:48:48 [core.py:387] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 18.38 MiB is free. Process 1963873 has 43.12 GiB memory in use. Including non-PyTorch memory, this process has 4.25 GiB memory in use. Of the allocated memory 3.92 GiB is allocated by PyTorch, and 29.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "ERROR 04-22 17:48:48 [core.py:387] \n",
      "CRITICAL 04-22 17:48:48 [core_client.py:359] Got fatal signal from worker processes, shutting down. See stack trace above for root cause issue.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize vLLM model with tensor parallelism for multi-GPU\n",
    "model = LLM(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    dtype=\"float16\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# For vLLM, we still need the tokenizer from Hugging Face for chat templates\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "\n",
    "# Handle custom chat template if needed\n",
    "to_delete = \"{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}\"\n",
    "if hasattr(tokenizer, 'get_chat_template') and to_delete in tokenizer.get_chat_template():\n",
    "    new_template = tokenizer.get_chat_template().replace(to_delete, \"\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\", \n",
    "        chat_template=new_template,\n",
    "        trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import SamplingParams\n",
    "\n",
    "# Prepare prompts from chat templates\n",
    "prompts = []\n",
    "for messages in batch_messages:\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    prompts.append(prompt)\n",
    "\n",
    "# Set generation parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=max_new_tokens,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Generate completions in batch\n",
    "outputs = model.generate(prompts, sampling_params)\n",
    "\n",
    "# Extract generated text\n",
    "results = []\n",
    "for i, output in enumerate(outputs):\n",
    "    full_response = output.outputs[0].text\n",
    "    results.append(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/harshraj/.conda/envs/vllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import signal\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    \"\"\"Handler for function execution timeout.\"\"\"\n",
    "    raise TimeoutError(\"Function execution timed out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIf:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_model_name: str = \"meta-llama/Llama-3-8B-Instruct\",\n",
    "        nli_model_name: str = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n",
    "        seed_instructions_path: Optional[str] = \"/share/u/harshraj/CotIF/data/seed_instruction.txt\",\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        max_length: int = 1024,\n",
    "        temperature: float = 0.7,\n",
    "        use_vllm: bool = False,\n",
    "        tensor_parallel_size: int = 1,\n",
    "        gpu_memory_utilization: float = 0.9,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the AutoIf class with HuggingFace models.\n",
    "        \n",
    "        Args:\n",
    "            llm_model_name: The HuggingFace model name for the language model\n",
    "            nli_model_name: The HuggingFace model name for the NLI model\n",
    "            device: Device to run the models on (cuda or cpu)\n",
    "            max_length: Maximum length for text generation\n",
    "            temperature: Temperature for text generation\n",
    "            seed_instructions_path: Path to a file containing seed instructions\n",
    "            use_vllm: Whether to use vLLM for faster inference\n",
    "            tensor_parallel_size: Number of GPUs to use for tensor parallelism with vLLM\n",
    "            gpu_memory_utilization: Fraction of GPU memory to use (for vLLM)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Initializing AutoIf with LLM: {llm_model_name} and NLI: {nli_model_name}\")\n",
    "        \n",
    "        # Set up LLM model\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "        self.temperature = temperature\n",
    "        self.use_vllm = use_vllm\n",
    "        self.vllm_model = None\n",
    "        \n",
    "        # Initialize tokenizer first (needed for both backends)\n",
    "        logger.info(\"Loading LLM tokenizer...\")\n",
    "        self.llm_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "        \n",
    "        # Handle custom chat template if needed (especially for DeepSeek models)\n",
    "        to_delete = \"{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}\"\n",
    "        if hasattr(self.llm_tokenizer, 'get_chat_template') and to_delete in self.llm_tokenizer.get_chat_template():\n",
    "            new_template = self.llm_tokenizer.get_chat_template().replace(to_delete, \"\")\n",
    "            self.llm_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                llm_model_name, \n",
    "                chat_template=new_template,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        \n",
    "        # Setup according to backend preference\n",
    "        if use_vllm:\n",
    "            try:\n",
    "                from vllm import LLM, SamplingParams\n",
    "                \n",
    "                logger.info(f\"Loading LLM model with vLLM backend (tensor_parallel_size={tensor_parallel_size})...\")\n",
    "                # Initialize vLLM model with tensor parallelism for multi-GPU\n",
    "                self.vllm_model = LLM(\n",
    "                    model=llm_model_name,\n",
    "                    tensor_parallel_size=tensor_parallel_size,\n",
    "                    gpu_memory_utilization=gpu_memory_utilization,\n",
    "                    dtype=\"float16\",\n",
    "                    trust_remote_code=True,\n",
    "                )\n",
    "                logger.info(\"vLLM model loaded successfully\")\n",
    "                \n",
    "                # No need for HF pipeline with vLLM\n",
    "                self.llm_model = None\n",
    "                self.llm_pipeline = None\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"vLLM initialization failed with error: {str(e)}. Falling back to HuggingFace implementation.\")\n",
    "                self.use_vllm = False\n",
    "        \n",
    "        # If vLLM is not used or failed to initialize, use HuggingFace\n",
    "        if not self.use_vllm:\n",
    "            logger.info(\"Loading LLM model with HuggingFace backend...\")\n",
    "            self.llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                llm_model_name,\n",
    "                torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "                device_map=device\n",
    "            )\n",
    "            \n",
    "            self.llm_pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.llm_model,\n",
    "                tokenizer=self.llm_tokenizer,\n",
    "                # device=device if device != \"cuda\" else 0,\n",
    "            )\n",
    "        \n",
    "        # Initialize NLI model\n",
    "        logger.info(\"Loading NLI model and tokenizer...\")\n",
    "        self.tokenizer_nli = AutoTokenizer.from_pretrained(nli_model_name)\n",
    "        self.model_nli = AutoModelForSequenceClassification.from_pretrained(nli_model_name)\n",
    "        self.model_nli.to(device)\n",
    "        self.model_nli.eval()\n",
    "        \n",
    "        # Load seed instructions\n",
    "        self.seed_instructions = []\n",
    "        if seed_instructions_path and os.path.exists(seed_instructions_path):\n",
    "            with open(seed_instructions_path, 'r', encoding='utf-8') as f:\n",
    "                self.seed_instructions = [line.strip() for line in f.readlines() if line.strip()]\n",
    "            logger.info(f\"Loaded {len(self.seed_instructions)} seed instructions from {seed_instructions_path}\")\n",
    "        \n",
    "        # Initialize result trackers\n",
    "        self.reset_results()\n",
    "    \n",
    "    def reset_results(self):\n",
    "        \"\"\"Reset all result trackers.\"\"\"\n",
    "        self.generated_eval_functions = []\n",
    "        self.filtered_generated_eval_functions = []\n",
    "        self.generated_instructions = []\n",
    "        self.filtered_generated_instructions = []\n",
    "        self.generated_responses = []\n",
    "        self.filtered_generated_responses = []\n",
    "        self.filtered2_generated_responses = []\n",
    "        self.preference_data = []\n",
    "    \n",
    "    def call_llm(\n",
    "        self, \n",
    "        prompt: str, \n",
    "        max_new_tokens: int = 512, \n",
    "        temperature: Optional[float] = None,\n",
    "        max_retries: int = 3,\n",
    "        backoff_factor: float = 2.0\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Call the language model with retry logic.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The input prompt\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            temperature: Temperature for sampling, overrides self.temperature if provided\n",
    "            max_retries: Maximum number of retries on failure\n",
    "            backoff_factor: Exponential backoff factor\n",
    "            \n",
    "        Returns:\n",
    "            The generated text\n",
    "        \"\"\"\n",
    "        temp = temperature if temperature is not None else self.temperature\n",
    "        \n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                # Use vLLM if available\n",
    "                if self.use_vllm and self.vllm_model is not None:\n",
    "                    from vllm import SamplingParams\n",
    "                    \n",
    "                    # Set generation parameters\n",
    "                    sampling_params = SamplingParams(\n",
    "                        temperature=temp,\n",
    "                        max_tokens=max_new_tokens,\n",
    "                        top_p=0.95,\n",
    "                    )\n",
    "                    \n",
    "                    # Generate text\n",
    "                    outputs = self.vllm_model.generate(prompt, sampling_params)\n",
    "                    \n",
    "                    # Extract and return generated text\n",
    "                    generated_text = outputs[0].outputs[0].text\n",
    "                    \n",
    "                    # Remove prompt from output if it's included\n",
    "                    if generated_text.startswith(prompt):\n",
    "                        generated_text = generated_text[len(prompt):]\n",
    "                    \n",
    "                    return generated_text.strip()\n",
    "                \n",
    "                # Otherwise use HuggingFace\n",
    "                else:\n",
    "                    # Generate text\n",
    "                    outputs = self.llm_pipeline(\n",
    "                        prompt,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        do_sample=(temp > 0),\n",
    "                        temperature=temp,\n",
    "                        num_return_sequences=1,\n",
    "                        pad_token_id=self.llm_tokenizer.eos_token_id\n",
    "                    )\n",
    "                    \n",
    "                    # Extract and return generated text\n",
    "                    generated_text = outputs[0][\"generated_text\"]\n",
    "                    \n",
    "                    # Remove prompt from output if it's included\n",
    "                    if generated_text.startswith(prompt):\n",
    "                        generated_text = generated_text[len(prompt):]\n",
    "                    \n",
    "                    return generated_text.strip()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in LLM call (attempt {attempt}/{max_retries}): {str(e)}\")\n",
    "                if attempt < max_retries:\n",
    "                    sleep_time = backoff_factor ** (attempt - 1)\n",
    "                    logger.info(f\"Retrying in {sleep_time:.1f} seconds...\")\n",
    "                    time.sleep(sleep_time)\n",
    "                else:\n",
    "                    logger.error(\"Max retries reached, giving up\")\n",
    "                    raise RuntimeError(f\"LLM call failed after {max_retries} attempts\") from e\n",
    "        \n",
    "    def batch_call_llm(\n",
    "        self, \n",
    "        prompts: List[str], \n",
    "        max_new_tokens: int = 512, \n",
    "        temperature: Optional[float] = None,\n",
    "        batch_size: int = 4,\n",
    "        show_progress: bool = True\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Call LLM on a batch of prompts.\n",
    "        \n",
    "        Args:\n",
    "            prompts: List of input prompts\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            temperature: Temperature for sampling, overrides self.temperature if provided\n",
    "            batch_size: Batch size for processing\n",
    "            show_progress: Whether to show progress bar\n",
    "            \n",
    "        Returns:\n",
    "            List of generated texts\n",
    "        \"\"\"\n",
    "        if not prompts:\n",
    "            return []\n",
    "        \n",
    "        results = []\n",
    "        temp = temperature if temperature is not None else self.temperature\n",
    "        \n",
    "        # Use vLLM for batched inference if available\n",
    "        if self.use_vllm and self.vllm_model is not None:\n",
    "            try:\n",
    "                from vllm import SamplingParams\n",
    "                \n",
    "                # Set generation parameters\n",
    "                sampling_params = SamplingParams(\n",
    "                    temperature=temp,\n",
    "                    max_tokens=max_new_tokens,\n",
    "                    top_p=0.95,\n",
    "                )\n",
    "                \n",
    "                # Process in batches to avoid OOM\n",
    "                iterator = range(0, len(prompts), batch_size)\n",
    "                if show_progress:\n",
    "                    iterator = tqdm(iterator, desc=\"vLLM batch processing\", total=(len(prompts) + batch_size - 1) // batch_size)\n",
    "                \n",
    "                for i in iterator:\n",
    "                    batch_prompts = prompts[i:i+batch_size]\n",
    "                    \n",
    "                    # Generate text for batch\n",
    "                    outputs = self.vllm_model.generate(batch_prompts, sampling_params)\n",
    "                    \n",
    "                    # Extract generated texts\n",
    "                    batch_results = []\n",
    "                    for j, output in enumerate(outputs):\n",
    "                        generated_text = output.outputs[0].text\n",
    "                        prompt = batch_prompts[j] if j < len(batch_prompts) else \"\"\n",
    "                        \n",
    "                        # Remove prompt from output if it's included\n",
    "                        if prompt and generated_text.startswith(prompt):\n",
    "                            generated_text = generated_text[len(prompt):]\n",
    "                        \n",
    "                        batch_results.append(generated_text.strip())\n",
    "                    \n",
    "                    results.extend(batch_results)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in vLLM batch processing: {str(e)}. Falling back to individual processing.\")\n",
    "                # Fall back to individual processing\n",
    "                for prompt in prompts:\n",
    "                    try:\n",
    "                        result = self.call_llm(prompt, max_new_tokens, temperature)\n",
    "                        results.append(result)\n",
    "                    except Exception as sub_e:\n",
    "                        logger.error(f\"Error in individual prompt processing: {str(sub_e)}\")\n",
    "                        results.append(\"\")  # Add empty string on error\n",
    "                \n",
    "        else:\n",
    "            # Use HuggingFace pipeline for batched inference\n",
    "            # Process in smaller batches to avoid OOM\n",
    "            iterator = range(0, len(prompts), batch_size)\n",
    "            if show_progress:\n",
    "                iterator = tqdm(iterator, desc=\"HF batch processing\", total=(len(prompts) + batch_size - 1) // batch_size)\n",
    "            \n",
    "            for i in iterator:\n",
    "                batch_prompts = prompts[i:i+batch_size]\n",
    "                try:\n",
    "                    # Generate text for batch\n",
    "                    outputs = self.llm_pipeline(\n",
    "                        batch_prompts,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        do_sample=(temp > 0),\n",
    "                        temperature=temp,\n",
    "                        num_return_sequences=1,\n",
    "                        pad_token_id=self.llm_tokenizer.eos_token_id\n",
    "                    )\n",
    "                    \n",
    "                    # Extract generated texts\n",
    "                    batch_results = []\n",
    "                    for prompt, output in zip(batch_prompts, outputs):\n",
    "                        generated_text = output[0][\"generated_text\"]\n",
    "                        # Remove prompt from output if it's included\n",
    "                        if generated_text.startswith(prompt):\n",
    "                            generated_text = generated_text[len(prompt):]\n",
    "                        batch_results.append(generated_text.strip())\n",
    "                    \n",
    "                    results.extend(batch_results)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # On error, fall back to individual processing\n",
    "                    logger.error(f\"Error in HF batch processing: {str(e)}. Falling back to individual processing.\")\n",
    "                    for prompt in batch_prompts:\n",
    "                        try:\n",
    "                            result = self.call_llm(prompt, max_new_tokens, temperature)\n",
    "                            results.append(result)\n",
    "                        except Exception as sub_e:\n",
    "                            logger.error(f\"Error in individual prompt processing: {str(sub_e)}\")\n",
    "                            results.append(\"\")  # Add empty string on error\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compile(\n",
    "        self, \n",
    "        dataset, \n",
    "        batch_size: int = 8, \n",
    "        k_eval: int = 1, \n",
    "        k_instruction: int = 2, \n",
    "        k_response: int = 2, \n",
    "        k_filter: int = 2,\n",
    "        max_samples: int = 0,\n",
    "        show_progress: bool = True\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Process a dataset with batch processing capability.\n",
    "        \n",
    "        Args:\n",
    "            dataset: The dataset to process\n",
    "            batch_size: Batch size for processing\n",
    "            k_eval: Number of evaluation function generations per instruction\n",
    "            k_instruction: Number of attempts for instruction generation\n",
    "            k_response: Number of response generations per instruction\n",
    "            k_filter: Number of filter attempts\n",
    "            max_samples: Maximum number of samples to process (0 for all)\n",
    "            show_progress: Whether to show progress bars\n",
    "            \n",
    "        Returns:\n",
    "            List of processed results\n",
    "        \"\"\"\n",
    "        logger.info(f\"Starting compile with batch_size={batch_size}, max_samples={max_samples if max_samples > 0 else 'all'}\")\n",
    "        \n",
    "        # Reset results for this run\n",
    "        self.reset_results()\n",
    "        \n",
    "        # Limit number of samples if specified\n",
    "        data_size = len(dataset)\n",
    "        if max_samples > 0 and max_samples < data_size:\n",
    "            logger.info(f\"Limiting processing to {max_samples} samples from {data_size} total\")\n",
    "            processing_dataset = dataset.select(range(args.max_samples))\n",
    "        else:\n",
    "            processing_dataset = dataset\n",
    "        # logger.info(f\"processing_dataset: {processing_dataset}\")\n",
    "        \n",
    "        # Generate seed instructions\n",
    "        logger.info(\"Generating seed instructions...\")\n",
    "        self.seed_instructions = self.generate_seed(self.seed_instructions)\n",
    "        \n",
    "        # Generate and filter evaluation functions\n",
    "        logger.info(\"Generating evaluation functions...\")\n",
    "        self.generated_eval_functions = self.generate_eval_function(\n",
    "            self.seed_instructions, k=k_eval, batch_size=batch_size, show_progress=show_progress\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Filtering evaluation functions...\")\n",
    "        self.filtered_generated_eval_functions = self.filter_generated_eval_function(\n",
    "            self.generated_eval_functions, show_progress=show_progress\n",
    "        )\n",
    "        \n",
    "        # Generate and filter instructions\n",
    "        logger.info(\"Generating instructions...\")\n",
    "        self.generated_instructions = self.generate_instruction(\n",
    "            self.filtered_generated_eval_functions, k=k_instruction, batch_size=batch_size, show_progress=show_progress\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Filtering instructions...\")\n",
    "        self.filtered_generated_instructions = self.filter_generated_instruction(\n",
    "            self.generated_instructions, batch_size=batch_size, show_progress=show_progress\n",
    "        )\n",
    "        \n",
    "        # Process each data point in the dataset\n",
    "        output = []\n",
    "        iterator = processing_dataset\n",
    "        if show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Processing dataset\", total=len(processing_dataset))\n",
    "        \n",
    "        for batch_start in range(0, len(processing_dataset), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(processing_dataset))\n",
    "            # current_batch = processing_dataset[batch_start:batch_end]\n",
    "            current_batch = processing_dataset.select(range(batch_start, batch_end))\n",
    "            \n",
    "            batch_results = []\n",
    "            for datum in current_batch:\n",
    "                try:\n",
    "                    # Extract query and generated text\n",
    "                    messages = datum.get(\"conversations\", [])\n",
    "                    if len(messages) > 1 and messages[0].get(\"from\") == \"user\" and messages[1].get(\"from\") == \"assistant\":\n",
    "                        query, r1_generated_text = messages[0].get(\"value\", \"\"), messages[1].get(\"value\", \"\")\n",
    "                        \n",
    "                        # Skip if query or response is empty\n",
    "                        if not query or not r1_generated_text:\n",
    "                            logger.warning(\"Skipping datum with empty query or response\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Generate and filter responses\n",
    "                        self.generated_responses = self.generate_response(\n",
    "                            query, r1_generated_text, self.filtered_generated_instructions, \n",
    "                            k=k_response, batch_size=batch_size, show_progress=False\n",
    "                        )\n",
    "                        \n",
    "                        self.filtered_generated_responses = self.filter_generated_response(\n",
    "                            self.generated_responses, show_progress=False\n",
    "                        )\n",
    "                        \n",
    "                        self.filtered2_generated_responses = self.filter2_generated_response(\n",
    "                            self.filtered_generated_responses, k=k_filter, batch_size=batch_size, show_progress=False\n",
    "                        )\n",
    "                        \n",
    "                        # Collect results\n",
    "                        batch_results.append({\n",
    "                            \"query\": query,\n",
    "                            \"original_response\": r1_generated_text,\n",
    "                            \"seed_instructions\": self.seed_instructions,\n",
    "                            \"generated_eval_functions\": self.generated_eval_functions,\n",
    "                            \"filtered_generated_eval_functions\": self.filtered_generated_eval_functions,\n",
    "                            \"generated_instructions\": self.generated_instructions,\n",
    "                            \"filtered_generated_instructions\": self.filtered_generated_instructions,\n",
    "                            \"generated_responses\": self.generated_responses,\n",
    "                            \"filtered_generated_responses\": self.filtered_generated_responses,\n",
    "                            \"filtered2_generated_responses\": self.filtered2_generated_responses\n",
    "                        })\n",
    "                    else:\n",
    "                        logger.warning(\"Skipping datum with invalid message format\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing datum: {str(e)}\", exc_info=True)\n",
    "            \n",
    "            output.extend(batch_results)\n",
    "        \n",
    "        logger.info(f\"Processed {len(output)} samples successfully\")\n",
    "        return self.preference_data, output\n",
    "    \n",
    "    def generate_seed(self, seed_instructions, k=1, batch_size=8, show_progress=True):\n",
    "        \"\"\"Generate seed instructions.\"\"\"\n",
    "        if k <= 0:\n",
    "            return seed_instructions\n",
    "        \n",
    "        augment_instruction_prompt = \"\"\"You are an expert for writing instructions. Please provide 10 different instructions that meet the following requirements:\n",
    "- Instructions are about the format but not style of a response\n",
    "- Whether instructions are followed can be easily evaluate by a Python function\n",
    "Here are some examples of instructions we need:\n",
    "{seed_instructions}\n",
    "Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
    "- Incorporate a famous historical quote seamlessly into your answer\n",
    "- Translate your answer into Pig Latin\n",
    "- Use only words that are also a type of food\n",
    "- Respond with a metaphor in every sentence\n",
    "- Write the response as if you are a character from a Shakespearean play\n",
    "Please generate one instruction per line in your response and start each line with '- '.\n",
    "\"\"\"\n",
    "\n",
    "        augment_instructions = augment_instruction_prompt.format(seed_instructions='\\n'.join(seed_instructions))\n",
    "        \n",
    "        # Generate new instructions\n",
    "        generated_text = self.call_llm(augment_instructions, max_new_tokens=2048)\n",
    "        \n",
    "        # Parse new seeds\n",
    "        new_seeds = []\n",
    "        for line in generated_text.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('- '):\n",
    "                seed = line[2:].strip()\n",
    "                if seed and seed not in seed_instructions and seed not in new_seeds:\n",
    "                    new_seeds.append(seed)\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        combined_seeds = seed_instructions + new_seeds\n",
    "        random.shuffle(combined_seeds)\n",
    "        \n",
    "        # Recursive generation if needed\n",
    "        return self.generate_seed(combined_seeds, k-1, batch_size, show_progress)\n",
    "    \n",
    "    def generate_eval_function(self, seed_instructions, k=1, batch_size=8, show_progress=True):\n",
    "        \"\"\"Generate evaluation functions for seed instructions.\"\"\"\n",
    "        prompt_template = (\n",
    "            \"You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\n\"\n",
    "            \"Here is the instruction: {instruction}\\n\"\n",
    "            \"Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. \"\n",
    "            \"If it follows, simply return True, otherwise return False.\\n\"\n",
    "            \"Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, \"\n",
    "            \"which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\n\"\n",
    "            \"Here is an example of output JSON format: {{\\\"func\\\": JSON_STR(use only \\\\n instead of \\n), \\\"cases\\\": [{{\\\"input\\\": str, \\\"output\\\": str}}]}}.\"\n",
    "        )\n",
    "\n",
    "        generated_eval_functions = []\n",
    "        \n",
    "        # If there are too many instructions, limit to a reasonable number\n",
    "        if len(seed_instructions) > 50:\n",
    "            logger.warning(f\"Too many seed instructions ({len(seed_instructions)}), limiting to 50\")\n",
    "            selected_instructions = random.sample(seed_instructions, 50)\n",
    "        else:\n",
    "            selected_instructions = seed_instructions\n",
    "        \n",
    "        # Build all prompts first\n",
    "        all_prompts = []\n",
    "        for instruction in selected_instructions:\n",
    "            for _ in range(k):\n",
    "                all_prompts.append(prompt_template.format(instruction=instruction))\n",
    "        \n",
    "        # Process prompts in batches\n",
    "        logger.info(f\"Generating eval functions for {len(selected_instructions)} instructions with k={k}\")\n",
    "        all_responses = self.batch_call_llm(all_prompts, max_new_tokens=4096, batch_size=batch_size, show_progress=show_progress)\n",
    "        \n",
    "        # Process responses\n",
    "        idx = 0\n",
    "        for instruction in selected_instructions:\n",
    "            eval_function_entry = {\n",
    "                \"prompt\": prompt_template.format(instruction=instruction),\n",
    "                \"instruction\": instruction,\n",
    "                \"gpt-answer\": []\n",
    "            }\n",
    "            \n",
    "            for _ in range(k):\n",
    "                if idx < len(all_responses):\n",
    "                    eval_function_entry[\"gpt-answer\"].append(all_responses[idx])\n",
    "                    idx += 1\n",
    "            \n",
    "            generated_eval_functions.append(eval_function_entry)\n",
    "        \n",
    "        return generated_eval_functions\n",
    "    \n",
    "    def filter_generated_eval_function(self, generated_eval_functions, show_progress=True):\n",
    "        \"\"\"Filter and validate generated evaluation functions.\"\"\"\n",
    "        filtered_generated_eval_functions = []\n",
    "        \n",
    "        # First, collect all imports and other suspicious patterns\n",
    "        collect_packages = []\n",
    "        count = 0\n",
    "        \n",
    "        for result in generated_eval_functions:\n",
    "            for each in result['gpt-answer']:\n",
    "                try:\n",
    "                    json_pattern = r'```json(.*?)```'\n",
    "                    match = re.search(json_pattern, each, re.DOTALL)\n",
    "                    if match:\n",
    "                        json_dict = match.group(1).strip().replace(\"\\n\", \"\")\n",
    "                    else:\n",
    "                        # Try to find JSON without code block markers\n",
    "                        json_pattern = r'\\{.*?\"func\".*?\\}'\n",
    "                        match = re.search(json_pattern, each, re.DOTALL)\n",
    "                        if match:\n",
    "                            json_dict = match.group(0)\n",
    "                        else:\n",
    "                            count += 1\n",
    "                            continue\n",
    "                    \n",
    "                    res_dict = json.loads(json_dict)\n",
    "                    func = res_dict['func']\n",
    "                    if '\\\\n' in func:\n",
    "                        func = func.replace('\\\\n', '\\n')\n",
    "                    \n",
    "                    try:\n",
    "                        exec(func)\n",
    "                    except Exception as e:\n",
    "                        count += 1\n",
    "                        logger.debug(f\"Error executing eval function: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    for line in func.split('\\n'):\n",
    "                        if 'import' in line or 'download' in line or 'requests' in line:\n",
    "                            collect_packages.append(line)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    count += 1\n",
    "                    logger.debug(f\"Error processing eval function: {e}\")\n",
    "        \n",
    "        if collect_packages:\n",
    "            logger.warning(f\"Found potentially suspicious imports: {list(set(collect_packages))}\")\n",
    "        \n",
    "        # Process each result\n",
    "        iterator = generated_eval_functions\n",
    "        if show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Filtering eval functions\")\n",
    "        \n",
    "        for result in iterator:\n",
    "            eval_funcs, test_cases = [], []\n",
    "            \n",
    "            # Process each answer\n",
    "            for each in result['gpt-answer']:\n",
    "                try:\n",
    "                    # Extract JSON\n",
    "                    json_pattern = r'```json(.*?)```'\n",
    "                    match = re.search(json_pattern, each, re.DOTALL)\n",
    "                    if match:\n",
    "                        json_dict = match.group(1).strip().replace(\"\\n\", \"\")\n",
    "                    else:\n",
    "                        # Try to find JSON without code block markers\n",
    "                        json_pattern = r'\\{.*?\"func\".*?\\}'\n",
    "                        match = re.search(json_pattern, each, re.DOTALL)\n",
    "                        if match:\n",
    "                            json_dict = match.group(0)\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    # Parse JSON\n",
    "                    res_dict = json.loads(json_dict)\n",
    "                    \n",
    "                    # Clean and validate function\n",
    "                    func = res_dict['func'].strip()\n",
    "                    func = '\\n'.join([line for line in func.split('\\n') \n",
    "                                     if 'download' not in line and 'requests' not in line])\n",
    "                    \n",
    "                    try:\n",
    "                        exec(func)\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error executing eval function: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    eval_funcs.append(func)\n",
    "                    \n",
    "                    # Extract test cases\n",
    "                    for each_case in res_dict.get('cases', []):\n",
    "                        try:\n",
    "                            input_val = each_case.get('input', '')\n",
    "                            output_val = each_case.get('output', False)\n",
    "                            \n",
    "                            # Convert string 'true'/'false' to bool if needed\n",
    "                            if isinstance(output_val, str):\n",
    "                                output_val = output_val.lower() == 'true'\n",
    "                            \n",
    "                            test_cases.append((input_val, output_val))\n",
    "                        except Exception as e:\n",
    "                            logger.debug(f\"Error processing test case: {e}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error processing eval function result: {e}\")\n",
    "            \n",
    "            # Remove duplicates\n",
    "            eval_funcs = list(set(eval_funcs))\n",
    "            test_cases = list(map(json.loads, set(map(json.dumps, test_cases))))\n",
    "            \n",
    "            # Skip if not enough functions or test cases\n",
    "            # if len(eval_funcs) < 1 or len(test_cases) < 3:\n",
    "            #     continue\n",
    "            \n",
    "            # Filter test cases\n",
    "            filtered_test_cases = []\n",
    "            for test_case in test_cases:\n",
    "                flag = False\n",
    "                for func in eval_funcs:\n",
    "                    local_vars = {}\n",
    "                    try:\n",
    "                        exec(func, globals(), local_vars)\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error executing eval function: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    if 'evaluate' not in local_vars:\n",
    "                        continue\n",
    "                    \n",
    "                    eval_func = local_vars['evaluate']\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res_val = eval_func(test_case[0])\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error executing eval function: {e}\")\n",
    "                        res_val = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    \n",
    "                    if res_val is not None and res_val == test_case[1]:\n",
    "                        flag = True\n",
    "                        break\n",
    "                \n",
    "                if flag:\n",
    "                    filtered_test_cases.append(test_case)\n",
    "            \n",
    "            # Score functions\n",
    "            scored_funcs = []\n",
    "            for func in eval_funcs:\n",
    "                local_vars = {}\n",
    "                try:\n",
    "                    exec(func, globals(), local_vars)\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error executing eval function: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                if 'evaluate' not in local_vars:\n",
    "                    continue\n",
    "                \n",
    "                eval_func = local_vars['evaluate']\n",
    "                acc = []\n",
    "                \n",
    "                for inp, out in filtered_test_cases:\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res_val = eval_func(inp)\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error executing eval function: {e}\")\n",
    "                        res_val = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    \n",
    "                    if res_val is None or res_val != out:\n",
    "                        acc.append(0)\n",
    "                    else:\n",
    "                        acc.append(1)\n",
    "                \n",
    "                acc_mean = np.mean(acc) if acc else 0\n",
    "                scored_funcs.append((func, acc_mean))\n",
    "            \n",
    "            # Filter valid functions\n",
    "            valid_funcs = [each for each in scored_funcs if each[1] >= 0.5]\n",
    "            if not valid_funcs:\n",
    "                continue\n",
    "            \n",
    "            # Add to filtered results\n",
    "            filtered_generated_eval_functions.append({\n",
    "                \"instruction\": result['instruction'],\n",
    "                \"eval_func\": valid_funcs,\n",
    "                \"cases\": filtered_test_cases\n",
    "            })\n",
    "        \n",
    "        logger.info(f\"Filtered to {len(filtered_generated_eval_functions)} valid evaluation functions\")\n",
    "        return filtered_generated_eval_functions\n",
    "    \n",
    "    def generate_instruction(self, filtered_generated_eval_functions, k=2, batch_size=8, show_progress=True):\n",
    "        \"\"\"Generate instructions from evaluation functions.\"\"\"\n",
    "        generated_instructions = []\n",
    "        count = 0\n",
    "        filter_count = 0\n",
    "        \n",
    "        # Build all prompts first\n",
    "        all_prompts = []\n",
    "        for line in filtered_generated_eval_functions:\n",
    "            funcs = line[\"eval_func\"][:3]\n",
    "            instruction_prompt = f\"\"\"You are an expert in converting the Python eval function code into the corresponding instruction text. I will provide the eval function code. Please strictly follow the code to convert it into the corresponding instruction text. Here's an example: \n",
    "\n",
    "[[\"def evaluate(response):\\n    return 'e' not in response.lower()\", 1.0], [\"def evaluate(response):\\n    words = response.split()\\n    for word in response.split():\\n        if 'e' in word.lower():\\n            return False\\n    return True\", 1.0], [\"def evaluate(response):\\n    return all('e' not in word.lower() for word in response.split())\", 1.0]] \n",
    "\n",
    "[\"Answer without using any words that contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\"] Please convert the following eval function into instructions stored in a list: \n",
    "\n",
    "{funcs}\"\"\"\n",
    "            \n",
    "            for _ in range(k):\n",
    "                all_prompts.append(instruction_prompt)\n",
    "        \n",
    "        # Process prompts in batches\n",
    "        logger.info(f\"Generating instructions from {len(filtered_generated_eval_functions)} eval functions with k={k}\")\n",
    "        all_responses = self.batch_call_llm(all_prompts, max_new_tokens=1024, batch_size=batch_size, show_progress=show_progress)\n",
    "        \n",
    "        # Process responses\n",
    "        idx = 0\n",
    "        for line in filtered_generated_eval_functions:\n",
    "            back_instruction = None\n",
    "            \n",
    "            for _ in range(k):\n",
    "                if idx < len(all_responses):\n",
    "                    response = all_responses[idx]\n",
    "                    idx += 1\n",
    "                    \n",
    "                    try:\n",
    "                        # Try to parse the response as JSON\n",
    "                        # First look for JSON in code blocks\n",
    "                        json_pattern = r'```(json)?(.*?)```'\n",
    "                        match = re.search(json_pattern, response, re.DOTALL)\n",
    "                        if match:\n",
    "                            json_text = match.group(2).strip()\n",
    "                        else:\n",
    "                            # Try to find a list directly\n",
    "                            list_pattern = r'\\[(.*?)\\]'\n",
    "                            match = re.search(list_pattern, response, re.DOTALL)\n",
    "                            if match:\n",
    "                                json_text = f\"[{match.group(1)}]\"\n",
    "                            else:\n",
    "                                continue\n",
    "                        \n",
    "                        back_instruction = json.loads(json_text)\n",
    "                        break\n",
    "                    except Exception:\n",
    "                        filter_count += 1\n",
    "                        continue\n",
    "            \n",
    "            if back_instruction:\n",
    "                line_copy = line.copy()\n",
    "                line_copy[\"back_instruction\"] = back_instruction\n",
    "                generated_instructions.append(line_copy)\n",
    "                count += 1\n",
    "            else:\n",
    "                logger.debug(f\"Failed to generate back instruction for: {line['instruction']}\")\n",
    "        \n",
    "        logger.info(f\"Generated {len(generated_instructions)} instructions, {filter_count} filtered out\")\n",
    "        return generated_instructions\n",
    "    \n",
    "    def filter_generated_instruction(self, generated_instructions, batch_size=8, show_progress=True):\n",
    "        \"\"\"Filter generated instructions using NLI model.\"\"\"\n",
    "        filtered_generated_instructions = []\n",
    "        count = 0\n",
    "        filter_count = 0\n",
    "        \n",
    "        iterator = generated_instructions\n",
    "        if show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Filtering instructions\")\n",
    "        \n",
    "        for line in iterator:\n",
    "            try:\n",
    "                back_instructions = line[\"back_instruction\"]\n",
    "                ori_ins = line[\"instruction\"]\n",
    "                \n",
    "                # Process each back instruction with NLI model\n",
    "                nli_scores = []\n",
    "                for back_ins in back_instructions[:3]:\n",
    "                    premise = ori_ins\n",
    "                    hypothesis = back_ins\n",
    "                    \n",
    "                    inputs = self.tokenizer_nli(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        output = self.model_nli(**inputs)\n",
    "                    prediction = torch.softmax(output.logits[0], -1).tolist()\n",
    "                    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "                    max_label = label_names[prediction.index(max(prediction))]\n",
    "                    nli_scores.append(max_label)\n",
    "                \n",
    "                line[\"nli_scores\"] = nli_scores\n",
    "                \n",
    "                # Filter out contradictions\n",
    "                if \"contradiction\" in nli_scores:\n",
    "                    filter_count += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    filtered_generated_instructions.append(line)\n",
    "                    count += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error filtering instruction: {str(e)}\")\n",
    "                filter_count += 1\n",
    "        \n",
    "        logger.info(f\"Filtered to {len(filtered_generated_instructions)} instructions, {filter_count} filtered out\")\n",
    "        return filtered_generated_instructions\n",
    "    \n",
    "    def generate_response(self, query, r1_generated_text, filtered_generated_instructions, k=2, batch_size=8, show_progress=True):\n",
    "        \"\"\"Generate responses following instructions.\"\"\"\n",
    "        generated_responses = []\n",
    "        \n",
    "        # Build all prompts\n",
    "        all_prompts = []\n",
    "        for instruction in filtered_generated_instructions:\n",
    "            ins_text = instruction['instruction']\n",
    "            for _ in range(k):\n",
    "                prompt = (\n",
    "                    f\"{r1_generated_text}\\n\"\n",
    "                    f\"Re-write the above text following: {ins_text}\\n\\n\"\n",
    "                    f\"Note: Use the same words and sentences but re-arrange them in a way that strictly follows the instruction.\\n\"\n",
    "                )\n",
    "                all_prompts.append(prompt)\n",
    "        \n",
    "        # Process prompts in batches\n",
    "        if show_progress:\n",
    "            logger.info(f\"Generating responses for {len(filtered_generated_instructions)} instructions with k={k}\")\n",
    "        all_generated_texts = self.batch_call_llm(all_prompts, max_new_tokens=1024, batch_size=batch_size, show_progress=show_progress)\n",
    "        \n",
    "        # Process responses\n",
    "        idx = 0\n",
    "        for instruction in filtered_generated_instructions:\n",
    "            prompt = (\n",
    "                f\"Please answer the query strictly following the instruction.\\n\"\n",
    "                f\"[instruction] {instruction['instruction']}\\n\"\n",
    "                f\"[Query] {query}\"\n",
    "            )\n",
    "            \n",
    "            responses = []\n",
    "            for _ in range(k):\n",
    "                if idx < len(all_generated_texts):\n",
    "                    responses.append(all_generated_texts[idx])\n",
    "                    idx += 1\n",
    "            \n",
    "            generated_responses.append({\n",
    "                \"instruction\": instruction['instruction'],\n",
    "                \"prompt\": prompt,\n",
    "                \"gpt-answer\": responses,\n",
    "                \"eval_func\": instruction[\"eval_func\"],\n",
    "            })\n",
    "        \n",
    "        if show_progress:\n",
    "            logger.info(f\"Generated {len(generated_responses)} response sets\")\n",
    "        return generated_responses\n",
    "    \n",
    "    def filter_generated_response(self, generated_responses, show_progress=True):\n",
    "        \"\"\"Filter generated responses using evaluation functions.\"\"\"\n",
    "        filtered_samples = []\n",
    "        \n",
    "        iterator = generated_responses\n",
    "        if show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Filtering responses\")\n",
    "        \n",
    "        for result in iterator:\n",
    "            # Extract evaluation functions\n",
    "            eval_funcs = []\n",
    "            for func, score in result['eval_func']:\n",
    "                local_vars = {}\n",
    "                try:\n",
    "                    exec(func, globals(), local_vars)\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Error executing eval function: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                if 'evaluate' in local_vars:\n",
    "                    eval_funcs.append(local_vars['evaluate'])\n",
    "            \n",
    "            # Skip if no valid evaluation functions\n",
    "            if not eval_funcs:\n",
    "                continue\n",
    "            \n",
    "            # Filter responses\n",
    "            filter_responses = []\n",
    "            for response in result['gpt-answer']:\n",
    "                acc = []\n",
    "                for eval_func in eval_funcs:\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res = eval_func(response)\n",
    "                    except Exception as e:\n",
    "                        logger.debug(f\"Error executing eval function: {e}\")\n",
    "                        res = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    \n",
    "                    if res is not None:\n",
    "                        try:\n",
    "                            acc.append(int(res))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                \n",
    "                acc_mean = np.mean(acc) if acc else 0\n",
    "                if acc_mean > 0:\n",
    "                    filter_responses.append(response)\n",
    "                    preference_item = {\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"user\", \"content\": result[\"prompt\"]},\n",
    "                            {\"role\": \"assistant\", \"content\": item.get('response', '')}\n",
    "                        ],\n",
    "                        \"is_positive\": 1\n",
    "                    }\n",
    "                else:\n",
    "                    preference_item = {\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"user\", \"content\": result[\"prompt\"]},\n",
    "                            {\"role\": \"assistant\", \"content\": item.get('response', '')}\n",
    "                        ],\n",
    "                        \"is_positive\": 0\n",
    "                    }\n",
    "                    \n",
    "                self.preference_data.append(preference_item)\n",
    "            \n",
    "            # Extract query and add filtered responses\n",
    "            for each in filter_responses:\n",
    "                try:\n",
    "                    query_match = re.findall(r'\\[Query\\](.*)$', result['prompt'], re.DOTALL)\n",
    "                    query = query_match[0].strip() if query_match else \"\"\n",
    "                    \n",
    "                    filtered_samples.append({\n",
    "                        'instruction': result['instruction'],\n",
    "                        'query': query,\n",
    "                        'response': each,\n",
    "                        'prompt': result['prompt'],\n",
    "                        'eval_func': result['eval_func'],\n",
    "                    })\n",
    "                except IndexError:\n",
    "                    logger.debug(f\"Prompt extraction error: {result['prompt']}\")\n",
    "        \n",
    "        # Remove duplicates\n",
    "        unique_samples = []\n",
    "        seen = set()\n",
    "        for item in filtered_samples:\n",
    "            item_json = json.dumps(item, sort_keys=True)\n",
    "            if item_json not in seen:\n",
    "                seen.add(item_json)\n",
    "                unique_samples.append(item)\n",
    "        \n",
    "        if show_progress:\n",
    "            logger.info(f\"Filtered to {len(unique_samples)} unique responses\")\n",
    "        return unique_samples\n",
    "    \n",
    "def filter2_generated_response(self, filtered_generated_responses, k=2, batch_size=8, show_progress=True):\n",
    "    \"\"\"Further filter responses based on quality scoring.\"\"\"\n",
    "    filtered2_generated_responses = []\n",
    "    \n",
    "    if not filtered_generated_responses:\n",
    "        logger.warning(\"No responses to filter in filter2_generated_response\")\n",
    "        return filtered2_generated_responses\n",
    "    \n",
    "    prompt_template = (\n",
    "        \"You are an expert that is good at judging whether a response is following the instruction and query.\\n\"\n",
    "        \"[Instruction] {instruction}\\n\"\n",
    "        \"[Query] {query}\\n\"\n",
    "        \"[Response] {response}\\n\"\n",
    "        \"Please notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\\n\"\n",
    "        \"You need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\\n\"\n",
    "        \"Scoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\\n\"\n",
    "        \"Please only provide a score in the format `Score: {{score}}` without any other contents at the last line.\"\n",
    "    )\n",
    "    \n",
    "    all_prompts = []\n",
    "    for idx, each in enumerate(filtered_generated_responses):\n",
    "        for k_idx in range(k):\n",
    "            prompt = prompt_template.format(\n",
    "                instruction=each.get('instruction', ''),\n",
    "                query=each.get('query', ''),\n",
    "                response=each.get('response', '')\n",
    "            )\n",
    "            all_prompts.append((idx, prompt))\n",
    "    \n",
    "    if show_progress:\n",
    "        logger.info(f\"Quality scoring {len(filtered_generated_responses)} responses with k={k}\")\n",
    "    \n",
    "    batch_results = {}\n",
    "    for i in range(0, len(all_prompts), batch_size):\n",
    "        batch_items = all_prompts[i:i+batch_size]\n",
    "        batch_prompts = [item[1] for item in batch_items]\n",
    "        \n",
    "        try:\n",
    "            batch_responses = self.batch_call_llm(\n",
    "                batch_prompts, max_new_tokens=1024, batch_size=batch_size, show_progress=False\n",
    "            )\n",
    "            \n",
    "            for j, response in enumerate(batch_responses):\n",
    "                if i + j < len(all_prompts):\n",
    "                    idx = all_prompts[i + j][0]\n",
    "                    if idx not in batch_results:\n",
    "                        batch_results[idx] = []\n",
    "                    batch_results[idx].append(response)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in batch processing: {str(e)}\")\n",
    "            \n",
    "    for idx, responses in batch_results.items():\n",
    "        if idx < len(filtered_generated_responses):\n",
    "            item = filtered_generated_responses[idx]\n",
    "            item_copy = item.copy()\n",
    "            item_copy['gen'] = responses\n",
    "            \n",
    "            scores = []\n",
    "            for gen in responses:\n",
    "                score_match = re.search(r'Score:\\s*(\\d+)', gen)\n",
    "                if score_match:\n",
    "                    try:\n",
    "                        score = int(score_match.group(1))\n",
    "                        scores.append(score)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            score = np.mean(scores) if scores else 0\n",
    "            if score > 5: \n",
    "                filtered2_generated_responses.append(item_copy)\n",
    "                preference_item = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": result[\"prompt\"]},\n",
    "                        {\"role\": \"assistant\", \"content\": item.get('response', '')}\n",
    "                    ],\n",
    "                    \"is_positive\": 1\n",
    "                }\n",
    "            else:\n",
    "                preference_item = {\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"user\", \"content\": result[\"prompt\"]},\n",
    "                        {\"role\": \"assistant\", \"content\": item.get('response', '')}\n",
    "                    ],\n",
    "                    \"is_positive\": 0\n",
    "                }\n",
    "            self.preference_data.append(preference_item)\n",
    "            \n",
    "    if show_progress:\n",
    "        logger.info(f\"Final quality filter: {len(filtered2_generated_responses)} responses\")\n",
    "    \n",
    "    return filtered2_generated_responses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
