{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def generate_text(prompt, max_length=5000, num_return_sequences=1):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate text using the model\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        no_repeat_ngram_size=2,  # Prevents repeating phrases\n",
    "        do_sample=True,          # Enables sampling; set to False for greedy decoding\n",
    "        top_k=50,                # Limits to the top 50 tokens for sampling\n",
    "        top_p=0.95,              # Nucleus sampling: considers tokens with cumulative probability of 0.95\n",
    "        temperature=0.7,         # Controls randomness in predictions\n",
    "        early_stopping=True      # Stops when an end-of-sentence token is generated\n",
    "    )\n",
    "    \n",
    "    # Decode the output tokens to strings\n",
    "    generated_texts = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/share/u/harshraj/CotIF/src/synthesize/data/forward/cotroller_train_dataset-mix-with_translation.json\", \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'applied_functions': 'indent_paragraphs_forward',\n",
      " 'messages': [{'content': 'Return your final response within \\\\boxed{}. When '\n",
      "                          '$\\\\frac{3}{1250}$ is written as a decimal, how many '\n",
      "                          'zeros are there between the decimal point and the '\n",
      "                          'first non-zero digit?\\n'\n",
      "                          '\\n'\n",
      "                          '## NOTES\\n'\n",
      "                          '\\n'\n",
      "                          'Instructions that you must follow in your reasoning '\n",
      "                          'chain (the part of your response before your final '\n",
      "                          'answer):\\n'\n",
      "                          '- indent each paragraph with a tab character',\n",
      "               'role': 'user'},\n",
      "              {'content': '<think>\\n'\n",
      "                          'Okay, so before I start reasoning, let me recall '\n",
      "                          \"the user's instructions for my reasoning chain: I \"\n",
      "                          'must indent each paragraph with a tab character.\\n'\n",
      "                          \"It's really important that I adhere to these \"\n",
      "                          'instructions in my reasoning chain, since otherwise '\n",
      "                          \"the user's need would not be met. So from the next \"\n",
      "                          \"paragraph onward, I'll indent each paragraph with a \"\n",
      "                          'tab character.\\n'\n",
      "                          '\\n',\n",
      "               'role': 'assistant'},\n",
      "              {'content': '\\tOkay, so I need to figure out how many zeros are '\n",
      "                          'between the decimal point and the first non-zero '\n",
      "                          'digit when 3/1250 is written as a decimal. Hmm, let '\n",
      "                          'me start by understanding the problem. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tFirst, I know that converting a fraction to a '\n",
      "                          'decimal involves dividing the numerator by the '\n",
      "                          'denominator. In this case, the fraction is 3 over '\n",
      "                          '1250. So maybe I can do the division 3 ÷ 1250. But '\n",
      "                          \"before jumping into long division, maybe there's a \"\n",
      "                          'smarter way to simplify this or convert the '\n",
      "                          'denominator to a power of 10 to make it easier?\\n'\n",
      "                          '\\n'\n",
      "                          '\\tWait, the denominator here is 1250. Let me break '\n",
      "                          'that down. 1250 is 125 times 10, right? And 125 is '\n",
      "                          '5 cubed (5^3). So 1250 is 5^3 * 2 * 5, which is 5^4 '\n",
      "                          '* 2. Hmm, or maybe I should factor it as 1250 = 5^4 '\n",
      "                          '* 2. Let me confirm: 5^3 is 125, times 10 is 1250. '\n",
      "                          'So, 5^3 * 10 = 1250. Alternatively, 1250 = 125 * 10 '\n",
      "                          \"= 5^3 * 2 * 5 = 5^4 * 2. Yeah, that's correct.\\n\"\n",
      "                          '\\n'\n",
      "                          '\\tSo, since the denominator is 5^4 * 2, to convert '\n",
      "                          'the fraction to a decimal, we might need to make '\n",
      "                          'the denominator a power of 10. Powers of 10 are '\n",
      "                          'multiples of 2 and 5. Since 1250 is 5^4 * 2, if we '\n",
      "                          'can get the denominator to be 10^4, we can have a '\n",
      "                          'decimal. Wait, but 10^4 is 10000. Let me see. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tIf I multiply both numerator and denominator by '\n",
      "                          '8, then the denominator becomes 1250 * 8 = 10000, '\n",
      "                          \"which is 10^4. Let's check: 1250 * 8 = 10000. Yes, \"\n",
      "                          'because 125 * 8 = 1000, so 1250 * 8 = 10000. So, if '\n",
      "                          'I multiply numerator and denominator by 8, the '\n",
      "                          'fraction becomes (3*8)/(1250*8) = 24/10000. Then, '\n",
      "                          '24 divided by 10000 is 0.0024. So, that would be '\n",
      "                          'the decimal representation. \\n'\n",
      "                          '\\n'\n",
      "                          \"\\tWait, let's confirm that. 24 divided by 10000: \"\n",
      "                          '24/10000 is 0.0024. Because 24 divided by 10 is '\n",
      "                          '2.4, divided by 100 is 0.24, divided by 1000 is '\n",
      "                          \"0.024, divided by 10000 is 0.0024. So yes, that's \"\n",
      "                          'correct. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tTherefore, 3/1250 as a decimal is 0.0024. Now, '\n",
      "                          'the question is asking how many zeros are between '\n",
      "                          'the decimal point and the first non-zero digit. '\n",
      "                          \"Let's look at 0.0024. The decimal point is followed \"\n",
      "                          'by two zeros, then the first non-zero digit is 2. '\n",
      "                          'So between the decimal point and the first non-zero '\n",
      "                          'digit (which is 2), there are two zeros. So the '\n",
      "                          'answer should be 2. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tBut wait, let me double-check. Maybe I made a '\n",
      "                          \"mistake in the calculation. Let's try another \"\n",
      "                          \"approach. Let's divide 3 by 1250 manually. \\n\"\n",
      "                          '\\n'\n",
      "                          \"\\tSo, 3 ÷ 1250. Let's see. Since 1250 goes into \"\n",
      "                          '3.0000... How many times does 1250 go into 3? Zero '\n",
      "                          'times. Then we add a decimal point and some zeros. '\n",
      "                          '1250 goes into 30 zero times, into 300 zero times, '\n",
      "                          'into 3000 two times because 1250 * 2 = 2500. '\n",
      "                          'Subtract 2500 from 3000, we get 500. Bring down the '\n",
      "                          'next zero to make 5000. 1250 goes into 5000 four '\n",
      "                          'times (1250 * 4 = 5000). Subtract that, we get 0. '\n",
      "                          'So the division gives 0.0024. Yep, same result. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tTherefore, the decimal is indeed 0.0024. So '\n",
      "                          'between the decimal point and the first non-zero '\n",
      "                          'digit (which is the 2 in the thousandths place), '\n",
      "                          'there are two zeros. So the number of zeros is 2. \\n'\n",
      "                          '\\n'\n",
      "                          \"\\tWait, but let's visualize the decimal places. The \"\n",
      "                          'first place after the decimal is tenths, then '\n",
      "                          'hundredths, thousandths, ten-thousandths. So 0.0024 '\n",
      "                          'is 0 in tenths, 0 in hundredths, 2 in thousandths, '\n",
      "                          'and 4 in ten-thousandths. So the first non-zero '\n",
      "                          'digit is in the thousandths place, which is two '\n",
      "                          'places after the decimal. Wait, but the tenths is '\n",
      "                          'first, hundredths second, thousandths third. Wait, '\n",
      "                          'no, tenths (1st), hundredths (2nd), thousandths '\n",
      "                          '(3rd), ten-thousandths (4th). So 0.0024 has:\\n'\n",
      "                          '\\n'\n",
      "                          '\\t- 0 in tenths (1st),\\n'\n",
      "                          '- 0 in hundredths (2nd),\\n'\n",
      "                          '- 2 in thousandths (3rd),\\n'\n",
      "                          '- 4 in ten-thousandths (4th).\\n'\n",
      "                          '\\n'\n",
      "                          '\\tSo the first non-zero digit is in the third '\n",
      "                          'decimal place. Therefore, there are two zeros after '\n",
      "                          'the decimal point before the first non-zero digit. '\n",
      "                          'Therefore, the answer is 2. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tBut hold on, maybe I need to check once more. Let '\n",
      "                          \"me write out 0.0024. After the decimal, it's 0, 0, \"\n",
      "                          '2, 4. So between the decimal point and the first 2, '\n",
      "                          'there are two zeros. So two zeros. So yes, the '\n",
      "                          'answer is 2. \\n'\n",
      "                          '\\n'\n",
      "                          \"\\tAlternatively, perhaps there's another way to \"\n",
      "                          'compute this without converting the fraction. For '\n",
      "                          'example, using exponents. Since 1250 is 10^4 / 8, '\n",
      "                          'as we saw earlier, but maybe that complicates '\n",
      "                          'things. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tAlternatively, using logarithms? Hmm, maybe not '\n",
      "                          'necessary here. Since we already have two methods '\n",
      "                          'leading to the same answer, and the decimal '\n",
      "                          'representation is confirmed, I think we can be '\n",
      "                          'confident that the number of zeros is 2. \\n'\n",
      "                          '\\n'\n",
      "                          \"\\tBut just to be thorough, let's think about \"\n",
      "                          \"another method. Let's express 3/1250 as a decimal \"\n",
      "                          \"by simplifying the fraction first. 3/1250. Let's \"\n",
      "                          'divide numerator and denominator by 2 first. 3/1250 '\n",
      "                          '= (3/2)/625. But 3 divided by 2 is 1.5, so 1.5/625. '\n",
      "                          \"Then, 1.5 divided by 625. Let's do that division. \\n\"\n",
      "                          '\\n'\n",
      "                          '\\t1.5 ÷ 625. Well, 625 goes into 1.5 zero times. So '\n",
      "                          'we write 0. and then proceed. 625 goes into 15 '\n",
      "                          '(after adding a zero) zero times. 625 goes into 150 '\n",
      "                          '(another zero) still zero times. 625 goes into 1500 '\n",
      "                          'two times (since 625*2=1250). Subtract 1250 from '\n",
      "                          '1500, get 250. Bring down the next zero, making '\n",
      "                          '2500. 625 goes into 2500 four times (625*4=2500). '\n",
      "                          'So we have 0.0024. Same result. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tTherefore, regardless of the method, the decimal '\n",
      "                          'is 0.0024, so two zeros between the decimal point '\n",
      "                          'and the first non-zero digit. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tAnother approach: Express the denominator as 10^n '\n",
      "                          '* something. Since 1250 = 10^3 * 1.25. Therefore, '\n",
      "                          '3/1250 = 3/(10^3 * 1.25) = (3/1.25)/10^3. 3 divided '\n",
      "                          \"by 1.25. Let's compute 3 ÷ 1.25. 1.25 goes into 3 \"\n",
      "                          'two times (1.25*2=2.50). Subtract 2.50 from 3, we '\n",
      "                          'get 0.50. Bring down a zero, making it 5.0. 1.25 '\n",
      "                          'goes into 5.0 four times (1.25*4=5.00). So 3 ÷ 1.25 '\n",
      "                          '= 2.4. Then divide by 10^3 (which is 1000), so 2.4 '\n",
      "                          '÷ 1000 = 0.0024. Again, same result. \\n'\n",
      "                          '\\n'\n",
      "                          '\\tYep, so this method also gives 0.0024, confirming '\n",
      "                          'the number of zeros is two. \\n'\n",
      "                          '\\n'\n",
      "                          \"\\tI think that's thorough enough. All methods lead \"\n",
      "                          'to the decimal 0.0024, so two zeros between the '\n",
      "                          'decimal and the first non-zero digit. Therefore, '\n",
      "                          'the answer should be 2.\\n'\n",
      "                          '</think>\\n'\n",
      "                          '\\n'\n",
      "                          'To determine how many zeros are between the decimal '\n",
      "                          'point and the first non-zero digit when '\n",
      "                          '\\\\(\\\\frac{3}{1250}\\\\) is written as a decimal, we '\n",
      "                          'can convert the fraction to its decimal form.\\n'\n",
      "                          '\\n'\n",
      "                          '1. **Factor the denominator**:  \\n'\n",
      "                          '   \\\\(1250 = 5^4 \\\\times 2\\\\).\\n'\n",
      "                          '\\n'\n",
      "                          '2. **Convert the denominator to a power of 10**:  \\n'\n",
      "                          '   Multiply both the numerator and denominator by '\n",
      "                          '\\\\(8\\\\) to get:  \\n'\n",
      "                          '   \\\\[\\n'\n",
      "                          '   \\\\frac{3 \\\\times 8}{1250 \\\\times 8} = '\n",
      "                          '\\\\frac{24}{10000}\\n'\n",
      "                          '   \\\\]\\n'\n",
      "                          '\\n'\n",
      "                          '3. **Simplify the fraction**:  \\n'\n",
      "                          '   \\\\(\\\\frac{24}{10000} = 0.0024\\\\).\\n'\n",
      "                          '\\n'\n",
      "                          '4. **Analyze the decimal representation**:  \\n'\n",
      "                          '   The decimal \\\\(0.0024\\\\) has two zeros between '\n",
      "                          'the decimal point and the first non-zero digit '\n",
      "                          '(which is \\\\(2\\\\)).\\n'\n",
      "                          '\\n'\n",
      "                          'Thus, the number of zeros between the decimal point '\n",
      "                          'and the first non-zero digit is \\\\(\\\\boxed{2}\\\\).',\n",
      "               'role': 'assistant'}],\n",
      " 'num_chars': 7421}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Harsh1729/qwenr1_1.5b_lora_sft-cotif-v1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Harsh1729/qwenr1_1.5b_lora_sft-cotif-v1\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0].replace(prompt, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"When generating your thought process between `<think>` and `</think>`, please ensure adherence to the following guidelines:\\n\\n1. Your explanation should consist of at least one but no more than three paragraphs.\\n2. As we have identified excessive use of parentheses as a concern (`'contains_excessive_parentheses': True`), try to minimize their usage without compromising clarity.\\n3. You are free to include letters containing the letter \\\"e\\\" if necessary (`'avoids_letter_e': False`).\\n4. Refrain from incorporating emojis into your response (`'contains_emojis': False`).\\n5. Since we need to track palindrome occurrences, keep note of palindromes found during your reasoning; however, this specific task does not require reaching the target count of 182 palindromes (`'palindrome_count': 182`). \\n6. Organize your thoughts using lists or enumeration when helpful for understanding (`'contains_list_or_enumeration': True`)\\n\\nNow answer this question: A student takes a multiple choice test with $5$ questions. Each question has $4$ possible choices, and only $1$ of them is correct. Let $X$ be the random variable representing the number of correct answers the student gets. Then $X$ follows a binomial distribution with parameters $n=5$ and $p=\\\\frac{1}{4}$. Compute $P(X=4)$.\"\n",
    "texts = generate_text(prompt).replace(prompt, \"\")\n",
    "for i, text in enumerate(texts, 1):\n",
    "    print(f\"\\nGenerated text {i}:\\n{text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "data = []\n",
    "with open(\"/share/u/harshraj/CotIF/data/data_1k-2.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/share/u/harshraj/CotIF/data/data_1k-using_templates-train-1.jsonl\", \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install snac ipywebrtc\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snac import SNAC\n",
    "import torch\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "from ipywebrtc import AudioRecorder, Audio\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from huggingface_hub import snapshot_download\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import torch\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "model_name = \"canopylabs/orpheus-tts-0.1-pretrained\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").eval().cuda()\n",
    "\n",
    "\n",
    "# Download only model config and safetensors\n",
    "model_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    allow_patterns=[\n",
    "        \"config.json\",\n",
    "        \"*.safetensors\",\n",
    "        \"model.safetensors.index.json\",\n",
    "    ],\n",
    "    ignore_patterns=[\n",
    "        \"optimizer.pt\",\n",
    "        \"pytorch_model.bin\",\n",
    "        \"training_args.bin\",\n",
    "        \"scheduler.pt\",\n",
    "        \"tokenizer.json\",\n",
    "        \"tokenizer_config.json\",\n",
    "        \"special_tokens_map.json\",\n",
    "        \"vocab.json\",\n",
    "        \"merges.txt\",\n",
    "        \"tokenizer.*\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE THIS TO YOUR OWN FILE AND TEXT\n",
    "\n",
    "my_wav_file_is = \"lgt/voice_acting_outputs-sage-emotion-prompts_sample_1288.wav\"\n",
    "and_the_transcript_is = \"Something or the other\"\n",
    "\n",
    "the_model_should_say = [\n",
    "  \"I finally got into the university of my dreams! I can't believe all this hard work actually  paid off!\",\n",
    "  \"Why is your frickin' Waymo blocking the frickin' road? GET OUT OF THE WAY!\",\n",
    "  \"I'm so sorry to hear about your pet, but you know, he'll pull through.\",\n",
    "  \"Conversational, uhm, systems, tend to speak pretty robotically, because- because they don't, really understand how, uhm, humans talk.\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Here we tokenise the prompt you gave us, we also tokenise the prompts you want the model to say\n",
    "\n",
    "The template is:\n",
    "\n",
    "start_of_human, start_of_text, text, end_of_text, start_of_ai, start_of_speech, speech, end_of_speech, end_of_ai, start_of_human, text, end_of_human and then generate from here\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "filename = my_wav_file_is\n",
    "\n",
    "audio_array, sample_rate = librosa.load(filename, sr=24000)\n",
    "\n",
    "def tokenise_audio(waveform):\n",
    "  waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "  waveform = waveform.to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "  waveform = waveform.unsqueeze(0)\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    codes = snac_model.encode(waveform)\n",
    "\n",
    "  all_codes = []\n",
    "  for i in range(codes[0].shape[1]):\n",
    "    all_codes.append(codes[0][0][i].item()+128266)\n",
    "    all_codes.append(codes[1][0][2*i].item()+128266+4096)\n",
    "    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))\n",
    "    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))\n",
    "\n",
    "\n",
    "  return all_codes\n",
    "\n",
    "myts = tokenise_audio(audio_array)\n",
    "start_tokens = torch.tensor([[ 128259]], dtype=torch.int64)\n",
    "end_tokens = torch.tensor([[128009, 128260, 128261, 128257]], dtype=torch.int64)\n",
    "final_tokens = torch.tensor([[128258, 128262]], dtype=torch.int64)\n",
    "voice_prompt = and_the_transcript_is\n",
    "prompt_tokked = tokenizer(voice_prompt, return_tensors=\"pt\")\n",
    "\n",
    "input_ids = prompt_tokked[\"input_ids\"]\n",
    "\n",
    "zeroprompt_input_ids = torch.cat([start_tokens, input_ids, end_tokens, torch.tensor([myts]), final_tokens], dim=1) # SOH SOT Text EOT EOH\n",
    "\n",
    "prompts = the_model_should_say\n",
    "\n",
    "all_modified_input_ids = []\n",
    "for prompt in prompts:\n",
    "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "  second_input_ids = torch.cat([zeroprompt_input_ids, start_tokens, input_ids, end_tokens], dim=1)\n",
    "  all_modified_input_ids.append(second_input_ids)\n",
    "\n",
    "\n",
    "all_padded_tensors = []\n",
    "all_attention_masks = []\n",
    "\n",
    "max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
    "\n",
    "for modified_input_ids in all_modified_input_ids:\n",
    "  padding = max_length - modified_input_ids.shape[1]\n",
    "  padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
    "  attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
    "  all_padded_tensors.append(padded_tensor)\n",
    "  all_attention_masks.append(attention_mask)\n",
    "\n",
    "all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
    "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "\n",
    "input_ids = all_padded_tensors.to(\"cuda\")\n",
    "attention_mask = all_attention_masks.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes[1].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  generated_ids = model.generate(\n",
    "      input_ids=input_ids,\n",
    "      # attention_mask=attention_mask,\n",
    "      max_new_tokens=990,\n",
    "      do_sample=True,\n",
    "      temperature=0.5,\n",
    "      # top_k=40,\n",
    "      top_p=0.9,\n",
    "      repetition_penalty=1.1,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=128258,\n",
    "      # end_token_id=128009\n",
    "  )\n",
    "\n",
    "# generated_ids = torch.cat([generated_ids, torch.tensor([[128262]]).to(\"cuda\")], dim=1) # EOAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_find = 128257\n",
    "token_to_remove = 128258\n",
    "\n",
    "# Check if the token exists in the tensor\n",
    "token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
    "\n",
    "if len(token_indices[1]) > 0:\n",
    "    last_occurrence_idx = token_indices[1][-1].item()\n",
    "    cropped_tensor = generated_ids[:, last_occurrence_idx+1:]\n",
    "else:\n",
    "    cropped_tensor = generated_ids\n",
    "\n",
    "mask = cropped_tensor != token_to_remove\n",
    "processed_rows = []\n",
    "for row in cropped_tensor:\n",
    "    # Apply the mask to each row\n",
    "    masked_row = row[row != token_to_remove]\n",
    "    processed_rows.append(masked_row)\n",
    "\n",
    "code_lists = []\n",
    "for row in processed_rows:\n",
    "    # row is a 1D tensor with its own length\n",
    "    row_length = row.size(0)\n",
    "    new_length = (row_length // 7) * 7  # largest multiple of 7 that fits in this row\n",
    "    trimmed_row = row[:new_length]\n",
    "    trimmed_row = [t - 128266 for t in trimmed_row]\n",
    "    code_lists.append(trimmed_row)\n",
    "\n",
    "def redistribute_codes(code_list):\n",
    "  layer_1 = []\n",
    "  layer_2 = []\n",
    "  layer_3 = []\n",
    "  for i in range((len(code_list)+1)//7):\n",
    "    layer_1.append(code_list[7*i])\n",
    "    layer_2.append(code_list[7*i+1]-4096)\n",
    "    layer_3.append(code_list[7*i+2]-(2*4096))\n",
    "    layer_3.append(code_list[7*i+3]-(3*4096))\n",
    "    layer_2.append(code_list[7*i+4]-(4*4096))\n",
    "    layer_3.append(code_list[7*i+5]-(5*4096))\n",
    "    layer_3.append(code_list[7*i+6]-(6*4096))\n",
    "  codes = [torch.tensor(layer_1).unsqueeze(0),\n",
    "         torch.tensor(layer_2).unsqueeze(0),\n",
    "         torch.tensor(layer_3).unsqueeze(0)]\n",
    "  audio_hat = snac_model.decode(codes)\n",
    "  return audio_hat\n",
    "\n",
    "my_samples = []\n",
    "for code_list in code_lists:\n",
    "  samples = redistribute_codes(code_list)\n",
    "  my_samples.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "for samples in my_samples:\n",
    "  display(Audio(samples.detach().squeeze().to(\"cpu\").numpy(), rate=24000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_array, sample_rate = librosa.load(\"/share/u/harshraj/CotIF/notebooks/lgt/voice_acting_outputs-sage-emotion-prompts_sample_1288.wav\", sr=24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_audio(waveform):\n",
    "  waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "  waveform = waveform.to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "  waveform = waveform.unsqueeze(0)\n",
    "\n",
    "  with torch.inference_mode():\n",
    "    codes = snac_model.encode(waveform)\n",
    "\n",
    "  all_codes = []\n",
    "  for i in range(codes[0].shape[1]):\n",
    "    all_codes.append(codes[0][0][i].item()+128266)\n",
    "    all_codes.append(codes[1][0][2*i].item()+128266+4096)\n",
    "    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))\n",
    "    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))\n",
    "\n",
    "\n",
    "  return all_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = audio_array\n",
    "waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
    "waveform = waveform.to(dtype=torch.float32)\n",
    "\n",
    "\n",
    "waveform = waveform.unsqueeze(0)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    codes = snac_model.encode(waveform)\n",
    "\n",
    "all_codes = []\n",
    "for i in range(codes[0].shape[1]):\n",
    "    all_codes.append(codes[0][0][i].item()+128266)\n",
    "    all_codes.append(codes[1][0][2*i].item()+128266+4096)\n",
    "    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))\n",
    "    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))\n",
    "    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes[2].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(4*i)+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = snac_model.encode(waveform.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myts = tokenise_audio(audio_array)\n",
    "myts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy \n",
    "from string import punctuation\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "numbering_list = ['3', '7)', '7.', '4', 'iii.', 'iii-', '8.', '4-', 'v:', 'I:', 'ii.', 'i.', 'V)', 'E)', 'I)', 'III.', 'III)', '2-', '1)', 'v-', 'III', 'I.', 'c)', '1.', 'V-', 'iv)', 'A)', 'v)', 'IV', 'C.', 'ii)', 'I', 'IV.', 'C)', 'II-', '2.', 'III-', 'IV)', 'd)', 'iii', 'i-', 'iii:', 'A.', 'B.', '1', '6)', 'ii', '8)', '3)', 'e)', 'ii-', '5-', 'II)', 'iv-', '2)', 'e.', 'IV:', 'III:', 'i)', '10.', 'V', 'V.', 'v.', 'D)', 'E.', 'iv:', 'B)', 'II', 'ii:', 'V:', 'a.', '5.', 'IV-', '9.', 'D.', '3.', '4:', '2:', 'i', 'II.', '3-', '2', 'c.', 'a)', '3:', '10)', 'd.', 'i:', 'iv.', '1-', '4.', '5', 'iv', 'iii)', 'b.', '1:', 'II:', 'v', '5:', '6.', 'b)', 'I-', '9)', '4)', '5)']\n",
    "\n",
    "stopwords_list = ['es', 'ing', 'ed', 'include', 'includes', 'also', 'haven', 'are', 'why', 'most', \"won't\", 'against', 'with', 'needn', 'couldn', 'now', 'mustn', 'who', 'under', 'doing', 'am', 'aren', 'they', \"didn't\", 'd', 'doesn', 'if', 'he', 'her', \"haven't\", 'isn', 'own', 'does', 'such', 'until', 'into', 'had', 'again', 'over', \"hadn't\", \"you'll\", 't', 'by', 'be', \"wasn't\", 'so', 'yours', 'both', 'any', 'did', \"you've\", 'these', 'myself', 'o', 'hasn', \"isn't\", 'you', 'other', 'shan', 'being', 'yourselves', 'was', 'no', 'm', 'those', 'will', 'its', 'itself', 'have', 'down', 'weren', 'having', 'wouldn', 'herself', \"mustn't\", 'very', 'do', \"should've\", 'him', \"you'd\", 'below', 'just', 'that', 'for', 'which', 'but', 'nor', 'all', 'then', 'i', 'whom', 'it', 'once', 'here', 've', \"you're\", 'ours', \"that'll\", 'a', 'won', 'himself', 'where', 'this', 'your', \"hasn't\", 'same', 'when', 'ourselves', 'because', \"needn't\", 'theirs', 'from', 'mightn', 'my', 'while', 'yourself', \"she's\", 'each', \"doesn't\", 'only', 'at', 's', 'their', \"wouldn't\", 'shouldn', 'and', 'themselves', 'hers', 'has', 'up', 'ma', 'in', 'll', 'we', 're', 'y', 'of', 'after', 'our', \"shan't\", 'before', 'wasn', 'can', 'should', 'been', 'through', 'as', 'further', 'during', 'between', 'there', 'me', 'on', 'don', \"shouldn't\", 'more', 'out', \"don't\", 'the', \"weren't\", \"aren't\", \"it's\", 'what', 'or', \"couldn't\", 'hadn', \"mightn't\", 'his', 'above', 'to', 'how', 'few', 'off', 'them', 'didn', 'ain', 'not', 'she', 'an', 'than', 'too', 'is', 'some', 'were', 'about']\n",
    "\n",
    "common_title_words_set = {'introduction', 'conclusion', 'section', 'chapter', 'works', 'notes', 'note', 'further', 'see', 'references', 'reference', 'section', 'title', 'conclusion', 'intro', 'introduction', 'executive', 'summary', 'key', 'plot', 'theme'}\n",
    "stopwords_set = set(stopwords_list + numbering_list)\n",
    "\n",
    "def strip_left_stopwords(e_text):\n",
    "  \"\"\"\n",
    "  Removes common stopwords from the left side of a text until a significant word is found.\n",
    "\n",
    "  Args:\n",
    "      e_text (str): The text to strip from the left side.\n",
    "\n",
    "  Returns:\n",
    "      str: Text with left-side stopwords removed.\n",
    "  \"\"\"\n",
    "  e_text2 = []\n",
    "  add_rest = False\n",
    "  for et in e_text.split():\n",
    "      if add_rest or ((et.lower() not in stopwords_set and et.lower() not in common_title_words_set) or et.lower().strip(\".\") in {\"a\", \"an\", \"united\", \"the\", \"new\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\",  \"asian\", \"american\", \"african\", \"european\", }):\n",
    "        add_rest = True\n",
    "        e_text2.append(et)\n",
    "  return \" \".join(e_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_nouns_common_hypernyms(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    # candidate_nouns = [strip_left_stopwords(e.text)  for e in doc.noun_chunks if len(e.text) > 4 and e.text.lower() not in stopwords_set]        \n",
    "    candidate_nouns = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.pos_ == \"NOUN\" and token.lemma_.lower() not in stopwords_set and len(token.text) > 2\n",
    "    ]\n",
    "    noun_synsets = {}\n",
    "    for noun in candidate_nouns:\n",
    "        syns = wn.synsets(noun, \"n\")\n",
    "        if syns:\n",
    "            noun_synsets[noun] = syns[0]\n",
    "\n",
    "    hypernym_map = {}\n",
    "    noun_list = sorted(noun_synsets.keys())\n",
    "    for i in range(len(noun_list)):\n",
    "        for j in range(i+1, len(noun_list)):\n",
    "            syn_i = noun_synsets[noun_list[i]]\n",
    "            syn_j = noun_synsets[noun_list[j]]\n",
    "            lch = syn_i.lowest_common_hypernyms(syn_j)\n",
    "            if lch:\n",
    "                for common_hyp in lch:\n",
    "                    name = common_hyp.lemmas()[0]\n",
    "                    if name.name() in {\"act\", \"whole\", \"communication\", \"thing\", \"content\", \"group\", \"social group\", \"unit\", \"organism\", \"region\", \"area\", \"geographic point\", \"relation\", \"attribute\", \"happening\", \"action\", \"entity\", \"causal agent\", 'administrative district', \"abstraction\", \"object\", \"point\", \"event\", \"physical entity\", \"matter\", \"part\", \"person\", \"state\"}:\n",
    "                        continue\n",
    "                    hypernym_map.setdefault(name, set()).update([noun_list[i], noun_list[j]])\n",
    "    \n",
    "    hypernym_map = {\n",
    "        hyp: list(nset) for hyp, nset in hypernym_map.items() if len(nset) >= 2\n",
    "    }\n",
    "\n",
    "    return hypernym_map\n",
    "\n",
    "ss = \"\"\"The scientist observed the dolphin and the octopus in the aquarium.  \n",
    "Nearby, a photographer captured images of a lion and a zebra in the savanna exhibit.  \n",
    "Visitors also saw a helicopter hovering over the mountains, and a truck delivering supplies to a remote village.  \n",
    "Inside the exhibition hall, children interacted with a robot and explored a model of the solar system.\"\"\"\n",
    "obj = analyze_nouns_common_hypernyms(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = \"\"\"The scientist observed the dolphin and the octopus in the aquarium.  \n",
    "Nearby, a photographer captured images of a lion and a zebra in the savanna exhibit.  \n",
    "Visitors also saw a helicopter hovering over the mountains, and a truck delivering supplies to a remote village.  \n",
    "Inside the exhibition hall, children interacted with a robot and explored a model of the solar system.\"\"\"\n",
    "\n",
    "def generate_ngrams(tokens, n=4):\n",
    "    return [tokens[i:i+n] for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def hidden_acrostic(text, length=5):\n",
    "    first_letters = \"\".join([s.strip()[0].lower() for s in text.split('.') if s.strip()])\n",
    "    for word in generate_ngrams(first_letters, 6):\n",
    "        syns = wn.synsets(word)\n",
    "        if syns:\n",
    "            return word\n",
    "        \n",
    "    for word in generate_ngrams(first_letters, 5):\n",
    "        syns = wn.synsets(word)\n",
    "        if syns:\n",
    "            return word\n",
    "        \n",
    "    for word in generate_ngrams(first_letters, 4):\n",
    "        syns = wn.synsets(word)\n",
    "        if syns:\n",
    "            return word        \n",
    "\n",
    "hidden_acrostic(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "k, v = random.sample(list(obj.items()), 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = spacy_nlp(ss)\n",
    "candidate_nouns = [\n",
    "    token.lemma_ for token in doc \n",
    "    if token.pos_ == \"NOUN\" and token.lemma_.lower() not in stopwords_set and len(token.text) > 2\n",
    "]\n",
    "candidate_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hypernym_map = {}\n",
    "noun_list = sorted(noun_synsets.keys())\n",
    "for i in range(len(noun_list)):\n",
    "    for j in range(i+1, len(noun_list)):\n",
    "        syn_i = noun_synsets[noun_list[i]]\n",
    "        syn_j = noun_synsets[noun_list[j]]\n",
    "        lch = syn_i.lowest_common_hypernyms(syn_j)\n",
    "        if lch:\n",
    "            for common_hyp in lch:\n",
    "                name = common_hyp.lemmas()[0]\n",
    "                if name in {\"act\", \"whole\", \"communication\", \"thing\", \"content\", \"group\", \"social group\", \"unit\", \"organism\", \"region\", \"area\", \"geographic point\", \"relation\", \"attribute\", \"happening\", \"action\", \"entity\", \"causal agent\", 'administrative district', \"abstraction\", \"object\", \"point\", \"event\", \"physical entity\", \"matter\", \"part\", \"person\", \"state\"}:\n",
    "                    continue\n",
    "                hypernym_map.setdefault(name, set()).update([noun_list[i], noun_list[j]])\n",
    "\n",
    "hypernym_map = {\n",
    "    hyp: list(nset) for hyp, nset in hypernym_map.items() if len(nset) >= 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install textblob\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "import re\n",
    "\n",
    "def replace_words_with_emojis(text):\n",
    "    emoji_map = {\n",
    "        'error': '❌',\n",
    "        'warning': '⚠️',\n",
    "        'analysis': '🔍',\n",
    "        'result': '✅',\n",
    "        'success': '🎉',\n",
    "        'failure': '💔',\n",
    "        'logic': '🤔',\n",
    "        'calculate': '🧮',\n",
    "        'problem': '❓',\n",
    "        'solution': '💡',\n",
    "        'reasoning': '🧠',\n",
    "        'trace': '📝',\n",
    "        'thought': '💭',\n",
    "        'data': '📊',\n",
    "        'compute': '💻',\n",
    "        'model': '🤖',\n",
    "        'optimization': '⚙️',\n",
    "        'test': '🧪',\n",
    "        'experiment': '🔬',\n",
    "        'input': '📥',\n",
    "        'output': '📤',\n",
    "        'unknown': '❓',\n",
    "        'uncertain': '🤷',\n",
    "        'prompt': '💬',\n",
    "        'instruct': '📝'\n",
    "    }\n",
    "    \n",
    "    pattern = r'\\b(' + '|'.join(re.escape(word) for word in emoji_map.keys()) + r')\\b'\n",
    "    \n",
    "    replaced = {}\n",
    "    \n",
    "    def replacer(match):\n",
    "        original = match.group(0)\n",
    "        lower_word = original.lower()\n",
    "        emoji = emoji_map[lower_word]\n",
    "        replaced.setdefault(original, emoji)\n",
    "        return emoji\n",
    "    \n",
    "    if len(replaced.keys()) > 1:\n",
    "        new_text = re.sub(pattern, replacer, text, flags=re.IGNORECASE)\n",
    "        description = (\"in your reasoning chain replace the words {} with their corresponding emojis {} respectively\"\n",
    "                    .format(list(replaced.keys()), list(replaced.values())))\n",
    "    else:\n",
    "        new_text, description, None, None\n",
    "    return {'description': description, 'new_text': new_text}\n",
    "\n",
    "# Example test text (simulating an AI reasoning trace)\n",
    "sample_text = (\n",
    "    \"I performed an analysis of the data and computed the result. \"\n",
    "    \"However, a warning was issued due to an error in the model. \"\n",
    "    \"The reasoning was uncertain, but the solution eventually emerged from the test experiments.\"\n",
    ")\n",
    "\n",
    "print(replace_words_with_emojis(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Okay, let's tackle this logic problem.\\nThe question is asking which of the following is equivalent to \\\"If P is true, then Q is false.\\\" Hmm, first, I need to recall how to translate conditional statements into logical expressions.\\nSo, the original statement is \\\"If P, then not Q.\\\" In symbolic terms, that would be P \\u2192 \\u00acQ, right?\\nNow, the next step is to figure out the logical equivalents of this statement.\\nI remember that implications can be converted using contrapositives or other equivalences.\\nLet me think.\\nThe contrapositive of P \\u2192 \\u00acQ would be Q \\u2192 \\u00acP because the contrapositive switches and negates the antecedent and consequent.\\nSo, if the original is \\\"If P, then not Q,\\\" then the contrapositive is \\\"If Q, then not P.\\\" That seems right.\\nAre those two equivalent?\\nYes, in classical logic, an implication and its contrapositive are logically equivalent.\\nSo that's one possible equivalent statement.\\nBut the question is probably presenting multiple options, so maybe there are other forms.\\nLet me recall other logical equivalences.\\nFor example, an implication P \\u2192 Q is equivalent to \\u00acP \\u2228 Q.\\nSo applying that to P \\u2192 \\u00acQ, it would be equivalent to \\u00acP \\u2228 \\u00acQ.\\nWhich can also be written as \\u00ac(P \\u2227 Q) because by De Morgan's laws, \\u00acP \\u2228 \\u00acQ is the same as \\u00ac(P \\u2227 Q).\\nSo that's another way to express it: \\\"It is not the case that both P and Q are true.\\\" Hmm, so \\\"If P is true, then Q is false\\\" can also be phrased as \\\"P and Q cannot both be true at the same time.\\\" That makes sense.\\nWait\"\n",
    "replace_words_with_emojis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"/share/u/harshraj/CotIF/data/cotroller_train_dataset-mix.json\", \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Return the cosine similarity between two vectors.\"\"\"\n",
    "    # Added a small constant to avoid division by zero.\n",
    "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2) + 1e-8)\n",
    "\n",
    "def check_for_metaphor(text, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Check for sentences that may contain metaphors based on low similarity between\n",
    "    the subject and the complement of a linking verb.\n",
    "    \n",
    "    Parameters:\n",
    "      text (str): The text to analyze.\n",
    "      threshold (float): Similarity threshold below which a sentence is flagged as metaphorical.\n",
    "      \n",
    "    Returns:\n",
    "      list: A list of sentences likely containing metaphors.\n",
    "    \"\"\"\n",
    "    # Load spaCy's medium English model (ensure you have 'en_core_web_md' installed)\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    doc = nlp(text)\n",
    "    metaphor_sentences = []\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        # Iterate over tokens in the sentence (no need to re-process the sentence)\n",
    "        for token in sent:\n",
    "            # Check if the token is a form of 'be' and is tagged as a VERB or AUX\n",
    "            if token.lemma_ == \"be\" and token.pos_ in {\"VERB\", \"AUX\"}:\n",
    "                # Find nominal subject and attribute/complement\n",
    "                subj = [child for child in token.children if child.dep_ in (\"nsubj\", \"nsubjpass\")]\n",
    "                comp = [child for child in token.children if child.dep_ in (\"attr\", \"acomp\")]\n",
    "                if subj and comp:\n",
    "                    sim = cosine_similarity(subj[0].vector, comp[0].vector)\n",
    "                    # If the similarity is below the threshold, flag the sentence as metaphorical\n",
    "                    if sim < threshold:\n",
    "                        metaphor_sentences.append(sent.text)\n",
    "                        break  # Proceed to the next sentence once a metaphor is found\n",
    "    return metaphor_sentences\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # sample_text = \"Time is a thief. The sky is blue. He is as brave as a lion.\"\n",
    "    metaphors = check_for_metaphor(sample_text)\n",
    "    print(\"Sentences with potential metaphors:\")\n",
    "    for m in metaphors:\n",
    "        print(\" -\", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Okay, so I need to solve this age problem. Let me read it again carefully. The question says:\\n\\n\\\"Two years ago Pete was three times as old as his cousin Claire. Two years before that, Pete was four times as old as Claire. In how many years will the ratio of their ages be 2 : 1? The options are A through E: 2, 4, 5, 6, 8.\\\"\\n\\nHmm, age problems usually involve setting up equations based on the given information. Let me try to break this down step by step.\\n\\nFirst, let's define variables for their current ages. Let me denote Pete's current age as P and Claire's current age as C. The goal is to find how many years into the future their ages will have a ratio of 2:1. Let's call that number of years x. So, in x years, Pete will be P + x years old, and Claire will be C + x years old, and we need (P + x)\\/(C + x) = 2\\/1, which simplifies to P + x = 2(C + x). That equation might come in handy later. Let's keep that in mind.\\n\\nNow, the problem gives information about their ages in the past. Two\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = data[0]['messages'][1]['content'] + data[0]['messages'][2]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_delete = \"{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}\"\n",
    "new_template = tokenizer.get_chat_template().replace(to_delete, \"\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, chat_template=new_template)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tokenizer.get_chat_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_point = [data[0]['messages'][0]] + [data[0]['messages'][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(new_data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_point[1]['content'] = '<think>' + new_data_point[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tokenizer.apply_chat_template(new_data_point, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tokenizer.apply_chat_template(data[0]['messages'], tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(tokenizer.get_chat_template())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "def get_tokenizer_with_new_chat_template(tokenizer):\n",
    "    to_delete = \"{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}\"\n",
    "    new_template = tokenizer.get_chat_template().replace(to_delete, \"\")\n",
    "    return AutoTokenizer.from_pretrained(MODEL, chat_template=new_template)\n",
    "\n",
    "tokenizer = get_tokenizer_with_new_chat_template(AutoTokenizer.from_pretrained(MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = [{\"role\": \"user\", \"content\": \"What is 1+1 = 2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"<think> the user wants me to tell 1+1 which is 2</think>\\nfinal_answer: 2\"}]\n",
    "\n",
    "conv2 = [{\"role\": \"user\", \"content\": \"What is 1+1 = 2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"<think> the user wants me to tell 1+1 which is 2\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"</think>\\nfinal_answer: 2\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "def get_tokenizer_with_new_chat_template(tokenizer):\n",
    "    to_delete = \"{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}\"\n",
    "    new_template = tokenizer.get_chat_template().replace(to_delete, \"\")\n",
    "    return AutoTokenizer.from_pretrained(MODEL, chat_template=new_template)\n",
    "\n",
    "tokenizer = get_tokenizer_with_new_chat_template(AutoTokenizer.from_pretrained(MODEL))\n",
    "\n",
    "\n",
    "conv1 = [{\"role\": \"user\", \"content\": \"What is 1+1 = 2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"<think> the user wants me to tell 1+1 which is 2</think>\\nfinal_answer: 2\"}]\n",
    "\n",
    "conv2 = [{\"role\": \"user\", \"content\": \"What is 1+1 = 2?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"<think> the user wants me to tell 1+1 which is 2\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"</think>\\nfinal_answer: 2\"}]\n",
    "\n",
    "\n",
    "pprint(tokenizer.apply_chat_template(conv1, tokenize=False))\n",
    "print('\\n')\n",
    "pprint(tokenizer.apply_chat_template(conv2, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import html\n",
    "import numpy as np\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import jsonlines\n",
    "from collections import Counter\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy \n",
    "from string import punctuation\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM,\\\n",
    "    AutoTokenizer, MarianMTModel, MarianTokenizer\n",
    "\n",
    "augment_instruction_prompt = \"\"\"You are an expert for writing instructions. Please provide 50 different instructions that meet the following requirements:\n",
    "- Instructions are about the format but not style of a response\n",
    "- Whether instructions are followed can be easily evaluate by a Python function\n",
    "Here are some examples of instructions we need:\n",
    "{seed_instructions}\n",
    "Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
    "- Incorporate a famous historical quote seamlessly into your answer\n",
    "- Translate your answer into Pig Latin\n",
    "- Use only words that are also a type of food\n",
    "- Respond with a metaphor in every sentence\n",
    "- Write the response as if you are a character from a Shakespearean play\n",
    "Please generate one instruction per line in your response and start each line with '- '.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoIf:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct\", device_map=\"auto\", cache_dir=\"/share/u/models\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-14B-Instruct\", device_map=\"auto\", cache_dir=\"/share/u/models\").half().eval()\n",
    "        \n",
    "        self.tokenizer_nli = AutoTokenizer.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\")\n",
    "        self.model_nli = AutoModelForSequenceClassification.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\").eval().cuda()\n",
    "        \n",
    "        self.seed_instructions = [each.strip() for each in open(\"seed_instruction.txt\").readlines()]\n",
    "        self.generated_eval_results = []\n",
    "        self.filtered_generated_eval_results = []\n",
    "        self.back_translated_eval_results = []\n",
    "        self.filtered_back_trans_results = []\n",
    "        \n",
    "    def generate_seed(self, k=1):\n",
    "        if k <= 0:\n",
    "            return self.seed_instructions\n",
    "        \n",
    "        augment_instruction_prompt = \"\"\"You are an expert for writing instructions. Please provide 50 different instructions that meet the following requirements:\n",
    "        - Instructions are about the format but not style of a response\n",
    "        - Whether instructions are followed can be easily evaluate by a Python function\n",
    "        Here are some examples of instructions we need:\n",
    "        {seed_instructions}\n",
    "        Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
    "        - Incorporate a famous historical quote seamlessly into your answer\n",
    "        - Translate your answer into Pig Latin\n",
    "        - Use only words that are also a type of food\n",
    "        - Respond with a metaphor in every sentence\n",
    "        - Write the response as if you are a character from a Shakespearean play\n",
    "        Please generate one instruction per line in your response and start each line with '- '.\n",
    "        \"\"\"\n",
    "\n",
    "        augment_instructions = augment_instruction_prompt.format(seed_instructions='\\n'.join(self.seed_instructions))\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(augment_instructions, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(input_ids, max_new_tokens=2048, do_sample=True, temperature=0.7)\n",
    "        generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "        \n",
    "        new_seeds = [line.strip() for line in generated_text.split('\\n') if line.strip()]\n",
    "        self.seed_instructions = self.seed_instructions + new_seeds\n",
    "        \n",
    "        random.shuffle(self.seed_instructions)\n",
    "        return self.generate_seed(k - 1)\n",
    "    \n",
    "    def generate_eval_functions(self, k=1):\n",
    "        prompt_template = (\n",
    "            \"You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\n\"\n",
    "            \"Here is the instruction: {instruction}\\n\"\n",
    "            \"Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. \"\n",
    "            \"If it follows, simply return True, otherwise return False.\\n\"\n",
    "            \"Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, \"\n",
    "            \"which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\n\"\n",
    "            \"Here is an example of output JSON format: {{\\\"func\\\": JSON_STR(use only \\\\n instead of \\n), \\\"cases\\\": [{{\\\"input\\\": str, \\\"output\\\": str}}]}}.\"\n",
    "        )\n",
    "\n",
    "        for instruction in self.seed_instructions:\n",
    "            prompt = prompt_template.format(instruction=instruction)\n",
    "            for _ in range(k):\n",
    "                input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                outputs = self.model.generate(input_ids, max_new_tokens=2048, do_sample=True, temperature=0.7)\n",
    "                generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "                self.generated_eval_results.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"instruction\": instruction,\n",
    "                    \"gpt-answer\": generated_text\n",
    "                })\n",
    "                \n",
    "    def filter_generated_eval_results(self):\n",
    "        collect_packages = []\n",
    "        for result in self.generated_eval_results:\n",
    "            res = result['gpt-answer']\n",
    "            eval_funcs, test_cases = [], []\n",
    "            for each in res:\n",
    "                try:\n",
    "                    json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                try:\n",
    "                    res_dict = json.loads(json_dict)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                func = res_dict['func']\n",
    "                if '\\\\n' in func:\n",
    "                    func = func.replace('\\\\n', '\\n')\n",
    "                try:\n",
    "                    exec(func)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                for line in func.split('\\n'):\n",
    "                    if 'import' in line or 'download' in line or 'requests' in line:\n",
    "                        collect_packages.append(line)\n",
    "        print(list(set(collect_packages)))\n",
    "\n",
    "        def timeout_handler(signum, frame):\n",
    "            raise TimeoutError(\"Function execution timed out\")\n",
    "\n",
    "        for result in tqdm(self.generated_eval_results):\n",
    "            res = result['gpt-answer']\n",
    "            eval_funcs, test_cases = [], []\n",
    "            for each in tqdm(res):\n",
    "                try:\n",
    "                    json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                try:\n",
    "                    res_dict = json.loads(json_dict)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                # func rejection and cleaning\n",
    "                func = res_dict['func'].strip()\n",
    "                func = '\\n'.join([line for line in func.split('\\n') if 'download' not in line and 'requests' not in line])\n",
    "                try:\n",
    "                    exec(func)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                eval_funcs.append(func)\n",
    "\n",
    "                for each_case in res_dict['cases']:\n",
    "                    try:\n",
    "                        test_cases.append((each_case['input'], each_case['output']))\n",
    "                    except KeyError:\n",
    "                        print(each_case)\n",
    "\n",
    "            eval_funcs = list(set(eval_funcs))\n",
    "            test_cases = list(map(json.loads, set(map(json.dumps, test_cases))))\n",
    "            if len(eval_funcs) < 3 or len(test_cases) < 10:\n",
    "                continue\n",
    "\n",
    "            filtered_test_cases = []\n",
    "            for each in tqdm(test_cases):\n",
    "                flag = False\n",
    "                for func in eval_funcs:\n",
    "                    local_vars = {}\n",
    "                    try:\n",
    "                        exec(func, globals(), local_vars)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if 'evaluate' not in local_vars:\n",
    "                        continue\n",
    "                    eval_func = local_vars['evaluate']\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res_val = eval_func(each[0])\n",
    "                    except Exception:\n",
    "                        res_val = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    if res_val is not None and res_val == each[1]:\n",
    "                        flag = True\n",
    "                if flag:\n",
    "                    filtered_test_cases.append(each)\n",
    "\n",
    "            scored_funcs = []\n",
    "            for func in tqdm(eval_funcs):\n",
    "                local_vars = {}\n",
    "                try:\n",
    "                    exec(func, globals(), local_vars)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if 'evaluate' not in local_vars:\n",
    "                    continue\n",
    "                eval_func = local_vars['evaluate']\n",
    "                acc = []\n",
    "                for inp, out in filtered_test_cases:\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res_val = eval_func(inp)\n",
    "                    except Exception:\n",
    "                        res_val = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    if res_val is None or res_val != out:\n",
    "                        acc.append(0)\n",
    "                    else:\n",
    "                        acc.append(1)\n",
    "                acc = np.mean(acc) if acc else 0\n",
    "                scored_funcs.append((func, acc))\n",
    "            valid_funcs = [each for each in scored_funcs if each[1] >= 0.8]\n",
    "            if not valid_funcs:\n",
    "                continue\n",
    "\n",
    "            self.filtered_generated_eval_results.append({\n",
    "                \"instruction\": result['instruction'],\n",
    "                \"eval_func\": valid_funcs,\n",
    "                \"cases\": filtered_test_cases\n",
    "            })\n",
    "            \n",
    "\n",
    "    def generate_back_trans_instructions(self):\n",
    "        self.back_translated_eval_results = []\n",
    "        count = 0\n",
    "        filter_count = 0\n",
    "\n",
    "        for i, line in enumerate(self.filtered_generated_eval_results):\n",
    "            funcs = line[\"eval_func\"][:3]\n",
    "\n",
    "            instruction_prompt = f\"\"\"You are an expert in converting the Python eval function code into the corresponding instruction text. I will provide the eval function code. Please strictly follow the code to convert it into the corresponding instruction text. Here's an example: \n",
    "\n",
    "[[\"def evaluate(response):\\n    return 'e' not in response.lower()\", 1.0], [\"def evaluate(response):\\n    words = response.split()\\n    for word in response.split():\\n        if 'e' in word.lower():\\n            return False\\n    return True\", 1.0], [\"def evaluate(response):\\n    return all('e' not in word.lower() for word in response.split())\", 1.0]] \n",
    "\n",
    "[\"Answer without using any words that contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\"] Please convert the following eval function into instructions stored in a list: \n",
    "\n",
    "{funcs}\"\"\"\n",
    "\n",
    "            input_ids = self.tokenizer.encode(instruction_prompt, return_tensors=\"pt\").cuda()\n",
    "            outputs = self.model.generate(input_ids, max_new_tokens=1024, do_sample=True, temperature=0.7)\n",
    "            generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "            try:\n",
    "                back_instruction = json.loads(generated_text)\n",
    "            except Exception:\n",
    "                filter_count += 1\n",
    "                continue\n",
    "\n",
    "            line[\"back_instruction\"] = back_instruction\n",
    "            self.back_translated_eval_results.append(line)\n",
    "            count += 1\n",
    "            \n",
    "\n",
    "    def filter_back_trans_instructions(self):\n",
    "        count = 0 \n",
    "        filter_count = 0\n",
    "\n",
    "        for i, line in enumerate(self.back_translated_eval_results):\n",
    "            back_instructions = line[\"back_instruction\"]\n",
    "            ori_ins = line[\"instruction\"]\n",
    "\n",
    "            nli_scores = []\n",
    "            for back_ins in back_instructions[:3]:\n",
    "                premise = ori_ins\n",
    "                hypothesis = back_ins\n",
    "\n",
    "                inputs = self.tokenizer_nli(premise, hypothesis, truncation=True, return_tensors=\"pt\").cuda()\n",
    "                output = self.model_nli(inputs[\"input_ids\"].cuda())\n",
    "                prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "                label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "                prediction_dict = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "                max_label = max(prediction_dict, key=prediction_dict.get)\n",
    "                nli_scores.append(max_label)\n",
    "\n",
    "            line[\"nli_scores\"] = nli_scores\n",
    "            if \"contradiction\" in nli_scores:\n",
    "                filter_count += 1\n",
    "                continue\n",
    "            else:\n",
    "                self.filtered_back_trans_results.append(line)\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08329b60ac0e45d1828d9323898b9d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "autoif = AutoIf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"\", \"r\") as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "    \n",
    "autoif.filter_generated_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = autoif.model.half(), autoif.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/harshraj/.conda/envs/r1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/share/u/harshraj/.conda/envs/r1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/share/u/harshraj/.conda/envs/r1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# autoif.generate_seed()\n",
    "seed_instructions = [each.strip() for each in open(\"seed_instruction.txt\").readlines()]\n",
    "augment_instruction_prompt = \"\"\"You are an expert for writing instructions. Please provide 50 different instructions that meet the following requirements:\n",
    "- Instructions are about the format but not style of a response\n",
    "- Whether instructions are followed can be easily evaluate by a Python function\n",
    "Here are some examples of instructions we need:\n",
    "{seed_instructions}\n",
    "Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
    "- Incorporate a famous historical quote seamlessly into your answer\n",
    "- Translate your answer into Pig Latin\n",
    "- Use only words that are also a type of food\n",
    "- Respond with a metaphor in every sentence\n",
    "- Write the response as if you are a character from a Shakespearean play\n",
    "Please generate one instruction per line in your response and start each line with '- '.\n",
    "\"\"\"\n",
    "\n",
    "augment_instructions = augment_instruction_prompt.format(seed_instructions='\\n'.join(seed_instructions))\n",
    "\n",
    "input_ids = tokenizer.encode(augment_instructions, return_tensors=\"pt\").cuda()\n",
    "outputs = model.generate(input_ids, max_new_tokens=1024, do_sample=False, temperature=0.7)\n",
    "generated_text = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "new_seeds = [line.strip() for line in generated_text.split('\\n') if line.strip()]\n",
    "seed_instructions = seed_instructions + new_seeds\n",
    "\n",
    "random.shuffle(seed_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- Respond with a list of bullet points',\n",
       " '- Use only questions to form your reply',\n",
       " '- Answer with a single sentence that is exactly 100 words long',\n",
       " '- Use no adjectives or adverbs',\n",
       " \"- Use no words containing the letter 'E'\",\n",
       " '- Use only words that rhyme with \"land\"',\n",
       " '- Use only uppercase letters',\n",
       " '- Use only words that rhyme with \"tree\"',\n",
       " '- Use only words that have an X in them',\n",
       " '- Answer with',\n",
       " '- Respond with a haiku (5-7-5 syllable structure)',\n",
       " \"- Use words that end with '-ing'\",\n",
       " '- Use only words that rhyme with \"cat\"',\n",
       " \"- Use words that end with '-ing'\",\n",
       " 'Write the response as a limerick',\n",
       " 'Use only words that are colors',\n",
       " '- Respond with a list of bullet points',\n",
       " '- Use only words that rhyme with \"dog\"',\n",
       " '- Use only words with double letters (e.g., \"bookkeeper\")',\n",
       " '- Write the response as a limerick',\n",
       " '- Write the response in iambic pentameter',\n",
       " \"- Answer with words that begin with the letter 'B'\",\n",
       " 'Use only words that start and end with the same letter',\n",
       " 'Answer with a sequence of puns',\n",
       " '- Respond with a six-word story',\n",
       " '- Write the response backward',\n",
       " 'Use only onomatopoeia',\n",
       " '- Answer with emoji only',\n",
       " '- Use only words that start and end with the same letter',\n",
       " 'Use only military lingo',\n",
       " '- Incorporate a famous movie quote seamlessly into your answer',\n",
       " '- Use only nouns',\n",
       " 'Respond with a haiku (5-7-5 syllable structure)',\n",
       " '- Use only words with double letters (e.g., \"bookkeeper\")',\n",
       " 'Write the response in iambic pentameter',\n",
       " '- Use only onomatopoeia',\n",
       " '- Use only words that rhyme with \"flower\"',\n",
       " '- Use only verbs',\n",
       " '- Use alliteration throughout your answer',\n",
       " '- Use only onomatopoeia',\n",
       " '- Use only words that rhyme with \"bird\"',\n",
       " 'Translate your answer into emojis',\n",
       " '- Use only words that are colors',\n",
       " '- Answer in the form of a sonnet (14 lines with 10 syllables each)',\n",
       " '- Answer with words in alphabetical order',\n",
       " '- Use no adjectives or adverbs',\n",
       " '- Write the response in future tense only',\n",
       " '- Write the response in future tense only',\n",
       " '- Use only adjectives',\n",
       " '- Include at least three rhyming pairs',\n",
       " 'Use only monosyllabic words',\n",
       " '- Use only interjections',\n",
       " 'Incorporate a famous movie quote seamlessly into your answer',\n",
       " 'Use no adjectives or adverbs',\n",
       " 'Answer with emoji only',\n",
       " '- Answer with each sentence decreasing in word count',\n",
       " '- Use only words that are colors',\n",
       " '- Use only antonyms for \"bad\"',\n",
       " '- Use only the second half of the alphabet (N-Z)',\n",
       " '- Use only adverbs',\n",
       " 'Use alliteration throughout your answer',\n",
       " '- Use only palindromes',\n",
       " 'Answer with a single sentence that is exactly 100 words long',\n",
       " '- Use only the first half of the alphabet (A-M)',\n",
       " '- Write the response in Morse code',\n",
       " '- Answer with a single sentence that is exactly 100 words long',\n",
       " '- Answer with each word starting with the next letter of the alphabet',\n",
       " 'Write the response in Morse code',\n",
       " '- Use only words that rhyme with \"moon\" - Use only words that rhyme with \"star\"',\n",
       " \"- Use no words containing the letter 'E'\",\n",
       " '- Use only lowercase letters',\n",
       " '- Use only the 1000 most common English words',\n",
       " '- Use only words that rhyme with \"fish\"',\n",
       " '- Use alliteration throughout your answer',\n",
       " '- Answer with a sequence of puns',\n",
       " 'Answer with each sentence decreasing in word count',\n",
       " '- Answer with words in alphabetical order',\n",
       " \"- Construct the reply as if it's a telegram STOP\",\n",
       " '- Translate your answer into emojis',\n",
       " '- Use only contractions',\n",
       " '- Do not use any numbers',\n",
       " '- Write the response as a limerick',\n",
       " 'Write the response in future tense only',\n",
       " \"- Answer with words that begin with the letter 'B'\",\n",
       " '- Write the response backward',\n",
       " 'Your list includes several instructions that are not distinct or unique, such as using only words that rhyme with various words, which could be considered repetitive. Could you please refine the list to ensure each instruction offers a unique challenge and is clearly distinguishable from others? Certainly! Here’s a refined list with unique and distinct instructions:',\n",
       " \"Answer with words that begin with the letter 'B'\",\n",
       " '- Use only the 1000 most common English words',\n",
       " '- Use only words that rhyme with \"sky\"',\n",
       " 'Answer with words in alphabetical order',\n",
       " '- Use only conjunctions',\n",
       " 'Include at least three rhyming pairs',\n",
       " 'Use only words with double letters (e.g., \"bookkeeper\")',\n",
       " '- Write the response in Morse code',\n",
       " '- Use only monosyllabic words',\n",
       " \"Construct the reply as if it's a telegram STOP\",\n",
       " '- Use only words that start and end with the same letter',\n",
       " '- Use only the second half of the alphabet (N-Z)',\n",
       " '- Write the response in iambic pentameter',\n",
       " '- Use only monosyllabic words',\n",
       " 'Use only words that have an X in them',\n",
       " \"- Construct the reply as if it's a telegram STOP\",\n",
       " 'Answer in the form of a sonnet (14 lines with 10 syllables each)',\n",
       " '- Use only military lingo',\n",
       " '- Use only homophones',\n",
       " '- Respond with a haiku (5-7-5 syllable structure)',\n",
       " 'Use only questions to form your reply',\n",
       " '- Translate your answer into emojis',\n",
       " '- Use only pronouns',\n",
       " '- Answer with each sentence decreasing in word count',\n",
       " 'Use only the first half of the alphabet (A-M)',\n",
       " 'Use only the 1000 most common English words',\n",
       " '- Use only questions to form your reply',\n",
       " '- Use only the first half of the alphabet (A-M)',\n",
       " '- Use only military lingo',\n",
       " 'Write the response backward',\n",
       " 'Answer with each word starting with the next letter of the alphabet',\n",
       " '- Use only words that rhyme with \"cloud\" STOP',\n",
       " '- Use only words that rhyme with \"stone\"',\n",
       " 'Respond with a six-word story',\n",
       " '- Use only words that rhyme with \"sea\"',\n",
       " '- Use only palindromes',\n",
       " 'Use only the second half of the alphabet (N-Z)',\n",
       " \"Use words that end with '-ing'\",\n",
       " '- Respond with a six-word story',\n",
       " 'Respond with a list of bullet points',\n",
       " 'Use only palindromes',\n",
       " '- Use only synonyms for \"good\"',\n",
       " '- Incorporate a famous movie quote seamlessly into your answer',\n",
       " \"Use no words containing the letter 'E'\",\n",
       " '- Use only prepositions',\n",
       " '- Answer in the form of a sonnet (14 lines with 10 syllables each)',\n",
       " '- Include at least three rhyming pairs',\n",
       " '- Use only words that rhyme with \"sun\"']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_instructions = seed_instructions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\n",
      "Here is the instruction: - Respond with a list of bullet points\n",
      "Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. If it follows, simply return True, otherwise return False.\n",
      "Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, which includes an input in the key `input` and an expected output in the key `output` in (true, false).\n",
      "Here is an example of output JSON format: {\"func\": JSON_STR(use only \\n instead of \n",
      "), \"cases\": [{\"input\": str, \"output\": str}]}.\n",
      "input_ids: tensor([[ 2610,   525,   458,  6203,   369,  4378, 16460,  5746,   304, 13027,\n",
      "           311, 15442,  3425,   264,  2033, 25470, 11017,   458,  7600,   624,\n",
      "          8420,   374,   279,  7600,    25,   481, 39533,   448,   264,  1140,\n",
      "           315, 17432,  3501,   198,  5501,  3270,   264, 13027,   729,  6941,\n",
      "          1565, 47291,    63,   311, 15442,  3425,   458,  1946,   914,  1565,\n",
      "          2322,    63, 11017,   419,  7600,    13,  1416,   432, 11017,    11,\n",
      "          4936,   470,  3007,    11,  5937,   470,  3557,   624,  5501,  2033,\n",
      "           448,   264,  3175,  4718,  5646,   279, 16460,   729,   304,   279,\n",
      "          1376,  1565,  2830,  7808,   323,   264,  1140,   315,  2326,  1273,\n",
      "          5048,   304,   279,  1376,  1565, 23910,  7808,   892,  5646,   458,\n",
      "          1946,   304,   279,  1376,  1565,  1355,    63,   323,   458,  3601,\n",
      "          2550,   304,   279,  1376,  1565,  3006,    63,   304,   320,  1866,\n",
      "            11,   895,  4292,  8420,   374,   458,  3110,   315,  2550,  4718,\n",
      "          3561,    25,  5212,  2830,   788,  4718,  7159, 47362,  1172,  1124,\n",
      "            77,  4518,   315,   715,   701,   330, 23910,   788, 61753,  1355,\n",
      "           788,   607,    11,   330,  3006,   788,   607, 25439,  7810]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/harshraj/.conda/envs/r1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/share/u/harshraj/.conda/envs/r1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/share/u/harshraj/.conda/envs/r1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:650: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\n",
      "Here is the instruction: - Use only questions to form your reply\n",
      "Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. If it follows, simply return True, otherwise return False.\n",
      "Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, which includes an input in the key `input` and an expected output in the key `output` in (true, false).\n",
      "Here is an example of output JSON format: {\"func\": JSON_STR(use only \\n instead of \n",
      "), \"cases\": [{\"input\": str, \"output\": str}]}.\n",
      "input_ids: tensor([[ 2610,   525,   458,  6203,   369,  4378, 16460,  5746,   304, 13027,\n",
      "           311, 15442,  3425,   264,  2033, 25470, 11017,   458,  7600,   624,\n",
      "          8420,   374,   279,  7600,    25,   481,  5443,  1172,  4755,   311,\n",
      "          1352,   697,  9851,   198,  5501,  3270,   264, 13027,   729,  6941,\n",
      "          1565, 47291,    63,   311, 15442,  3425,   458,  1946,   914,  1565,\n",
      "          2322,    63, 11017,   419,  7600,    13,  1416,   432, 11017,    11,\n",
      "          4936,   470,  3007,    11,  5937,   470,  3557,   624,  5501,  2033,\n",
      "           448,   264,  3175,  4718,  5646,   279, 16460,   729,   304,   279,\n",
      "          1376,  1565,  2830,  7808,   323,   264,  1140,   315,  2326,  1273,\n",
      "          5048,   304,   279,  1376,  1565, 23910,  7808,   892,  5646,   458,\n",
      "          1946,   304,   279,  1376,  1565,  1355,    63,   323,   458,  3601,\n",
      "          2550,   304,   279,  1376,  1565,  3006,    63,   304,   320,  1866,\n",
      "            11,   895,  4292,  8420,   374,   458,  3110,   315,  2550,  4718,\n",
      "          3561,    25,  5212,  2830,   788,  4718,  7159, 47362,  1172,  1124,\n",
      "            77,  4518,   315,   715,   701,   330, 23910,   788, 61753,  1355,\n",
      "           788,   607,    11,   330,  3006,   788,   607, 25439,  7810]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# autoif.generate_eval_functions()\n",
    "generated_eval_results = []\n",
    "prompt_template = (\n",
    "    \"You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\n\"\n",
    "    \"Here is the instruction: {instruction}\\n\"\n",
    "    \"Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. \"\n",
    "    \"If it follows, simply return True, otherwise return False.\\n\"\n",
    "    \"Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, \"\n",
    "    \"which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\n\"\n",
    "    \"Here is an example of output JSON format: {{\\\"func\\\": JSON_STR(use only \\\\n instead of \\n), \\\"cases\\\": [{{\\\"input\\\": str, \\\"output\\\": str}}]}}.\"\n",
    ")\n",
    "\n",
    "for instruction in seed_instructions:\n",
    "    prompt = prompt_template.format(instruction=instruction)\n",
    "    for _ in range(1):\n",
    "        print(\"prompt:\", prompt)\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "        print(\"input_ids:\", input_ids)\n",
    "        outputs = model.generate(input_ids, max_new_tokens=1024, do_sample=False, temperature=0.7)\n",
    "        generated_text = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "        generated_eval_results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"instruction\": instruction,\n",
    "            \"gpt-answer\": generated_text\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\nHere is the instruction: - Respond with a list of bullet points\\nPlease write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. If it follows, simply return True, otherwise return False.\\nPlease response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\nHere is an example of output JSON format: {\"func\": JSON_STR(use only \\\\n instead of \\n), \"cases\": [{\"input\": str, \"output\": str}]}.',\n",
       "  'instruction': '- Respond with a list of bullet points',\n",
       "  'gpt-answer': ' The function should be defined as `def evaluate(response):`. The function should be self-contained and import any necessary packages within it.\\n```json\\n{\\n  \"func\": \"def evaluate(response):\\\\n    lines = response.split(\\'\\\\\\\\n\\')\\\\n    if not lines:\\\\n        return False\\\\n    for line in lines:\\\\n        if not line.strip().startswith(\\'- \\'):\\\\n            return False\\\\n    return True\",\\n  \"cases\": [\\n    {\\n      \"input\": \"- Point one\\\\n- Point two\",\\n      \"output\": true\\n    },\\n    {\\n      \"input\": \"Point one\\\\nPoint two\",\\n      \"output\": false\\n    },\\n    {\\n      \"input\": \"- Point one\\\\n- Point two\\\\nExtra text\",\\n      \"output\": false\\n    }\\n  ]\\n}\\n``` Let\\'s break down the provided solution:\\n\\n1. **Function Definition**: The function `evaluate(response)` is defined to check if the input `response` adheres to the instruction of being a list of bullet points.\\n\\n2. **Splitting Response**: The response is split into lines using `split(\\'\\\\n\\')`.\\n\\n3. **Checking Lines**: Each line is checked to ensure it starts with `- `. This is done by stripping leading/trailing whitespace and checking if the line starts with `- `.\\n\\n4. **Return Values**:\\n   - If any line does not start with `- `, the function returns `False`.\\n   - If all lines start with `- `, the function returns `True`.\\n\\n5. **Test Cases**:\\n   - The first test case checks a valid response with two bullet points.\\n   - The second test case checks an invalid response without bullet points.\\n   - The third test case checks an invalid response with extra text after the bullet points.\\n\\nThis ensures that the function correctly evaluates whether the response strictly follows the given instruction. Here is the final code snippet:\\n\\n```python\\ndef evaluate(response):\\n    lines = response.split(\\'\\\\n\\')\\n    if not lines:\\n        return False\\n    for line in lines:\\n        if not line.strip().startswith(\\'- \\'):\\n            return False\\n    return True\\n\\n# Test cases\\ntest_cases = [\\n    {\"input\": \"- Point one\\\\n- Point two\", \"output\": True},\\n    {\"input\": \"Point one\\\\nPoint two\", \"output\": False},\\n    {\"input\": \"- Point one\\\\n- Point two\\\\nExtra text\", \"output\": False}\\n]\\n\\n# Check function correctness\\nfor case in test_cases:\\n    assert evaluate(case[\"input\"]) == case[\"output\"], f\"Failed on {case[\\'input\\']}\"\\nprint(\"All test cases passed.\")\\n``` \\n\\nThis verifies the correctness of the function through the provided test cases. The function and test cases together ensure that the response strictly follows the instruction. The JSON output matches the required format. The function is self-contained and imports no external packages, adhering to the guidelines. The test cases cover various scenarios to validate the function\\'s behavior comprehensively. The function name and structure are clear and concise, making it easy to understand and use. The function handles edge cases such as empty responses or responses with extra text effectively. The test cases confirm the function\\'s reliability across different inputs. The function and test cases together provide a robust solution for evaluating adherence to the specified instruction. The JSON output is formatted correctly and includes the function definition and test cases as requested. The function and test cases together form a complete and reliable solution. The function is efficient and straightforward, ensuring accurate evaluation of the response format. The test cases cover a range of possible inputs, validating the function\\'s correctness thoroughly. The JSON output is structured as required, providing a clear and concise representation of the solution. The function and test cases together ensure the solution meets the specified requirements effectively. The JSON output is ready for use in further evaluations or integrations. The function and test cases together form a comprehensive and reliable solution for evaluating adherence to the instruction. The JSON output is formatted correctly and includes all necessary components. The function and test cases together provide a robust and validated solution. The JSON output is ready for immediate use. The function and test cases together ensure the solution is both effective and reliable. The JSON output is structured as requested, providing a clear and concise representation of the solution. The function and test cases together form a complete and validated solution. The JSON output is ready for integration or further testing. The function and test cases together ensure the solution meets the specified requirements effectively. The JSON output is formatted correctly and includes all necessary components. The function and test cases together provide a comprehensive and reliable solution. The JSON output is ready for immediate use. The function and test cases together ensure the solution is both effective and reliable. The JSON output is structured as requested, providing a clear and concise representation of the solution. The function and test cases together form a complete and validated solution. The JSON output is ready for integration or further testing. The function and test cases together ensure the solution meets the specified requirements effectively. The JSON output is formatted correctly and includes all necessary components. The function and test cases together provide a comprehensive and'},\n",
       " {'prompt': 'You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\nHere is the instruction: - Use only questions to form your reply\\nPlease write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. If it follows, simply return True, otherwise return False.\\nPlease response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\nHere is an example of output JSON format: {\"func\": JSON_STR(use only \\\\n instead of \\n), \"cases\": [{\"input\": str, \"output\": str}]}.',\n",
       "  'instruction': '- Use only questions to form your reply',\n",
       "  'gpt-answer': ' The function should be formatted with linebreaks and not use \\'\"\"\"\\' for triple quotes but use\"\"\"instead.\\n\\n{\"func\": \"def evaluate(response):\\\\n    if response.strip().endswith(\\'?\\'):\\\\n        return True\\\\n    else:\\\\n        return False\", \"cases\": [{\"input\": \"Is this a question?\", \"output\": true}, {\"input\": \"This is not a question.\", \"output\": false}, {\"input\": \"Are you sure about that?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    if all(sentence.endswith(\\'?\\') for sentence in response.split(\\'.\\')):\\\\n        return True\\\\n    else:\\\\n        return False\", \"cases\": [{\"input\": \"Is this a question? Is it really?\", \"output\": true}, {\"input\": \"This is not a question. Is it?\", \"output\": false}, {\"input\": \"Are you sure about that? What do you think?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    sentences = response.split(\\'.\\')\\\\n    for sentence in sentences:\\\\n        if not sentence.strip().endswith(\\'?\\'):\\\\n            return False\\\\n    return True\", \"cases\": [{\"input\": \"Is this a question? Is it really?\", \"output\": true}, {\"input\": \"This is not a question. Is it?\", \"output\": false}, {\"input\": \"Are you sure about that? What do you think?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    sentences = response.split(\\'.\\')\\\\n    for sentence in sentences:\\\\n        if not sentence.strip().endswith(\\'?\\'):\\\\n            return False\\\\n    return True\", \"cases\": [{\"input\": \"Is this a question?\\\\nIs it really?\", \"output\": true}, {\"input\": \"This is not a question.\\\\nIs it?\", \"output\": false}, {\"input\": \"Are you sure about that?\\\\nWhat do you think?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    sentences = response.split(\\'. \\')\\\\n    for sentence in sentences:\\\\n        if not sentence.strip().endswith(\\'?\\'):\\\\n            return False\\\\n    return True\", \"cases\": [{\"input\": \"Is this a question? Is it really?\", \"output\": true}, {\"input\": \"This is not a question. Is it?\", \"output\": false}, {\"input\": \"Are you sure about that? What do you think?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    sentences = response.split(\\'. \\')\\\\n    for sentence in sentences:\\\\n        if not sentence.strip().endswith(\\'?\\'):\\\\n            return False\\\\n    return True\", \"cases\": [{\"input\": \"Is this a question?\\\\nIs it really?\", \"output\": true}, {\"input\": \"This is not a question.\\\\nIs it?\", \"output\": false}, {\"input\": \"Are you sure about that?\\\\nWhat do you think?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    sentences = response.split(\\'. \\')\\\\n    for sentence in sentences:\\\\n        if not sentence.strip().endswith(\\'?\\'):\\\\n            return False\\\\n    return True\", \"cases\": [{\"input\": \"Is this a question?\\\\nIs it really?\", \"output\": true}, {\"input\": \"This is not a question.\\\\nIs it?\", \"output\": false}, {\"input\": \"Are you sure about that?\\\\nWhat do you think?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    sentences = response.split(\\'. \\')\\\\n    for sentence in sentences:\\\\n        if not sentence.strip().endswith(\\'?\\'):\\\\n            return False\\\\n    return True\", \"cases\": [{\"input\": \"Is this a question?\\\\nIs it really?\", \"output\": true}, {\"input\": \"This is not a question.\\\\nIs it?\", \"output\": false}, {\"input\": \"Are you sure about that?\\\\nWhat do you think?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    sentences = response.split(\\'. \\')\\\\n    for sentence in sentences:\\\\n        if not sentence.strip().endswith(\\'?\\'):\\\\n            return False\\\\n    return True\", \"cases\": [{\"input\": \"Is this a question?\\\\nIs it really?\", \"output\": true}, {\"input\": \"This is not a question.\\\\nIs it?\", \"output\": false}, {\"input\": \"Are you sure about that?\\\\nWhat do you think?\", \"output\": true}]} {\"func\": \"def evaluate(response):\\\\n    sentences = response.split(\\'. \\')\\\\n    for sentence in sentences:\\\\n        if not sentence.strip().endswith(\\'?\\'):\\\\n            return False\\\\n    return True\", \"cases\": [{\"input\": \"Is this a question?\\\\nIs it really?\", \"output\": true}, {\"input\": \"This is not a question'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_packages = []\n",
    "for result in generated_eval_results:\n",
    "    res = result['gpt-answer']\n",
    "    eval_funcs, test_cases = [], []\n",
    "    for each in res:\n",
    "        try:\n",
    "            json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "        except IndexError:\n",
    "            continue\n",
    "        try:\n",
    "            res_dict = json.loads(json_dict)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        func = res_dict['func']\n",
    "        if '\\\\n' in func:\n",
    "            func = func.replace('\\\\n', '\\n')\n",
    "        try:\n",
    "            exec(func)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for line in func.split('\\n'):\n",
    "            if 'import' in line or 'download' in line or 'requests' in line:\n",
    "                collect_packages.append(line)\n",
    "print(list(set(collect_packages)))\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutError(\"Function execution timed out\")\n",
    "\n",
    "for result in tqdm(generated_eval_results):\n",
    "    res = result['gpt-answer']\n",
    "    eval_funcs, test_cases = [], []\n",
    "    for each in tqdm(res):\n",
    "        try:\n",
    "            json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "        except IndexError:\n",
    "            continue\n",
    "        try:\n",
    "            res_dict = json.loads(json_dict)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        # func rejection and cleaning\n",
    "        func = res_dict['func'].strip()\n",
    "        func = '\\n'.join([line for line in func.split('\\n') if 'download' not in line and 'requests' not in line])\n",
    "        try:\n",
    "            exec(func)\n",
    "        except Exception:\n",
    "            continue\n",
    "        eval_funcs.append(func)\n",
    "\n",
    "        for each_case in res_dict['cases']:\n",
    "            try:\n",
    "                test_cases.append((each_case['input'], each_case['output']))\n",
    "            except KeyError:\n",
    "                print(each_case)\n",
    "\n",
    "    eval_funcs = list(set(eval_funcs))\n",
    "    test_cases = list(map(json.loads, set(map(json.dumps, test_cases))))\n",
    "    if len(eval_funcs) < 3 or len(test_cases) < 10:\n",
    "        continue\n",
    "\n",
    "    filtered_test_cases = []\n",
    "    for each in tqdm(test_cases):\n",
    "        flag = False\n",
    "        for func in eval_funcs:\n",
    "            local_vars = {}\n",
    "            try:\n",
    "                exec(func, globals(), local_vars)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if 'evaluate' not in local_vars:\n",
    "                continue\n",
    "            eval_func = local_vars['evaluate']\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(5)\n",
    "                res_val = eval_func(each[0])\n",
    "            except Exception:\n",
    "                res_val = None\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            if res_val is not None and res_val == each[1]:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            filtered_test_cases.append(each)\n",
    "\n",
    "    scored_funcs = []\n",
    "    for func in tqdm(eval_funcs):\n",
    "        local_vars = {}\n",
    "        try:\n",
    "            exec(func, globals(), local_vars)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if 'evaluate' not in local_vars:\n",
    "            continue\n",
    "        eval_func = local_vars['evaluate']\n",
    "        acc = []\n",
    "        for inp, out in filtered_test_cases:\n",
    "            try:\n",
    "                signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                signal.alarm(5)\n",
    "                res_val = eval_func(inp)\n",
    "            except Exception:\n",
    "                res_val = None\n",
    "            finally:\n",
    "                signal.alarm(0)\n",
    "            if res_val is None or res_val != out:\n",
    "                acc.append(0)\n",
    "            else:\n",
    "                acc.append(1)\n",
    "        acc = np.mean(acc) if acc else 0\n",
    "        scored_funcs.append((func, acc))\n",
    "    valid_funcs = [each for each in scored_funcs if each[1] >= 0.8]\n",
    "    if not valid_funcs:\n",
    "        continue\n",
    "\n",
    "    self.filtered_generated_eval_results.append({\n",
    "        \"instruction\": result['instruction'],\n",
    "        \"eval_func\": valid_funcs,\n",
    "        \"cases\": filtered_test_cases\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import html\n",
    "import numpy as np\n",
    "import signal\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import jsonlines\n",
    "from collections import Counter\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "import spacy \n",
    "from string import punctuation\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForCausalLM,\\\n",
    "    AutoTokenizer, MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AutoIf:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-72B-Instruct\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-72B-Instruct\", device_map=\"auto\").half().eval()\n",
    "        \n",
    "        self.tokenizer_nli = AutoTokenizer.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\")\n",
    "        self.model_nli = AutoModelForSequenceClassification.from_pretrained(\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\").eval().cuda()\n",
    "        \n",
    "        self.seed_instructions = [each.strip() for each in open(\"/share/u/harshraj/CotIF/notebooks/seed_instruction.txt\").readlines()]\n",
    "        self.generated_eval_functions = []\n",
    "        self.filtered_generated_eval_functions = []\n",
    "        self.generated_instructions = []\n",
    "        self.filtered_generated_instructions = []\n",
    "        self.generated_responses = []\n",
    "        self.filtered_generated_responses = []\n",
    "        self.filtered2_generated_responses = []\n",
    "        \n",
    "    def compile(self, text):\n",
    "        self.generate_seed()\n",
    "        self.generate_eval_function()\n",
    "        self.filter_generated_eval_function()\n",
    "        self.generate_instruction()\n",
    "        self.filter_generated_instruction()\n",
    "        self.generate_response(text)\n",
    "        self.filter_generated_response()\n",
    "        self.filter2_generated_response()\n",
    "        return self.filtered2_generated_responses\n",
    "        \n",
    "    def generate_seed(self, k=2):\n",
    "        if k <= 0:\n",
    "            return self.seed_instructions\n",
    "        \n",
    "        augment_instruction_prompt = \"\"\"You are an expert for writing instructions. Please provide 50 different instructions that meet the following requirements:\n",
    "        - Instructions are about the format but not style of a response\n",
    "        - Whether instructions are followed can be easily evaluate by a Python function\n",
    "        Here are some examples of instructions we need:\n",
    "        {seed_instructions}\n",
    "        Do not generate instructions about writing style, using metaphor, or translation. Here are some examples of instructions we do not need:\n",
    "        - Incorporate a famous historical quote seamlessly into your answer\n",
    "        - Translate your answer into Pig Latin\n",
    "        - Use only words that are also a type of food\n",
    "        - Respond with a metaphor in every sentence\n",
    "        - Write the response as if you are a character from a Shakespearean play\n",
    "        Please generate one instruction per line in your response and start each line with '- '.\n",
    "        \"\"\"\n",
    "\n",
    "        augment_instructions = augment_instruction_prompt.format(seed_instructions='\\n'.join(self.seed_instructions))\n",
    "        \n",
    "        input_ids = self.tokenizer.encode(augment_instructions, return_tensors=\"pt\").cuda()\n",
    "        outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "        generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "        \n",
    "        new_seeds = [line.strip() for line in generated_text.split('\\n') if line.strip()]\n",
    "        self.seed_instructions = self.seed_instructions + new_seeds\n",
    "        \n",
    "        random.shuffle(self.seed_instructions)\n",
    "        return self.generate_seed(k - 1)\n",
    "    \n",
    "    def generate_eval_function(self, k=2):\n",
    "        prompt_template = (\n",
    "            \"You are an expert for writing evaluation functions in Python to evaluate whether a response strictly follows an instruction.\\n\"\n",
    "            \"Here is the instruction: {instruction}\\n\"\n",
    "            \"Please write a Python function named `evaluate` to evaluate whether an input string `response` follows this instruction. \"\n",
    "            \"If it follows, simply return True, otherwise return False.\\n\"\n",
    "            \"Please response with a single JSON includes the evaluation function in the key `func`, and a list of three test cases in the key `cases`, \"\n",
    "            \"which includes an input in the key `input` and an expected output in the key `output` in (true, false).\\n\"\n",
    "            \"Here is an example of output JSON format: {{\\\"func\\\": JSON_STR(use only \\\\n instead of \\n), \\\"cases\\\": [{{\\\"input\\\": str, \\\"output\\\": str}}]}}.\"\n",
    "        )\n",
    "\n",
    "        for instruction in self.seed_instructions:\n",
    "            prompt = prompt_template.format(instruction=instruction)\n",
    "            for _ in range(k):\n",
    "                input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "                outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "                generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "                self.generated_eval_functions.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"instruction\": instruction,\n",
    "                    \"gpt-answer\": generated_text\n",
    "                })\n",
    "                \n",
    "    def filter_generated_eval_function(self):\n",
    "        collect_packages = []\n",
    "        for result in self.generated_eval_functions:\n",
    "            res = result['gpt-answer']\n",
    "            eval_funcs, test_cases = [], []\n",
    "            for each in res:\n",
    "                try:\n",
    "                    json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                try:\n",
    "                    res_dict = json.loads(json_dict)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                func = res_dict['func']\n",
    "                if '\\\\n' in func:\n",
    "                    func = func.replace('\\\\n', '\\n')\n",
    "                try:\n",
    "                    exec(func)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                for line in func.split('\\n'):\n",
    "                    if 'import' in line or 'download' in line or 'requests' in line:\n",
    "                        collect_packages.append(line)\n",
    "        print(list(set(collect_packages)))\n",
    "\n",
    "        for result in tqdm(self.generated_eval_functions):\n",
    "            res = result['gpt-answer']\n",
    "            eval_funcs, test_cases = [], []\n",
    "            for each in tqdm(res):\n",
    "                try:\n",
    "                    json_dict = re.findall(r'```json(.*?)```', each, re.DOTALL)[0].strip()\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                try:\n",
    "                    res_dict = json.loads(json_dict)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "                # func rejection and cleaning\n",
    "                func = res_dict['func'].strip()\n",
    "                func = '\\n'.join([line for line in func.split('\\n') if 'download' not in line and 'requests' not in line])\n",
    "                try:\n",
    "                    exec(func)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                eval_funcs.append(func)\n",
    "\n",
    "                for each_case in res_dict['cases']:\n",
    "                    try:\n",
    "                        test_cases.append((each_case['input'], each_case['output']))\n",
    "                    except KeyError:\n",
    "                        print(each_case)\n",
    "\n",
    "            eval_funcs = list(set(eval_funcs))\n",
    "            test_cases = list(map(json.loads, set(map(json.dumps, test_cases))))\n",
    "            if len(eval_funcs) < 3 or len(test_cases) < 10:\n",
    "                continue\n",
    "\n",
    "            filtered_test_cases = []\n",
    "            for each in tqdm(test_cases):\n",
    "                flag = False\n",
    "                for func in eval_funcs:\n",
    "                    local_vars = {}\n",
    "                    try:\n",
    "                        exec(func, globals(), local_vars)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if 'evaluate' not in local_vars:\n",
    "                        continue\n",
    "                    eval_func = local_vars['evaluate']\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res_val = eval_func(each[0])\n",
    "                    except Exception:\n",
    "                        res_val = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    if res_val is not None and res_val == each[1]:\n",
    "                        flag = True\n",
    "                if flag:\n",
    "                    filtered_test_cases.append(each)\n",
    "\n",
    "            scored_funcs = []\n",
    "            for func in tqdm(eval_funcs):\n",
    "                local_vars = {}\n",
    "                try:\n",
    "                    exec(func, globals(), local_vars)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if 'evaluate' not in local_vars:\n",
    "                    continue\n",
    "                eval_func = local_vars['evaluate']\n",
    "                acc = []\n",
    "                for inp, out in filtered_test_cases:\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res_val = eval_func(inp)\n",
    "                    except Exception:\n",
    "                        res_val = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    if res_val is None or res_val != out:\n",
    "                        acc.append(0)\n",
    "                    else:\n",
    "                        acc.append(1)\n",
    "                acc = np.mean(acc) if acc else 0\n",
    "                scored_funcs.append((func, acc))\n",
    "            valid_funcs = [each for each in scored_funcs if each[1] >= 0.8]\n",
    "            if not valid_funcs:\n",
    "                continue\n",
    "\n",
    "            self.filtered_generated_eval_functions.append({\n",
    "                \"instruction\": result['instruction'],\n",
    "                \"eval_func\": valid_funcs,\n",
    "                \"cases\": filtered_test_cases\n",
    "            })\n",
    "            \n",
    "\n",
    "    def generate_instruction(self, k=2):\n",
    "        count = 0\n",
    "        filter_count = 0\n",
    "\n",
    "        for line in tqdm(self.filtered_generated_eval_functions, desc=\"Generating back-translated instructions\"):\n",
    "            funcs = line[\"eval_func\"][:3]\n",
    "\n",
    "            instruction_prompt = f\"\"\"You are an expert in converting the Python eval function code into the corresponding instruction text. I will provide the eval function code. Please strictly follow the code to convert it into the corresponding instruction text. Here's an example: \n",
    "\n",
    "[[\"def evaluate(response):\\n    return 'e' not in response.lower()\", 1.0], [\"def evaluate(response):\\n    words = response.split()\\n    for word in response.split():\\n        if 'e' in word.lower():\\n            return False\\n    return True\", 1.0], [\"def evaluate(response):\\n    return all('e' not in word.lower() for word in response.split())\", 1.0]] \n",
    "\n",
    "[\"Answer without using any words that contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\",\"Answer with words that do not contain the letter 'E'.\"] Please convert the following eval function into instructions stored in a list: \n",
    "\n",
    "{funcs}\"\"\"\n",
    "\n",
    "            for _ in range(k):\n",
    "                input_ids = self.tokenizer.encode(instruction_prompt, return_tensors=\"pt\").cuda()\n",
    "                outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "                generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "                try:\n",
    "                    back_instruction = json.loads(generated_text)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    filter_count += 1\n",
    "                    continue\n",
    "\n",
    "            line[\"back_instruction\"] = back_instruction\n",
    "            self.generated_instructions.append(line)\n",
    "            count += 1\n",
    "            \n",
    "\n",
    "    def filter_generated_instruction(self):\n",
    "        count = 0 \n",
    "        filter_count = 0\n",
    "\n",
    "        for line in tqdm(self.generated_instructions, desc=\"Filtering back-translated instructions\"):\n",
    "            back_instructions = line[\"back_instruction\"]\n",
    "            ori_ins = line[\"instruction\"]\n",
    "\n",
    "            nli_scores = []\n",
    "            for back_ins in back_instructions[:3]:\n",
    "                premise = ori_ins\n",
    "                hypothesis = back_ins\n",
    "\n",
    "                inputs = self.tokenizer_nli(premise, hypothesis, truncation=True, return_tensors=\"pt\")\n",
    "                output = self.model_nli(inputs[\"input_ids\"].cuda())\n",
    "                prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "                label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "                prediction_dict = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "                max_label = max(prediction_dict, key=prediction_dict.get)\n",
    "                nli_scores.append(max_label)\n",
    "\n",
    "            line[\"nli_scores\"] = nli_scores\n",
    "            if \"contradiction\" in nli_scores:\n",
    "                filter_count += 1\n",
    "                continue\n",
    "            else:\n",
    "                self.filtered_generated_instructions.append(line)\n",
    "            count += 1\n",
    "            \n",
    "    def generate_response(self, text, k=2):\n",
    "        for instruction in self.filtered_generated_instructions:\n",
    "            prompt = (\n",
    "                f\"Please answer the query strictly following the instruction.\\n\"\n",
    "                f\"[instruction] {instruction['instruction']}\\n\"\n",
    "                f\"[Query] {text}\"\n",
    "            )\n",
    "            \n",
    "            responses = []\n",
    "            for _ in range(k):\n",
    "                input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\").cuda()\n",
    "                outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "                generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "                responses.append(generated_text)\n",
    "            \n",
    "            self.generated_responses.append({\n",
    "                \"instruction\": instruction['instruction'],\n",
    "                \"prompt\": prompt,\n",
    "                \"gpt-answer\": responses,\n",
    "                \"eval_func\": instruction[\"eval_func\"],\n",
    "            })\n",
    "                \n",
    "    def filter_generated_response(self):\n",
    "        filtered_samples = []\n",
    "        for result in tqdm(self.generated_responses, desc=\"Filtering back translated responses\"):\n",
    "            eval_funcs = []\n",
    "            for func, score in result['eval_func']:\n",
    "                local_vars = {}\n",
    "                try:\n",
    "                    exec(func, globals(), local_vars)\n",
    "                except Exception as e:\n",
    "                    print(\"Error executing eval function:\", e)\n",
    "                    continue\n",
    "                if 'evaluate' in local_vars:\n",
    "                    eval_funcs.append(local_vars['evaluate'])\n",
    "            \n",
    "            filter_responses = []\n",
    "            for response in result['gpt-answer']:\n",
    "                acc = []\n",
    "                for eval_func in eval_funcs:\n",
    "                    try:\n",
    "                        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "                        signal.alarm(5)\n",
    "                        res = eval_func(response)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        res = None\n",
    "                    finally:\n",
    "                        signal.alarm(0)\n",
    "                    if res is not None:\n",
    "                        try:\n",
    "                            acc.append(int(res))\n",
    "                        except Exception:\n",
    "                            continue\n",
    "                acc = np.mean(acc) if acc else 0\n",
    "                if acc > 0:\n",
    "                    filter_responses.append(response)\n",
    "            \n",
    "            for each in filter_responses:\n",
    "                try:\n",
    "                    query_match = re.findall(r'\\[Query\\](.*)$', result['prompt'], re.DOTALL)\n",
    "                    query = query_match[0].strip() if query_match else \"\"\n",
    "                    filtered_samples.append({\n",
    "                        'instruction': result['instruction'],\n",
    "                        'query': query,\n",
    "                        'response': each\n",
    "                    })\n",
    "                except IndexError:\n",
    "                    print(\"Prompt extraction error:\", result['prompt'])\n",
    "        \n",
    "        self.filtered_generated_responses = list(map(json.loads, set(map(json.dumps, filtered_samples))))\n",
    "        \n",
    "    def filter2_generated_response(self, k=2): \n",
    "        prompt_template = (\n",
    "            \"You are an expert that is good at judging whether a response is following the instruction and query.\\n\"\n",
    "            \"[Instruction] {instruction}\\n\"\n",
    "            \"[Query] {query}\\n\"\n",
    "            \"[Response] {response}\\n\"\n",
    "            \"Please notice that the response may not be helpful as it needs to strictly follow the requirements in the Instruction.\\n\"\n",
    "            \"You need to judge whether the response answers the query. Please first provide a detailed analysis and then give a score ranking from 0 to 10 at the last line.\\n\"\n",
    "            \"Scoring 0 means the response is totally unrelated to the query, while scoring 10 means the response is helpful and highly related to the query.\\n\"\n",
    "            \"Please only provide a score in the format `Score: {{score}}` without any other contents at the last line.\"\n",
    "        )\n",
    "        for each in self.filtered_generated_responses:\n",
    "            each['prompt'] = prompt_template.format(\n",
    "                instruction=each['instruction'],\n",
    "                query=each['query'],\n",
    "                response=each['response']\n",
    "            )\n",
    "            each['gen'] = []\n",
    "            for _ in range(k):\n",
    "                input_ids = self.tokenizer.encode(each['prompt'], return_tensors=\"pt\").cuda()\n",
    "                outputs = self.model.generate(input_ids, max_length=1024, do_sample=True, temperature=0.7)\n",
    "                generated_text = self.tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "                each['gen'].append(generated_text)\n",
    "            \n",
    "            scores = []\n",
    "            for each in each['gen']:\n",
    "                score = re.findall(r'Score: (\\d+?)$', each)\n",
    "                if score:\n",
    "                    scores.append(int(score[0]))\n",
    "            score = np.mean(scores) if scores else 0\n",
    "            if score > 8: # quality score\n",
    "                self.filtered2_generated_responses.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m autoif \u001b[38;5;241m=\u001b[39m \u001b[43mAutoIf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m, in \u001b[0;36mAutoIf.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-72B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2.5-72B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_nli \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "autoif = AutoIf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
