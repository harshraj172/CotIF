{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track the Axolotl finetune \n",
    "\n",
    "This notebook is an example of how to track the status of a finetune run started using `finetune_axolotl-status.ipynb`.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure to follow the setup instructions in the README to install axolotl and related libraries.\n",
    "\n",
    "Let's start with loading the code components we need. The `FinetuneConfig` class holds configurations, and the `Finetune` class is used to create and run a finetuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "from finetune import Finetune, FinetuneConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also make sure to login to Hugging Face Hub to save the output model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ad2a7ad25a4498941d0962727ed3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "We then load up any config to initialize the `FinetuneConfig` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Specify the path to your YAML file (you can pass any yaml file)\n",
    "file_path = os.path.join(os.getcwd(), '..', 'finetune/examples/llama-2/qlora.yml')\n",
    "\n",
    "# Open the file and load the data\n",
    "with open(file_path, encoding='utf-8') as file:\n",
    "    config_dict = yaml.safe_load(file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now load the config dict in the `FinetuneConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-03 18:16:58,367] [DEBUG] [axolotl.normalize_config:79] [PID:8249] [RANK:0] bf16 support detected, enabling for this configuration.\u001b[39m\n",
      "[2024-05-03 18:16:58,487] [INFO] [axolotl.normalize_config:182] [PID:8249] [RANK:0] GPU memory usage baseline: 0.000GB (+2.235GB misc)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# see all config options in './finetune/examples/config.qmd'\n",
    "config = FinetuneConfig(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track the Status\n",
    "Now simply load up the config into a `FineTune` object and track the finetune job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a finetune object with the config and run\n",
    "finetune = Finetune(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"{'type_of_model': 'LlamaForCausalLM', 'lisa_layers_attribute': 'model.layers', 'gradient_accumulation_steps': 4, 'micro_batch_size': 2, 'train_on_inputs': False, 'group_by_length': False, 'learning_rate': 0.0002, 'weight_decay': 0.0, 'optimizer': <OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, 'lr_scheduler': <SchedulerType.COSINE: 'cosine'>, 'num_epochs': 4, 'load_in_8bit': False, 'load_in_4bit': True, 'adapter': 'qlora', 'lora_r': 32, 'lora_alpha': 16, 'lora_target_linear': True, 'lora_dropout': 0.05, 'loraplus_lr_embedding': 1e-06, 'output_dir': './qlora-out', 'hub_model_id': 'vijil/my_lora_tune', 'base_model': 'NousResearch/Llama-2-7b-hf', 'tokenizer_type': 'LlamaTokenizer', 'strict': False, 'datasets': [{'path': 'mhenrichsen/alpaca_2k_test', 'type': 'alpaca'}], 'shuffle_merged_datasets': True, 'dataset_processes': 4, 'bf16': True, 'tf32': False, 'gradient_checkpointing': True, 'sequence_len': 4096, 'sample_packing': True, 'eval_sample_packing': False, 'pad_to_sequence_len': True, 'pretrain_multipack_buffer_size': 10000, 'pretrain_multipack_attn': True, 'flash_attention': True, 'val_set_size': 0.05, 'warmup_steps': 10, 'evals_per_epoch': 4, 'saves_per_epoch': 1, 'logging_steps': 1, 'load_best_model_at_end': False, 'axolotl_config_path': '', 'capabilities': {'bf16': True, 'fp8': False, 'n_gpu': 1, 'n_node': 1, 'compute_capability': 'sm_86'}, 'batch_size': 8, 'eval_batch_size': 2, 'world_size': 1, 'local_rank': 0, 'eval_table_size': 0, 'eval_max_new_tokens': 128, 'eval_causal_lm_metrics': ['sacrebleu', 'comet', 'ter', 'chrf'], 'device': 'cuda:0', 'device_map': 'auto', 'ddp': False, 'fp16': False, 'torch_dtype': torch.bfloat16, 'save_steps': 0.25, 'eval_steps': 0.0625, 'base_model_config': 'NousResearch/Llama-2-7b-hf', 'model_config_type': 'llama', 'tokenizer_config': 'NousResearch/Llama-2-7b-hf', 'is_llama_derived_model': True, 'is_falcon_derived_model': False, 'is_mistral_derived_model': False, 'is_qwen_derived_model': None, 'gradient_checkpointing_kwargs': {'use_reentrant': True}, 'total_num_tokens': 414041, 'total_supervised_tokens': 294246, 'sample_packing_eff_est': 0.98}\",\n",
       " 'Training in progress')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetune.status(\"385293fa-0979-11ef-b980-024398e8947b\") # track status"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_venv",
   "language": "python",
   "name": "conda_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
